{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08c392a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "gnn_experiment_refactored.py\n",
    "—————————————————————————————————\n",
    "End-to-end GNN experimentation wrapper for **binary rare-event\n",
    "classification** on temporal node snapshots.\n",
    "\n",
    "Key additions vs. original version\n",
    "• Per-node positive-class weights built into BCE loss\n",
    "• Edge-dropout (train-time only)\n",
    "• Configurable LR scheduler with LR print at each epoch\n",
    "\"\"\"\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 0. Imports\n",
    "# ----------------------------------------------------------------------\n",
    "import json\n",
    "import pathlib\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import Dict, Any, Optional, Union, List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import (\n",
    "    GCNConv,\n",
    "    GATv2Conv,\n",
    "    GCN,\n",
    "    GraphSAGE,\n",
    "    GAT,\n",
    ")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1. Configuration object\n",
    "# ----------------------------------------------------------------------\n",
    "@dataclass\n",
    "class GNNConfig:\n",
    "    # --- model ---------------------------------------------------------\n",
    "    task: str = \"node_clf\"            # {\"node_clf\", \"edge_clf\"}\n",
    "    model_name: str = \"gcn\"           # {\"gcn\",\"graphsage\",\"gat\",\"gatv2\"}\n",
    "    num_layers: int = 2\n",
    "    hidden_dim: int = 128\n",
    "    heads: int = 8                    # for attention models\n",
    "    norm: str = \"batch\"               # {\"batch\",\"layer\",None}\n",
    "\n",
    "    # --- data & label --------------------------------------------------\n",
    "    target_col: str = \"target\"\n",
    "\n",
    "    # --- split ---------------------------------------------------------\n",
    "    split_mode: str = \"date\"          # {\"date\",\"ratio\"}\n",
    "    cutoff_date: Optional[str] = None\n",
    "    val_ratio: float = 0.10\n",
    "    test_ratio: float = 0.10\n",
    "    shuffle_in_split: bool = False\n",
    "\n",
    "    # --- optimisation --------------------------------------------------\n",
    "    batch_size: int = 32\n",
    "    lr: float = 1e-3\n",
    "    epochs: int = 100\n",
    "    patience: int = 10\n",
    "    grad_clip: float = 1.0\n",
    "\n",
    "    optimiser: str = \"adam\"           # {\"adam\",\"adamw\",\"sgd\"}\n",
    "    optimiser_kwargs: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "    # --- LR scheduler --------------------------------------------------\n",
    "    lr_scheduler: Optional[str] = None         # {\"step\",\"plateau\",\"cosine\",\"onecycle\"}\n",
    "    lr_scheduler_kwargs: Dict[str, Any] = field(default_factory=dict)\n",
    "    print_lr_each_epoch: bool = True\n",
    "\n",
    "    # --- loss / weights -----------------------------------------------\n",
    "    loss_fn: str = \"bce\"              # {\"bce\",\"cross_entropy\"}\n",
    "    class_weights: Optional[List[float]] = None   # global BCE/CE weights\n",
    "    node_pos_weights: Optional[List[float]] = None  # per-node list (len == #nodes)\n",
    "\n",
    "    # --- regularisation -----------------------------------------------\n",
    "    in_dropout: float = 0.0           # feature-level dropout\n",
    "    layer_dropout: float = 0.5        # model internal dropout\n",
    "    edge_dropout: float = 0.0         # probability of dropping each edge at train-time\n",
    "\n",
    "    # --- misc ----------------------------------------------------------\n",
    "    device: str = \"cuda\"\n",
    "    run_name: str = \"default_run\"\n",
    "    seed: int = 42\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    @staticmethod\n",
    "    def load(cfg: Union[\"GNNConfig\", str, Dict[str, Any]]) -> \"GNNConfig\":\n",
    "        if isinstance(cfg, GNNConfig):\n",
    "            return cfg\n",
    "        if isinstance(cfg, dict):\n",
    "            return GNNConfig(**cfg)\n",
    "        if isinstance(cfg, (str, pathlib.Path)):\n",
    "            path = pathlib.Path(cfg)\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                if path.suffix == \".json\":\n",
    "                    data = json.load(f)\n",
    "                else:\n",
    "                    raise ValueError(\"Unsupported file type for config path\")\n",
    "            return GNNConfig(**data)\n",
    "        raise TypeError(f\"Unsupported cfg type: {type(cfg)}\")\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return asdict(self)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2. Utilities\n",
    "# ----------------------------------------------------------------------\n",
    "def has_nan(t: torch.Tensor) -> bool:\n",
    "    return torch.isnan(t).any() or torch.isinf(t).any()\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3. Mini model zoo\n",
    "# ----------------------------------------------------------------------\n",
    "class GNNNodeSimple(torch.nn.Module):\n",
    "    def __init__(self, d_in: int, hidden: int, d_out: int, p_drop: float):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(d_in, hidden)\n",
    "        self.conv2 = GCNConv(hidden, hidden)\n",
    "        self.dropout = torch.nn.Dropout(p_drop)\n",
    "        self.fc = torch.nn.Linear(hidden, d_out)\n",
    "\n",
    "    def forward(self, data: Data):\n",
    "        x, ei = data.x, data.edge_index\n",
    "        x = torch.relu(self.conv1(x, ei))\n",
    "        x = torch.relu(self.conv2(x, ei))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class GNNGCN(torch.nn.Module):\n",
    "    def __init__(self, d_in: int, hidden: int, d_out: int, layers: int, p_drop: float):\n",
    "        super().__init__()\n",
    "        self.body = GCN(d_in, hidden, layers, norm=\"batch\", act=\"relu\")\n",
    "        self.dropout = torch.nn.Dropout(p_drop)\n",
    "        self.fc = torch.nn.Linear(hidden, d_out)\n",
    "\n",
    "    def forward(self, data: Data):\n",
    "        x = self.body(data.x, data.edge_index)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class GNNSage(torch.nn.Module):\n",
    "    def __init__(self, d_in: int, hidden: int, d_out: int, layers: int, p_drop: float):\n",
    "        super().__init__()\n",
    "        self.body = GraphSAGE(d_in, hidden, layers, norm=\"batch\", act=\"relu\")\n",
    "        self.dropout = torch.nn.Dropout(p_drop)\n",
    "        self.fc = torch.nn.Linear(hidden, d_out)\n",
    "\n",
    "    def forward(self, data: Data):\n",
    "        x = self.body(data.x, data.edge_index)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class GNNGAT(torch.nn.Module):\n",
    "    def __init__(self, d_in: int, hidden: int, d_out: int, layers: int, heads: int, p_drop: float):\n",
    "        super().__init__()\n",
    "        self.body = GAT(d_in, hidden, layers, heads=heads, norm=\"batch\", act=\"relu\")\n",
    "        self.dropout = torch.nn.Dropout(p_drop)\n",
    "        self.fc = torch.nn.Linear(hidden, d_out)\n",
    "\n",
    "    def forward(self, data: Data):\n",
    "        x = self.body(data.x, data.edge_index)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class GNNGAT2(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_in: int,\n",
    "        hidden: int,\n",
    "        d_out: int,\n",
    "        layers: int,\n",
    "        heads: int,\n",
    "        edge_dim: int,\n",
    "        p_drop: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATv2Conv(d_in, hidden, heads=heads, concat=True, edge_dim=edge_dim)\n",
    "        self.conv2 = GATv2Conv(hidden * heads, hidden, heads=1, concat=False, edge_dim=edge_dim)\n",
    "        self.dropout = torch.nn.Dropout(p_drop)\n",
    "        self.fc = torch.nn.Linear(hidden, d_out)\n",
    "\n",
    "    def forward(self, data: Data):\n",
    "        x = torch.relu(self.conv1(data.x, data.edge_index, data.edge_attr))\n",
    "        x = torch.relu(self.conv2(x, data.edge_index, data.edge_attr))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 4. Experiment wrapper\n",
    "# ----------------------------------------------------------------------\n",
    "class GNNExperiment:\n",
    "    \"\"\"\n",
    "    Raw node/edge time-series → PyG snapshots → DataLoaders → Model.\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------------------ constructor\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_frames: Dict[str, pd.DataFrame],\n",
    "        edge_frames: Dict[str, pd.DataFrame],\n",
    "        graph: nx.DiGraph,\n",
    "        cfg: Union[GNNConfig, str, Dict[str, Any]] = GNNConfig(),\n",
    "    ):\n",
    "        # raw inputs\n",
    "        self.node_frames = node_frames\n",
    "        self.edge_frames = edge_frames\n",
    "        self.graph = graph\n",
    "\n",
    "        # configuration\n",
    "        self.cfg = GNNConfig.load(cfg)\n",
    "\n",
    "        # runtime placeholders\n",
    "        self.reg_order: List[str] = []\n",
    "        self.edge_order: List[Tuple[int, int]] = []\n",
    "        self.snapshots: Optional[List[Data]] = None\n",
    "\n",
    "        self.train_dl = self.val_dl = self.test_dl = None\n",
    "\n",
    "        self.model = None\n",
    "        self.optimizer = None\n",
    "        self.scheduler = None\n",
    "        self.loss_fn = None\n",
    "        self.metric_fn = None\n",
    "\n",
    "        self.history: Dict[str, list] = {\"train_loss\": [], \"val_loss\": [], \"val_metric\": []}\n",
    "        self.best_val_loss = float(\"inf\")\n",
    "        self.best_ckpt = None\n",
    "        self.patience_counter = 0\n",
    "\n",
    "        # per-node weight lookup\n",
    "        self._node_weight_lookup: Optional[torch.Tensor] = None\n",
    "\n",
    "        # device\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if (self.cfg.device == \"cuda\" and torch.cuda.is_available()) else \"cpu\"\n",
    "        )\n",
    "\n",
    "    # ---------------------------------------------------------------- helpers\n",
    "    @staticmethod\n",
    "    def _edge_key(src: str, dst: str) -> str:\n",
    "        return f\"{src}-{dst}\".lower()\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_same_index(idxs: List[pd.DatetimeIndex]) -> List[pd.Timestamp]:\n",
    "        common = sorted(set.intersection(*(set(ix) for ix in idxs)))\n",
    "        if not common:\n",
    "            raise ValueError(\"No common timestamps across supplied DataFrames\")\n",
    "        return common\n",
    "\n",
    "    # ------------------------------------------------------- 1. snapshots\n",
    "    def prepare_snapshots(self) -> \"GNNExperiment\":\n",
    "        # node & edge order\n",
    "        self.reg_order = sorted(self.graph.nodes)\n",
    "        node_pos = {n: i for i, n in enumerate(self.reg_order)}\n",
    "        self.edge_order = [(node_pos[s], node_pos[d]) for (s, d) in self.graph.edges]\n",
    "\n",
    "        # node-weight lookup\n",
    "        if self.cfg.node_pos_weights is not None:\n",
    "            if len(self.cfg.node_pos_weights) != len(self.reg_order):\n",
    "                raise ValueError(\"len(node_pos_weights) must equal number of nodes\")\n",
    "            self._node_weight_lookup = torch.tensor(self.cfg.node_pos_weights, dtype=torch.float32)\n",
    "        else:\n",
    "            self._node_weight_lookup = None\n",
    "\n",
    "        # timestamps intersection\n",
    "        ts_common = self._ensure_same_index(\n",
    "            [df.index for df in self.node_frames.values()]\n",
    "            + [df.index for df in self.edge_frames.values()]\n",
    "        )\n",
    "\n",
    "        snapshots: List[Data] = []\n",
    "        for ts in ts_common:\n",
    "            # node features + label\n",
    "            feats, labels = [], []\n",
    "            for region in self.reg_order:\n",
    "                row = self.node_frames[region].loc[ts]\n",
    "                labels.append(row[self.cfg.target_col])\n",
    "                feats.append(row.drop(self.cfg.target_col).to_numpy(dtype=np.float32))\n",
    "            x = torch.tensor(np.vstack(feats), dtype=torch.float32)\n",
    "            y = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "            # edge features\n",
    "            edge_rows = []\n",
    "            for s_idx, d_idx in self.edge_order:\n",
    "                s, d = self.reg_order[s_idx], self.reg_order[d_idx]\n",
    "                edge_rows.append(\n",
    "                    self.edge_frames[self._edge_key(s, d)].loc[ts].to_numpy(dtype=np.float32)\n",
    "                )\n",
    "            edge_attr = torch.tensor(np.vstack(edge_rows), dtype=torch.float32)\n",
    "            edge_index = torch.tensor(np.array(self.edge_order).T, dtype=torch.long)\n",
    "\n",
    "            node_weight = (\n",
    "                self._node_weight_lookup.clone()\n",
    "                if self._node_weight_lookup is not None\n",
    "                else torch.ones(len(self.reg_order))\n",
    "            )\n",
    "\n",
    "            snapshots.append(\n",
    "                Data(\n",
    "                    x=x,\n",
    "                    edge_index=edge_index,\n",
    "                    edge_attr=edge_attr,\n",
    "                    y=y,\n",
    "                    node_weight=node_weight,\n",
    "                    snap_time=torch.tensor([pd.Timestamp(ts).value]),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.snapshots = snapshots\n",
    "        return self\n",
    "\n",
    "    # ------------------------------------------------------- 2. loaders\n",
    "    def build_loaders(self) -> \"GNNExperiment\":\n",
    "        if self.snapshots is None:\n",
    "            raise RuntimeError(\"Call prepare_snapshots() before build_loaders()\")\n",
    "\n",
    "        snaps_sorted = sorted(self.snapshots, key=lambda g: g.snap_time.item())\n",
    "\n",
    "        if self.cfg.split_mode == \"date\":\n",
    "            if self.cfg.cutoff_date is None:\n",
    "                raise ValueError(\"cutoff_date must be set for date split\")\n",
    "            cutoff_int = pd.Timestamp(self.cfg.cutoff_date).value\n",
    "            train_set = [g for g in snaps_sorted if g.snap_time.item() <= cutoff_int]\n",
    "            holdout = [g for g in snaps_sorted if g.snap_time.item() > cutoff_int]\n",
    "            if not holdout:\n",
    "                raise ValueError(\"No snapshots after cutoff_date for val/test split\")\n",
    "            mid = len(holdout) // 2\n",
    "            val_set, test_set = holdout[:mid], holdout[mid:]\n",
    "        else:\n",
    "            n_total = len(snaps_sorted)\n",
    "            n_test = int(n_total * self.cfg.test_ratio)\n",
    "            n_val = int(n_total * self.cfg.val_ratio)\n",
    "            n_train = n_total - n_val - n_test\n",
    "            train_set = snaps_sorted[:n_train]\n",
    "            val_set = snaps_sorted[n_train : n_train + n_val]\n",
    "            test_set = snaps_sorted[n_train + n_val :]\n",
    "\n",
    "        dl_kw = dict(batch_size=self.cfg.batch_size, shuffle=self.cfg.shuffle_in_split)\n",
    "        self.train_dl = DataLoader(train_set, **dl_kw)\n",
    "        self.val_dl = DataLoader(val_set, batch_size=self.cfg.batch_size, shuffle=False)\n",
    "        self.test_dl = DataLoader(test_set, batch_size=self.cfg.batch_size, shuffle=False)\n",
    "        return self\n",
    "\n",
    "    # ------------------------------------------------------- 3. model\n",
    "    def init_model(self) -> \"GNNExperiment\":\n",
    "        if self.snapshots is None:\n",
    "            raise RuntimeError(\"Call prepare_snapshots() first.\")\n",
    "\n",
    "        d_in = self.snapshots[0].x.size(1)\n",
    "        d_out = 1  # binary logit\n",
    "        edge_dim = self.snapshots[0].edge_attr.size(1)\n",
    "        p_drop = self.cfg.layer_dropout\n",
    "\n",
    "        name = self.cfg.model_name.lower()\n",
    "        if name in {\"simple\", \"baseline\"}:\n",
    "            model = GNNNodeSimple(d_in, self.cfg.hidden_dim, d_out, p_drop)\n",
    "        elif name == \"gcn\":\n",
    "            model = GNNGCN(d_in, self.cfg.hidden_dim, d_out, self.cfg.num_layers, p_drop)\n",
    "        elif name in {\"graphsage\", \"sage\"}:\n",
    "            model = GNNSage(d_in, self.cfg.hidden_dim, d_out, self.cfg.num_layers, p_drop)\n",
    "        elif name == \"gat\":\n",
    "            model = GNNGAT(d_in, self.cfg.hidden_dim, d_out, self.cfg.num_layers, self.cfg.heads, p_drop)\n",
    "        elif name in {\"gatv2\", \"gat2\"}:\n",
    "            model = GNNGAT2(\n",
    "                d_in,\n",
    "                self.cfg.hidden_dim,\n",
    "                d_out,\n",
    "                self.cfg.num_layers,\n",
    "                self.cfg.heads,\n",
    "                edge_dim,\n",
    "                p_drop,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model_name '{self.cfg.model_name}'\")\n",
    "\n",
    "        self.model = model.to(self.device)\n",
    "        return self\n",
    "\n",
    "    # ------------------------------------------------------- 4. compile\n",
    "    def compile(self) -> \"GNNExperiment\":\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"Call init_model() before compile()\")\n",
    "\n",
    "        # ---- loss ------------------------------------------------------\n",
    "        if self.cfg.loss_fn.lower() in {\"bce\", \"binary\"}:\n",
    "            self.loss_fn = torch.nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "            self.metric_fn = torch.nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "        elif self.cfg.loss_fn.lower() == \"cross_entropy\":\n",
    "            self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "            self.metric_fn = self.loss_fn\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported loss_fn '{self.cfg.loss_fn}'\")\n",
    "\n",
    "        # ---- optimiser -------------------------------------------------\n",
    "        opt_kw = dict(lr=self.cfg.lr, **self.cfg.optimiser_kwargs)\n",
    "        opt_name = self.cfg.optimiser.lower()\n",
    "        if opt_name == \"adam\":\n",
    "            self.optimizer = torch.optim.Adam(self.model.parameters(), **opt_kw)\n",
    "        elif opt_name == \"adamw\":\n",
    "            self.optimizer = torch.optim.AdamW(self.model.parameters(), **opt_kw)\n",
    "        elif opt_name == \"sgd\":\n",
    "            self.optimizer = torch.optim.SGD(self.model.parameters(), **opt_kw)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown optimiser '{self.cfg.optimiser}'\")\n",
    "\n",
    "        # ---- scheduler -------------------------------------------------\n",
    "        self.scheduler = None\n",
    "        if self.cfg.lr_scheduler is not None:\n",
    "            sname = self.cfg.lr_scheduler.lower()\n",
    "            kw = self.cfg.lr_scheduler_kwargs\n",
    "            if sname == \"step\":\n",
    "                self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, **kw)\n",
    "            elif sname == \"cosine\":\n",
    "                self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, **kw)\n",
    "            elif sname == \"plateau\":\n",
    "                self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, **kw)\n",
    "            elif sname == \"onecycle\":\n",
    "                self.scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optimizer, **kw)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown lr_scheduler '{self.cfg.lr_scheduler}'\")\n",
    "\n",
    "        # track last LR for pretty print\n",
    "        self._last_lr = self.optimizer.param_groups[0][\"lr\"]\n",
    "        return self\n",
    "\n",
    "    # ------------------------------------------------ edge-dropout helper\n",
    "    def _apply_edge_dropout(self, data: Data) -> Data:\n",
    "        p = getattr(self.cfg, \"edge_dropout\", 0.0)\n",
    "        if p <= 0.0 or not self.model.training:\n",
    "            return data\n",
    "        E = data.edge_index.size(1)\n",
    "        keep_mask = torch.rand(E, device=data.edge_index.device) >= p\n",
    "        if keep_mask.all():\n",
    "            return data\n",
    "        data.edge_index = data.edge_index[:, keep_mask]\n",
    "        if data.edge_attr is not None:\n",
    "            data.edge_attr = data.edge_attr[keep_mask]\n",
    "        return data\n",
    "\n",
    "    # ------------------------------------------------ train-epoch helper\n",
    "    def _train_epoch(self, debug: bool = False) -> float:\n",
    "        self.model.train()\n",
    "        total = 0.0\n",
    "        for step, data in enumerate(self.train_dl):\n",
    "            data = data.to(self.device)\n",
    "            data = self._apply_edge_dropout(data)\n",
    "\n",
    "            data.x = data.x.float()\n",
    "            if data.edge_attr is not None:\n",
    "                data.edge_attr = data.edge_attr.float()\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            logits = self.model(data).squeeze(-1)\n",
    "            loss_vec = self.loss_fn(logits, data.y)\n",
    "            loss_vec = loss_vec * data.node_weight.to(loss_vec.device)\n",
    "            loss = loss_vec.mean()\n",
    "\n",
    "            if debug and (has_nan(loss) or has_nan(logits)):\n",
    "                raise RuntimeError(f\"NaN detected at step {step}\")\n",
    "\n",
    "            loss.backward()\n",
    "            if self.cfg.grad_clip is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.cfg.grad_clip)\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total += loss.item()\n",
    "        return total / len(self.train_dl)\n",
    "\n",
    "    # ------------------------------------------------ eval helper\n",
    "    @torch.no_grad()\n",
    "    def _eval_loader(self, loader: DataLoader) -> Tuple[float, float]:\n",
    "        self.model.eval()\n",
    "        total_loss = total_metric = 0.0\n",
    "        for data in loader:\n",
    "            data = data.to(self.device)\n",
    "            data.x = data.x.float()\n",
    "            if data.edge_attr is not None:\n",
    "                data.edge_attr = data.edge_attr.float()\n",
    "\n",
    "            logits = self.model(data).squeeze(-1)\n",
    "            loss_vec = self.loss_fn(logits, data.y)\n",
    "            metric_vec = self.metric_fn(logits, data.y)\n",
    "\n",
    "            w = data.node_weight.to(loss_vec.device)\n",
    "            total_loss += (loss_vec * w).mean().item()\n",
    "            total_metric += (metric_vec * w).mean().item()\n",
    "        n = len(loader)\n",
    "        return total_loss / n, total_metric / n\n",
    "\n",
    "    # ------------------------------------------------ main training loop\n",
    "    def train(self, debug: bool = False):\n",
    "        if any(v is None for v in (self.model, self.optimizer, self.loss_fn)):\n",
    "            raise RuntimeError(\"Call init_model() and compile() before train()\")\n",
    "\n",
    "        for epoch in range(1, self.cfg.epochs + 1):\n",
    "            tr_loss = self._train_epoch(debug)\n",
    "            val_loss, val_metric = self._eval_loader(self.val_dl)\n",
    "\n",
    "            # scheduler\n",
    "            if self.scheduler is not None:\n",
    "                if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                    self.scheduler.step(val_loss)\n",
    "                else:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "            # LR print\n",
    "            if self.cfg.print_lr_each_epoch:\n",
    "                curr_lr = self.optimizer.param_groups[0][\"lr\"]\n",
    "                lr_msg = f\"LR changed → {curr_lr:.2e}\" if curr_lr != self._last_lr else f\"LR={curr_lr:.2e}\"\n",
    "                self._last_lr = curr_lr\n",
    "            else:\n",
    "                lr_msg = \"\"\n",
    "\n",
    "            # store history\n",
    "            self.history[\"train_loss\"].append(tr_loss)\n",
    "            self.history[\"val_loss\"].append(val_loss)\n",
    "            self.history[\"val_metric\"].append(val_metric)\n",
    "\n",
    "            print(\n",
    "                f\"[Epoch {epoch:03d}/{self.cfg.epochs}] \"\n",
    "                f\"train={tr_loss:.4f}  val={val_loss:.4f}  \"\n",
    "                f\"metric={val_metric:.4f}  {lr_msg}\"\n",
    "            )\n",
    "\n",
    "            # early stopping\n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "                self.patience_counter = 0\n",
    "                self.best_ckpt = {k: v.cpu() for k, v in self.model.state_dict().items()}\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "                if self.patience_counter >= self.cfg.patience:\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "        return self\n",
    "\n",
    "    # ------------------------------------------------ evaluate\n",
    "    def evaluate(self, split: str = \"test\") -> Dict[str, float]:\n",
    "        loader = {\"train\": self.train_dl, \"val\": self.val_dl, \"test\": self.test_dl}.get(split)\n",
    "        if loader is None:\n",
    "            raise ValueError(\"split must be 'train', 'val' or 'test'\")\n",
    "        loss, metric = self._eval_loader(loader)\n",
    "        return {\"loss\": loss, \"metric\": metric}\n",
    "\n",
    "    # ------------------------------------------------ predict\n",
    "    @torch.no_grad()\n",
    "    def predict(\n",
    "        self,\n",
    "        node_frames_new: Dict[str, pd.DataFrame],\n",
    "        edge_frames_new: Dict[str, pd.DataFrame],\n",
    "        timestamps: Optional[List[pd.Timestamp]] = None,\n",
    "        return_df: bool = True,\n",
    "    ):\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"Train or load a model before calling predict()\")\n",
    "\n",
    "        # decide timestamps\n",
    "        if timestamps is None:\n",
    "            idxs = (\n",
    "                [df.index for df in node_frames_new.values()]\n",
    "                + [df.index for df in edge_frames_new.values()]\n",
    "            )\n",
    "            timestamps = sorted(set.intersection(*(set(ix) for ix in idxs)))\n",
    "        else:\n",
    "            timestamps = [pd.Timestamp(ts) for ts in timestamps]\n",
    "\n",
    "        snaps = []\n",
    "        for ts in timestamps:\n",
    "            feats = []\n",
    "            for region in self.reg_order:\n",
    "                feats.append(\n",
    "                    node_frames_new[region]\n",
    "                    .loc[ts]\n",
    "                    .drop(self.cfg.target_col)\n",
    "                    .to_numpy(dtype=np.float32)\n",
    "                )\n",
    "            x = torch.tensor(np.vstack(feats), dtype=torch.float32)\n",
    "\n",
    "            edge_rows = []\n",
    "            for s_idx, d_idx in self.edge_order:\n",
    "                s, d = self.reg_order[s_idx], self.reg_order[d_idx]\n",
    "                edge_rows.append(\n",
    "                    edge_frames_new[f\"{s}-{d}\".lower()].loc[ts].to_numpy(dtype=np.float32)\n",
    "                )\n",
    "            edge_attr = torch.tensor(np.vstack(edge_rows), dtype=torch.float32)\n",
    "            edge_index = torch.tensor(np.array(self.edge_order).T, dtype=torch.long)\n",
    "\n",
    "            snaps.append(Data(x=x, edge_index=edge_index, edge_attr=edge_attr))\n",
    "\n",
    "        loader = DataLoader(snaps, batch_size=self.cfg.batch_size, shuffle=False)\n",
    "        self.model.eval()\n",
    "        preds = []\n",
    "        for data in loader:\n",
    "            data = data.to(self.device)\n",
    "            data.x = data.x.float()\n",
    "            if data.edge_attr is not None:\n",
    "                data.edge_attr = data.edge_attr.float()\n",
    "            logits = self.model(data).squeeze(-1).cpu()\n",
    "            preds.append(logits)\n",
    "\n",
    "        y_hat = torch.cat(preds, dim=0)           # (len(ts)*N)\n",
    "        y_hat = y_hat.reshape(len(timestamps), len(self.reg_order))\n",
    "\n",
    "        if return_df:\n",
    "            return pd.DataFrame(\n",
    "                y_hat.numpy(),\n",
    "                index=pd.to_datetime(timestamps),\n",
    "                columns=self.reg_order,\n",
    "            )\n",
    "        return y_hat\n",
    "\n",
    "    # ------------------------------------------------ plot history\n",
    "    def plot_history(self, metric: str = \"loss\"):\n",
    "        if not self.history[\"train_loss\"]:\n",
    "            raise RuntimeError(\"Nothing in history – did you call train()?\")\n",
    "\n",
    "        if metric == \"loss\":\n",
    "            plt.figure(figsize=(6, 4))\n",
    "            plt.plot(self.history[\"train_loss\"], label=\"Train\")\n",
    "            plt.plot(self.history[\"val_loss\"], label=\"Val\")\n",
    "            plt.xlabel(\"Epoch\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.title(\"Training / Validation Loss\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "\n",
    "        elif metric in {\"metric\", \"mae\", \"accuracy\"}:\n",
    "            if not self.history[\"val_metric\"]:\n",
    "                raise ValueError(\"Metric history empty; choose 'loss' instead.\")\n",
    "            plt.figure(figsize=(6, 4))\n",
    "            plt.plot(self.history[\"val_metric\"], label=f\"Val {metric}\")\n",
    "            plt.xlabel(\"Epoch\")\n",
    "            plt.ylabel(metric.upper())\n",
    "            plt.title(f\"Validation {metric.upper()} Curve\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "        else:\n",
    "            raise ValueError(\"metric must be 'loss' or 'metric'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
