{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c85a0693",
   "metadata": {},
   "source": [
    "### Sequential Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739494a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightning >=2.2\n",
    "import torch, pytorch_lightning as pl\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---------- 1.  Build an iterable Dataset that streams in time order ----------\n",
    "class RollingWindowDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    Yields (X_window, y_target) pairs in chronological order.\n",
    "    Assumes df already sorted by timestamp ascending.\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame, feature_cols, target_col,\n",
    "                 window: int = 24, horizon: int = 1):\n",
    "        self.X = df[feature_cols].values.astype(np.float32)\n",
    "        self.y = df[target_col].values.astype(np.float32)\n",
    "        self.window, self.horizon = window, horizon\n",
    "\n",
    "    def __iter__(self):\n",
    "        # leave the last `horizon` rows unused as inputs\n",
    "        for t in range(self.window, len(self.X) - self.horizon):\n",
    "            X_win = self.X[t - self.window:t]          # shape (window, n_feat)\n",
    "            y_tar = self.y[t + self.horizon - 1]       # scalar regression target\n",
    "            yield torch.from_numpy(X_win), torch.tensor(y_tar)\n",
    "\n",
    "# ---------- 2.  LightningDataModule ----------\n",
    "class TSDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, df, feature_cols, target_col, window=24, horizon=1,\n",
    "                 val_size=0.05, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        df = self.hparams.df\n",
    "        split_idx = int(len(df) * (1 - self.hparams.val_size))\n",
    "        self.train_ds = RollingWindowDataset(df.iloc[:split_idx],\n",
    "                                             self.hparams.feature_cols,\n",
    "                                             self.hparams.target_col,\n",
    "                                             self.hparams.window,\n",
    "                                             self.hparams.horizon)\n",
    "        self.val_ds   = RollingWindowDataset(df.iloc[split_idx:],\n",
    "                                             self.hparams.feature_cols,\n",
    "                                             self.hparams.target_col,\n",
    "                                             self.hparams.window,\n",
    "                                             self.hparams.horizon)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_ds,\n",
    "                          batch_size=self.hparams.batch_size,\n",
    "                          shuffle=False,  # critical!\n",
    "                          drop_last=False)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_ds,\n",
    "                          batch_size=self.hparams.batch_size,\n",
    "                          shuffle=False)\n",
    "\n",
    "# ---------- 3.  LightningModule ----------\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleTSRegressor(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Example network: flatten window → 2-layer MLP → scalar.\n",
    "    Replace with LSTM/Transformer for better TS handling.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, window, lr=1e-3, hidden=128):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        in_dim = n_features * window\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),                       # (B, window*n_feat)\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden),\n",
    "            nn.Linear(hidden, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)           # (B,)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
    "        sched = torch.optim.lr_scheduler.StepLR(opt, step_size=10, gamma=0.5)\n",
    "        return [opt], [sched]\n",
    "\n",
    "# ---------- 4.  Putting it together ----------\n",
    "def run_training(df, feature_cols, target_col):\n",
    "    window = 24          # last 24 timesteps → predict t+1\n",
    "    dm = TSDataModule(df, feature_cols, target_col, window=window,\n",
    "                      horizon=1, val_size=0.05, batch_size=64)\n",
    "    n_feat = len(feature_cols)\n",
    "    model = SimpleTSRegressor(n_features=n_feat, window=window)\n",
    "    trainer = pl.Trainer(max_epochs=50,\n",
    "                         gradient_clip_val=1.0,\n",
    "                         callbacks=[\n",
    "                             pl.callbacks.ModelCheckpoint(\n",
    "                                 monitor=\"val_loss\", save_top_k=3, mode=\"min\"\n",
    "                             )\n",
    "                         ])\n",
    "    trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f74ed6",
   "metadata": {},
   "source": [
    "Loss Logger "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f1fe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"seq_ts\")\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=50,\n",
    "                     logger=logger,              # ⬅️ add logger\n",
    "                     callbacks=[\n",
    "                         pl.callbacks.ModelCheckpoint(\n",
    "                             monitor=\"val_loss\", mode=\"min\", save_top_k=3),\n",
    "                         pl.callbacks.RichProgressBar()  # nicer bar (optional)\n",
    "                     ])\n",
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530e5871",
   "metadata": {},
   "source": [
    "### Choosing different mechanisms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7211854e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "def build_backbone(arch: str,\n",
    "                   n_features: int,\n",
    "                   window: int,\n",
    "                   hidden: int = 128,\n",
    "                   n_layers: int = 2,\n",
    "                   n_heads: int = 4):\n",
    "    \"\"\"\n",
    "    Returns a nn.Module that maps (B, window, n_features) ➜ (B, 1)\n",
    "    Supported arch: \"mlp\", \"lstm\", \"gru\", \"cnn\", \"transformer\"\n",
    "    \"\"\"\n",
    "    arch = arch.lower()\n",
    "\n",
    "    # 1) Plain MLP (baseline we used before)\n",
    "    if arch == \"mlp\":\n",
    "        in_dim = n_features * window\n",
    "        return nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden),\n",
    "            nn.Linear(hidden, 1)\n",
    "        )\n",
    "\n",
    "    # 2) LSTM (take last hidden state)\n",
    "    if arch == \"lstm\":\n",
    "        class LSTMHead(nn.Module):\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "                self.lstm = nn.LSTM(\n",
    "                    input_size=n_features,\n",
    "                    hidden_size=hidden,\n",
    "                    num_layers=n_layers,\n",
    "                    batch_first=True)\n",
    "                self.fc = nn.Linear(hidden, 1)\n",
    "\n",
    "            def forward(self, x):             # x: (B, w, n_feat)\n",
    "                out, _ = self.lstm(x)\n",
    "                return self.fc(out[:, -1])     # last time step\n",
    "        return LSTMHead()\n",
    "\n",
    "    # 3) GRU\n",
    "    if arch == \"gru\":\n",
    "        class GRUHead(nn.Module):\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "                self.gru = nn.GRU(\n",
    "                    input_size=n_features,\n",
    "                    hidden_size=hidden,\n",
    "                    num_layers=n_layers,\n",
    "                    batch_first=True)\n",
    "                self.fc = nn.Linear(hidden, 1)\n",
    "\n",
    "            def forward(self, x):\n",
    "                out, _ = self.gru(x)\n",
    "                return self.fc(out[:, -1])\n",
    "        return GRUHead()\n",
    "\n",
    "    # 4) 1-D Temporal Convolution (Causal)\n",
    "    if arch == \"cnn\":\n",
    "        class CNNHead(nn.Module):\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "                self.conv = nn.Sequential(\n",
    "                    nn.Conv1d(n_features, hidden,\n",
    "                              kernel_size=3, padding=2, dilation=2),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv1d(hidden, hidden,\n",
    "                              kernel_size=3, padding=4, dilation=4),\n",
    "                    nn.ReLU(),\n",
    "                    nn.AdaptiveAvgPool1d(1)\n",
    "                )\n",
    "                self.fc = nn.Linear(hidden, 1)\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = x.permute(0, 2, 1)        # (B, C=n_feat, L=window)\n",
    "                x = self.conv(x).squeeze(-1)   # (B, hidden)\n",
    "                return self.fc(x)\n",
    "        return CNNHead()\n",
    "\n",
    "    # 5) Transformer Encoder (positional embedding + last token)\n",
    "    if arch == \"transformer\":\n",
    "        class TransEncHead(nn.Module):\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "                self.input_proj = nn.Linear(n_features, hidden)\n",
    "                layer = nn.TransformerEncoderLayer(\n",
    "                    d_model=hidden, nhead=n_heads,\n",
    "                    batch_first=True, norm_first=True)\n",
    "                self.encoder = nn.TransformerEncoder(layer, num_layers=n_layers)\n",
    "                # learned positional embedding\n",
    "                self.pos = nn.Parameter(torch.zeros(window, hidden))\n",
    "                nn.init.uniform_(self.pos, -0.02, 0.02)\n",
    "                self.fc = nn.Linear(hidden, 1)\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = self.input_proj(x) + self.pos  # add position\n",
    "                x = self.encoder(x)\n",
    "                return self.fc(x[:, -1])\n",
    "        return TransEncHead()\n",
    "\n",
    "    raise ValueError(f\"Unknown architecture: {arch}\")\n",
    "\n",
    "\n",
    "    ### Calling Function ###\n",
    "    \n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FlexibleTSRegressor(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Same loss-switcher we used before; only backbone replaced by build_backbone.\n",
    "    \"\"\"\n",
    "    def __init__(self, *,\n",
    "                 n_features: int,\n",
    "                 window: int,\n",
    "                 backbone: str = \"mlp\",\n",
    "                 hidden: int = 128,\n",
    "                 loss_name: str = \"mse\",\n",
    "                 lr: float = 1e-3,\n",
    "                 **loss_kw):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # build chosen architecture\n",
    "        self.backbone = build_backbone(backbone,\n",
    "                                       n_features, window,\n",
    "                                       hidden=hidden)\n",
    "\n",
    "        # ----- loss selection (identical to earlier snippet) ------------\n",
    "        ln = loss_name.lower()\n",
    "        if ln == \"mse\":\n",
    "            self.criterion = nn.MSELoss()\n",
    "        elif ln == \"mae\":\n",
    "            self.criterion = nn.L1Loss()\n",
    "        elif ln == \"huber\":\n",
    "            beta = loss_kw.get(\"huber_beta\", 1.0)\n",
    "            self.criterion = nn.SmoothL1Loss(beta=beta)\n",
    "        else:\n",
    "            self.criterion = None             # rmse / quantile handled below\n",
    "        self.loss_name = ln\n",
    "        self.quantile = loss_kw.get(\"quantile\", 0.9)\n",
    "\n",
    "    # ----- forward & loss ------------------------------------------------\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x).squeeze(-1)\n",
    "\n",
    "    def _compute_loss(self, y_hat, y):\n",
    "        if self.loss_name in (\"mse\", \"mae\", \"huber\"):\n",
    "            return self.criterion(y_hat, y)\n",
    "        if self.loss_name == \"rmse\":\n",
    "            return F.mse_loss(y_hat, y).sqrt()\n",
    "        if self.loss_name == \"quantile\":\n",
    "            q = self.quantile\n",
    "            diff = y_hat - y\n",
    "            return torch.where(diff >= 0, q * diff, (q - 1) * diff).mean()\n",
    "        raise RuntimeError\n",
    "\n",
    "    # ----- training & validation ----------------------------------------\n",
    "    def training_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        loss = self._compute_loss(self(x), y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        loss = self._compute_loss(self(x), y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.AdamW(self.parameters(),\n",
    "                                lr=self.hparams.lr)\n",
    "        sch = torch.optim.lr_scheduler.StepLR(opt, 10, gamma=0.5)\n",
    "        return [opt], [sch]\n",
    "\n",
    "# keep your existing RollingWindowDataset and TSDataModule\n",
    "\n",
    "n_feat  = len(feature_cols)\n",
    "window  = 24\n",
    "\n",
    "# ➊ MLP (baseline)\n",
    "model = FlexibleTSRegressor(n_features=n_feat,\n",
    "                            window=window,\n",
    "                            backbone=\"mlp\")\n",
    "\n",
    "# ➋ LSTM\n",
    "model = FlexibleTSRegressor(n_features=n_feat,\n",
    "                            window=window,\n",
    "                            backbone=\"lstm\",\n",
    "                            hidden=256, n_layers=2)\n",
    "\n",
    "# ➌ GRU\n",
    "model = FlexibleTSRegressor(n_features=n_feat,\n",
    "                            window=window,\n",
    "                            backbone=\"gru\",\n",
    "                            hidden=256, n_layers=3)\n",
    "\n",
    "#  Temporal-CNN\n",
    "model = FlexibleTSRegressor(n_features=n_feat,\n",
    "                            window=window,\n",
    "                            backbone=\"cnn\",\n",
    "                            hidden=128)\n",
    "\n",
    "# Transformer Encoder\n",
    "model = FlexibleTSRegressor(n_features=n_feat,\n",
    "                            window=window,\n",
    "                            backbone=\"transformer\",\n",
    "                            hidden=128,\n",
    "                            n_layers=2,\n",
    "                            n_heads=4,\n",
    "                            loss_name=\"quantile\",\n",
    "                            quantile=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c709a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 1. Imports\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "import os, torch, pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 2. Rolling-window Dataset  (indexable so Lightning knows its length)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "class RollingWindowDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame,\n",
    "                 feature_cols, target_col,\n",
    "                 window: int = 24, horizon: int = 1):\n",
    "        self.X  = df[feature_cols].values.astype(\"float32\")\n",
    "        self.y  = df[target_col].values.astype(\"float32\")\n",
    "        self.w  = window\n",
    "        self.h  = horizon\n",
    "        self.indices = range(window, len(df) - horizon)\n",
    "\n",
    "    def __len__(self):               # Lightning can now draw 0/??? bars\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        t = self.indices[idx]\n",
    "        X_win = self.X[t-self.w : t]              # shape (window, n_feat)\n",
    "        y_tar = self.y[t + self.h - 1]            # scalar\n",
    "        return torch.from_numpy(X_win), torch.tensor(y_tar)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 3. DataModule (keeps order, no shuffle!)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "class TSDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, df, feature_cols, target_col,\n",
    "                 window=24, horizon=1,\n",
    "                 val_size=0.05, batch_size=64,\n",
    "                 num_workers=os.cpu_count()//2):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=[\"df\"])\n",
    "        self.df = df.copy()\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        split = int(len(self.df) * (1 - self.hparams.val_size))\n",
    "        train_df = self.df.iloc[:split]\n",
    "        val_df   = self.df.iloc[split:]\n",
    "\n",
    "        self.train_ds = RollingWindowDataset(train_df,\n",
    "                                             self.hparams.feature_cols,\n",
    "                                             self.hparams.target_col,\n",
    "                                             self.hparams.window,\n",
    "                                             self.hparams.horizon)\n",
    "        self.val_ds   = RollingWindowDataset(val_df,\n",
    "                                             self.hparams.feature_cols,\n",
    "                                             self.hparams.target_col,\n",
    "                                             self.hparams.window,\n",
    "                                             self.hparams.horizon)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_ds,\n",
    "                          batch_size=self.hparams.batch_size,\n",
    "                          shuffle=False,\n",
    "                          drop_last=False,\n",
    "                          num_workers=self.hparams.num_workers,\n",
    "                          persistent_workers=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_ds,\n",
    "                          batch_size=self.hparams.batch_size,\n",
    "                          shuffle=False,\n",
    "                          num_workers=self.hparams.num_workers,\n",
    "                          persistent_workers=True)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 4. Simple MLP regressor (swap with LSTM/GRU later if you like)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "class SimpleTSRegressor(pl.LightningModule):\n",
    "    def __init__(self, n_features, window, hidden=128, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(window * n_features, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden),\n",
    "            nn.Linear(hidden, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "    def training_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        loss = F.mse_loss(self(x), y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        loss = F.mse_loss(self(x), y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt  = torch.optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
    "        sched = torch.optim.lr_scheduler.StepLR(opt, 10, gamma=0.5)\n",
    "        return [opt], [sched]\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 5. One-shot training + return best checkpoint path\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def train_and_get_best_ckpt(df, feature_cols, target_col,\n",
    "                            window=24, horizon=1):\n",
    "    dm = TSDataModule(df, feature_cols, target_col,\n",
    "                      window=window, horizon=horizon)\n",
    "\n",
    "    model = SimpleTSRegressor(n_features=len(feature_cols), window=window)\n",
    "\n",
    "    ckpt_cb = pl.callbacks.ModelCheckpoint(\n",
    "        monitor=\"val_loss\", mode=\"min\", save_top_k=1)\n",
    "\n",
    "    logger = TensorBoardLogger(\"lightning_logs\", name=\"seq_ts\")\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=50,\n",
    "        accelerator=\"auto\",\n",
    "        devices=\"auto\",\n",
    "        logger=logger,\n",
    "        callbacks=[ckpt_cb, pl.callbacks.RichProgressBar()])\n",
    "    trainer.fit(model, dm)\n",
    "\n",
    "    return ckpt_cb.best_model_path\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 6. ==== RUN TRAINING =========================================================\n",
    "# df, feature_cols, target_col must already exist in your interpreter\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "ckpt_path = train_and_get_best_ckpt(df, feature_cols, target_col)\n",
    "print(\"Best model checkpoint ➜\", ckpt_path)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 7. Load best model & make predictions on (for example) the validation slice\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "best_model = SimpleTSRegressor.load_from_checkpoint(\n",
    "                 ckpt_path,\n",
    "                 n_features=len(feature_cols),\n",
    "                 window=24).eval()\n",
    "\n",
    "val_ds = RollingWindowDataset(df.iloc[int(len(df)*0.95):],\n",
    "                              feature_cols, target_col,\n",
    "                              window=24, horizon=1)\n",
    "val_dl = DataLoader(val_ds, batch_size=64, shuffle=False)\n",
    "\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for x, _ in val_dl:\n",
    "        preds.append(best_model(x).cpu())\n",
    "preds = torch.cat(preds).numpy()\n",
    "print(\"Inference done. 1st five predictions:\", preds[:5])\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 8. Launch TensorBoard (run this **in a terminal**, not inside Python)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "#   tensorboard --logdir lightning_logs\n",
    "# Then open the printed URL in your browser to see train/val loss curves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5147a2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard --logdir lightning_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18e19f5",
   "metadata": {},
   "source": [
    "### New NN layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd6be3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary_classifier.py\n",
    "\n",
    "import yaml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class BinaryClassifier:\n",
    "    def __init__(self, config: dict):\n",
    "        \"\"\"\n",
    "        config should contain:\n",
    "          - 'train_end', 'val_end' (YYYY-MM-DD strings)\n",
    "          - optional 'test_end'\n",
    "          - 'batch_size', 'epochs', 'learning_rate'\n",
    "          - 'hidden_size', 'pos_weight' (float)\n",
    "        \"\"\"\n",
    "        self.cfg = config\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.history = {'train_loss': [], 'val_loss': []}\n",
    "\n",
    "    def load_data(self, df: pd.DataFrame, target_col: str):\n",
    "        # ensure datetime index\n",
    "        if not isinstance(df.index, pd.DatetimeIndex):\n",
    "            raise ValueError(\"DataFrame must have a DatetimeIndex\")\n",
    "        # split by date\n",
    "        t0 = pd.to_datetime(self.cfg['train_end'])\n",
    "        v0 = pd.to_datetime(self.cfg['val_end'])\n",
    "        df_train = df.loc[:t0]\n",
    "        df_val   = df.loc[t0 + pd.Timedelta(days=1): v0]\n",
    "        if 'test_end' in self.cfg:\n",
    "            t1 = pd.to_datetime(self.cfg['test_end'])\n",
    "            df_test = df.loc[v0 + pd.Timedelta(days=1): t1]\n",
    "        else:\n",
    "            df_test = df.loc[v0 + pd.Timedelta(days=1):]\n",
    "\n",
    "        # separate X/y\n",
    "        def split(df_):\n",
    "            X = df_.drop(columns=[target_col]).values.astype(float)\n",
    "            y = df_[target_col].values.astype(float)\n",
    "            return X, y\n",
    "\n",
    "        X_train, y_train = split(df_train)\n",
    "        X_val,   y_val   = split(df_val)\n",
    "        X_test,  y_test  = split(df_test)\n",
    "\n",
    "        # scale features\n",
    "        X_train = self.scaler.fit_transform(X_train)\n",
    "        X_val   = self.scaler.transform(X_val)\n",
    "        X_test  = self.scaler.transform(X_test)\n",
    "\n",
    "        # wrap in DataLoader\n",
    "        bs = self.cfg.get('batch_size', 64)\n",
    "        def make_loader(X, y, shuffle):\n",
    "            tX = torch.tensor(X, dtype=torch.float32)\n",
    "            ty = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "            ds = TensorDataset(tX, ty)\n",
    "            return DataLoader(ds, batch_size=bs, shuffle=shuffle)\n",
    "        self.train_loader = make_loader(X_train, y_train, shuffle=True)\n",
    "        self.val_loader   = make_loader(X_val,   y_val,   shuffle=False)\n",
    "        self.test_loader  = make_loader(X_test,  y_test,  shuffle=False)\n",
    "\n",
    "    def _build_model(self, input_dim: int):\n",
    "        hs = self.cfg.get('hidden_size', 32)\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hs),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hs, 1),\n",
    "            nn.Sigmoid()\n",
    "        ).to(self.device)\n",
    "        self.model = model\n",
    "\n",
    "        # weighted BCE\n",
    "        pw = self.cfg.get('pos_weight', 1.0)\n",
    "        weight = torch.tensor([pw], dtype=torch.float32, device=self.device)\n",
    "        self.criterion = nn.BCEWithLogitsLoss(pos_weight=weight)\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=self.cfg.get('learning_rate', 1e-3))\n",
    "\n",
    "    def train(self):\n",
    "        if self.model is None:\n",
    "            # assume load_data called\n",
    "            input_dim = next(iter(self.train_loader))[0].shape[1]\n",
    "            self._build_model(input_dim)\n",
    "\n",
    "        epochs = self.cfg.get('epochs', 10)\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            self.model.train()\n",
    "            total_loss = 0.0\n",
    "            for Xb, yb in self.train_loader:\n",
    "                Xb, yb = Xb.to(self.device), yb.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                logits = self.model[0:3](Xb)  # before sigmoid\n",
    "                loss = self.criterion(logits, yb)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                total_loss += loss.item() * Xb.size(0)\n",
    "            avg_train = total_loss / len(self.train_loader.dataset)\n",
    "\n",
    "            # validation\n",
    "            self.model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for Xb, yb in self.val_loader:\n",
    "                    Xb, yb = Xb.to(self.device), yb.to(self.device)\n",
    "                    logits = self.model[0:3](Xb)\n",
    "                    val_loss += self.criterion(logits, yb).item() * Xb.size(0)\n",
    "            avg_val = val_loss / len(self.val_loader.dataset)\n",
    "\n",
    "            self.history['train_loss'].append(avg_train)\n",
    "            self.history['val_loss'].append(avg_val)\n",
    "            print(f\"Epoch {epoch}/{epochs} — train_loss: {avg_train:.4f}, val_loss: {avg_val:.4f}\")\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        preds_list, true_list = [], []\n",
    "        with torch.no_grad():\n",
    "            for Xb, yb in self.test_loader:\n",
    "                Xb, yb = Xb.to(self.device), yb.to(self.device)\n",
    "                logits = self.model[0:3](Xb)\n",
    "                loss = self.criterion(logits, yb)\n",
    "                total_loss += loss.item() * Xb.size(0)\n",
    "                probs = torch.sigmoid(logits)\n",
    "                preds = (probs > 0.5).float()\n",
    "                correct += (preds == yb).sum().item()\n",
    "                total += Xb.size(0)\n",
    "                preds_list.extend(probs.cpu().numpy().flatten())\n",
    "                true_list.extend(yb.cpu().numpy().flatten())\n",
    "\n",
    "        avg_loss = total_loss / total\n",
    "        accuracy = correct / total\n",
    "        print(f\"Test loss: {avg_loss:.4f}, Accuracy: {accuracy:.4%}\")\n",
    "        return {'loss': avg_loss, 'accuracy': accuracy, 'preds': preds_list, 'true': true_list}\n",
    "\n",
    "    def predict(self, df_new: pd.DataFrame) -> pd.Series:\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"Model has not been trained yet\")\n",
    "        X = df_new.copy()\n",
    "        if not isinstance(X.index, pd.DatetimeIndex):\n",
    "            X.index = pd.to_datetime(X.index)\n",
    "        X = self.scaler.transform(X.values)\n",
    "        tX = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = self.model[0:3](tX)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy().flatten()\n",
    "        return pd.Series(probs, index=df_new.index, name='predicted_proba')\n",
    "\n",
    "    def plot_history(self):\n",
    "        plt.figure()\n",
    "        plt.plot(self.history['train_loss'], label='train loss')\n",
    "        plt.plot(self.history['val_loss'],   label='val loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.title('Training History')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Example usage script\n",
    "if __name__ == \"__main__\":\n",
    "    import pandas as pd\n",
    "\n",
    "    # 1) Define your config as a Python dict:\n",
    "    config = {\n",
    "        'train_end'    : '2020-12-31',\n",
    "        'val_end'      : '2021-06-30',\n",
    "        # 'test_end'   : '2021-12-31',  # optional\n",
    "        'batch_size'   : 128,\n",
    "        'epochs'       : 20,\n",
    "        'learning_rate': 1e-3,\n",
    "        'hidden_size'  : 64,\n",
    "        'pos_weight'   : 5.0,\n",
    "    }\n",
    "\n",
    "    # 2) Load your dataframe (assumes first column is datetime index)\n",
    "    df = pd.read_csv('your_data.csv', parse_dates=[0], index_col=0)\n",
    "\n",
    "    # 3) Instantiate, load/split, train, evaluate, predict, and plot:\n",
    "    clf = BinaryClassifier(config)\n",
    "    clf.load_data(df, target_col='your_target_column')\n",
    "    clf.train()\n",
    "    test_results = clf.evaluate()\n",
    "    print(test_results)\n",
    "    \n",
    "    # 4) Predict on new unseen data (no target column):\n",
    "    new_df = pd.read_csv('new_data.csv', parse_dates=[0], index_col=0)\n",
    "    preds = clf.predict(new_df)\n",
    "    print(preds.head())\n",
    "\n",
    "    # 5) Visualize training history:\n",
    "    clf.plot_history()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c838e9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary_classifier.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class BinaryClassifier:\n",
    "    def __init__(self, config: dict):\n",
    "        \"\"\"\n",
    "        config keys:\n",
    "          - 'train_end', 'val_end' (YYYY-MM-DD strings)\n",
    "          - optional 'test_end'\n",
    "          - 'batch_size', 'epochs', 'learning_rate'\n",
    "          - 'hidden_sizes': list of ints for each hidden layer\n",
    "          - 'dropout': float probability\n",
    "          - 'activation': 'relu' or 'tanh'\n",
    "          - 'pos_weight': float\n",
    "          - 'optimizer': 'adam' or 'sgd'\n",
    "          - 'weight_decay': float\n",
    "          - optional 'momentum' (for SGD)\n",
    "          - 'threshold': float in [0,1] for classification\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_cols = []\n",
    "        self.history = {'train_loss': [], 'val_loss': []}\n",
    "        self.threshold = config.get('threshold', 0.5)\n",
    "\n",
    "    def load_data(self, df: pd.DataFrame, target_col: str):\n",
    "        # ensure datetime index\n",
    "        if not isinstance(df.index, pd.DatetimeIndex):\n",
    "            raise ValueError(\"DataFrame index must be a pd.DatetimeIndex for splitting.\")\n",
    "        df = df.sort_index().dropna()\n",
    "\n",
    "        # split by dates\n",
    "        t0 = pd.to_datetime(self.config['train_end'])\n",
    "        v0 = pd.to_datetime(self.config['val_end'])\n",
    "        df_train = df.loc[:t0]\n",
    "        df_val   = df.loc[t0 + pd.Timedelta(days=1): v0]\n",
    "        if 'test_end' in self.config:\n",
    "            t1 = pd.to_datetime(self.config['test_end'])\n",
    "            df_test = df.loc[v0 + pd.Timedelta(days=1): t1]\n",
    "        else:\n",
    "            df_test = df.loc[v0 + pd.Timedelta(days=1):]\n",
    "\n",
    "        # save columns\n",
    "        self.target_col = target_col\n",
    "        self.feature_cols = [c for c in df.columns if c != target_col]\n",
    "\n",
    "        # split features/targets\n",
    "        def split(df_):\n",
    "            X = df_[self.feature_cols].values.astype(float)\n",
    "            y = df_[target_col].values.astype(float).reshape(-1, 1)\n",
    "            return X, y\n",
    "\n",
    "        X_train, y_train = split(df_train)\n",
    "        X_val,   y_val   = split(df_val)\n",
    "        X_test,  y_test  = split(df_test)\n",
    "\n",
    "        # scale features\n",
    "        X_train = self.scaler.fit_transform(X_train)\n",
    "        X_val   = self.scaler.transform(X_val)\n",
    "        X_test  = self.scaler.transform(X_test)\n",
    "\n",
    "        # create DataLoaders\n",
    "        bs = self.config.get('batch_size', 64)\n",
    "        def make_loader(X, y, shuffle):\n",
    "            tX = torch.tensor(X, dtype=torch.float32)\n",
    "            ty = torch.tensor(y, dtype=torch.float32)\n",
    "            ds = TensorDataset(tX, ty)\n",
    "            return DataLoader(ds, batch_size=bs, shuffle=shuffle)\n",
    "\n",
    "        self.train_loader = make_loader(X_train, y_train, shuffle=True)\n",
    "        self.val_loader   = make_loader(X_val,   y_val,   shuffle=False)\n",
    "        self.test_loader  = make_loader(X_test,  y_test,  shuffle=False)\n",
    "\n",
    "    def _build_model(self, input_dim: int):\n",
    "        hs_list = self.config.get('hidden_sizes', [self.config.get('hidden_size', 32)])\n",
    "        dropout_p = self.config.get('dropout', 0.5)\n",
    "        act = self.config.get('activation', 'relu').lower()\n",
    "\n",
    "        layers = []\n",
    "        in_dim = input_dim\n",
    "        for hs in hs_list:\n",
    "            layers.append(nn.Linear(in_dim, hs))\n",
    "            if act == 'tanh':\n",
    "                layers.append(nn.Tanh())\n",
    "            else:\n",
    "                layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_p))\n",
    "            in_dim = hs\n",
    "        layers.append(nn.Linear(in_dim, 1))  # output logits\n",
    "\n",
    "        self.model = nn.Sequential(*layers).to(self.device)\n",
    "\n",
    "        # loss\n",
    "        pw = torch.tensor([self.config.get('pos_weight', 1.0)], device=self.device)\n",
    "        self.criterion = nn.BCEWithLogitsLoss(pos_weight=pw)\n",
    "\n",
    "        # optimizer\n",
    "        opt_name = self.config.get('optimizer', 'adam').lower()\n",
    "        lr = self.config.get('learning_rate', 1e-3)\n",
    "        wd = self.config.get('weight_decay', 0.0)\n",
    "        if opt_name == 'sgd':\n",
    "            momentum = self.config.get('momentum', 0.9)\n",
    "            self.optimizer = torch.optim.SGD(self.model.parameters(), lr=lr,\n",
    "                                             momentum=momentum, weight_decay=wd)\n",
    "        else:\n",
    "            self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr,\n",
    "                                              weight_decay=wd)\n",
    "\n",
    "    def train(self):\n",
    "        # build model if not done\n",
    "        if not hasattr(self, 'model') or self.model is None:\n",
    "            inp_dim = next(iter(self.train_loader))[0].shape[1]\n",
    "            self._build_model(inp_dim)\n",
    "\n",
    "        epochs = self.config.get('epochs', 10)\n",
    "        for epoch in range(1, epochs+1):\n",
    "            # training\n",
    "            self.model.train()\n",
    "            running_loss = 0.0\n",
    "            for Xb, yb in self.train_loader:\n",
    "                Xb, yb = Xb.to(self.device), yb.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                logits = self.model(Xb)\n",
    "                loss = self.criterion(logits, yb)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                running_loss += loss.item() * Xb.size(0)\n",
    "            train_loss = running_loss / len(self.train_loader.dataset)\n",
    "\n",
    "            # validation\n",
    "            self.model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for Xb, yb in self.val_loader:\n",
    "                    Xb, yb = Xb.to(self.device), yb.to(self.device)\n",
    "                    logits = self.model(Xb)\n",
    "                    val_loss += self.criterion(logits, yb).item() * Xb.size(0)\n",
    "            val_loss /= len(self.val_loader.dataset)\n",
    "\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            print(f\"Epoch {epoch}/{epochs} - train_loss: {train_loss:.4f}, val_loss: {val_loss:.4f}\")\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        total_loss, total_correct, total = 0.0, 0, 0\n",
    "        all_preds, all_true = [], []\n",
    "        with torch.no_grad():\n",
    "            for Xb, yb in self.test_loader:\n",
    "                Xb, yb = Xb.to(self.device), yb.to(self.device)\n",
    "                logits = self.model(Xb)\n",
    "                total_loss += self.criterion(logits, yb).item() * Xb.size(0)\n",
    "                probs = torch.sigmoid(logits)\n",
    "                preds = (probs >= self.threshold).float()\n",
    "                total_correct += (preds == yb).sum().item()\n",
    "                total += Xb.size(0)\n",
    "                all_preds.extend(probs.cpu().numpy().flatten())\n",
    "                all_true.extend(yb.cpu().numpy().flatten())\n",
    "\n",
    "        avg_loss = total_loss / total\n",
    "        accuracy = total_correct / total\n",
    "        print(f\"Test_loss: {avg_loss:.4f}, Accuracy: {accuracy:.4%}\")\n",
    "        return {\n",
    "            'loss': avg_loss,\n",
    "            'accuracy': accuracy,\n",
    "            'predictions': np.array(all_preds),\n",
    "            'true': np.array(all_true)\n",
    "        }\n",
    "\n",
    "    def predict(self, df_new: pd.DataFrame) -> pd.Series:\n",
    "        if not hasattr(self, 'model') or self.model is None:\n",
    "            raise RuntimeError(\"Model is not trained yet.\")\n",
    "        if not isinstance(df_new.index, pd.DatetimeIndex):\n",
    "            df_new = df_new.copy()\n",
    "            df_new.index = pd.to_datetime(df_new.index)\n",
    "        X = df_new[self.feature_cols].values.astype(float)\n",
    "        X = self.scaler.transform(X)\n",
    "        tX = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            probs = torch.sigmoid(self.model(tX)).cpu().numpy().flatten()\n",
    "        return pd.Series(probs, index=df_new.index, name='prediction')\n",
    "\n",
    "    def plot_history(self):\n",
    "        if not self.history['train_loss']:\n",
    "            raise RuntimeError(\"No training history. Call train() first.\")\n",
    "        plt.figure()\n",
    "        plt.plot(self.history['train_loss'], label='train_loss')\n",
    "        plt.plot(self.history['val_loss'],   label='val_loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.title('Training vs Validation Loss')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define config dictionary\n",
    "    config = {\n",
    "        'train_end':     '2020-12-31',\n",
    "        'val_end':       '2021-06-30',\n",
    "        # 'test_end':    '2021-12-31',        # optional\n",
    "        'batch_size':    128,\n",
    "        'epochs':        20,\n",
    "        'learning_rate': 1e-3,\n",
    "        'weight_decay':  1e-4,\n",
    "        'optimizer':     'adam',\n",
    "        'hidden_sizes':  [64, 32, 16],\n",
    "        'dropout':       0.5,\n",
    "        'activation':    'relu',\n",
    "        'pos_weight':    5.0,\n",
    "        'threshold':     0.5\n",
    "    }\n",
    "\n",
    "    # Load your DataFrame (first column must be datetime index)\n",
    "    df = pd.read_csv('your_data.csv', parse_dates=[0], index_col=0)\n",
    "\n",
    "    # Initialize, load/split, train, evaluate, predict, plot\n",
    "    clf = BinaryClassifier(config)\n",
    "    clf.load_data(df, target_col='target')\n",
    "    clf.train()\n",
    "    results = clf.evaluate()\n",
    "    print(results)\n",
    "\n",
    "    # Predict on new data\n",
    "    new_df = pd.read_csv('new_data.csv', parse_dates=[0], index_col=0)\n",
    "    preds = clf.predict(new_df)\n",
    "    print(preds.head())\n",
    "\n",
    "    # Plot loss curves\n",
    "    clf.plot_history()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
