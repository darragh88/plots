{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "518c29aa",
   "metadata": {},
   "source": [
    "### DataLoading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5211df25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Literal, Union\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Module-level docs & type aliases\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "Generic loaders & converters for regional-timeseries data.\n",
    "\n",
    "Canonical representation\n",
    "------------------------\n",
    "    CanonicalData = dict[str, pd.DataFrame]\n",
    "        key   – region name\n",
    "        value – DataFrame indexed by datetime; columns = features\n",
    "\n",
    "Disk layouts we support\n",
    "-----------------------\n",
    "    \"multi\" : MultiIndex columns (level-0 = region, level-1 = feature)\n",
    "    \"long\"  : Normal DataFrame with a *region* column\n",
    "    \"wide\"  : Wide DataFrame whose column names are \"<region><sep><feature>\"\n",
    "\"\"\"\n",
    "\n",
    "CanonicalData = Dict[str, pd.DataFrame]\n",
    "Layout        = Literal[\"multi\", \"long\", \"wide\"]\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# ─── Converters: DISK ➜ CANONICAL ────────────────────────────────────────────\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def _multi_to_dict(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    cols: List[str] | None = None\n",
    ") -> CanonicalData:\n",
    "    \"\"\"\n",
    "    Convert a Multi-Index column DataFrame to the canonical dict form.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df   : DataFrame with two column levels (region, feature)\n",
    "    cols : optional list of feature names to *keep* (others are dropped)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, DataFrame]  mapping region → feature-matrix\n",
    "    \"\"\"\n",
    "    out: CanonicalData = {}\n",
    "    for region in df.columns.get_level_values(0).unique():\n",
    "        sub = df.xs(region, axis=1, level=0)\n",
    "        if cols is not None:\n",
    "            sub = sub[[c for c in cols if c in sub.columns]]\n",
    "        out[region] = sub.copy()\n",
    "    return out\n",
    "\n",
    "\n",
    "def _long_to_dict(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    region_col: str = \"region\",\n",
    "    cols: List[str] | None = None,\n",
    ") -> CanonicalData:\n",
    "    \"\"\"\n",
    "    Convert a ‘long’ (tidy) DataFrame to canonical dict form.\n",
    "\n",
    "    A *long* frame must contain a `region_col` column; all other columns\n",
    "    are interpreted as features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df         : tidy DataFrame with duplicate timestamps per region\n",
    "    region_col : column that identifies the region\n",
    "    cols       : optional feature filter\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Canonical dict keyed by region\n",
    "    \"\"\"\n",
    "    if region_col not in df.columns:\n",
    "        raise KeyError(f\"Expected column '{region_col}' in long-layout dataframe.\")\n",
    "\n",
    "    features = [c for c in df.columns if c != region_col]\n",
    "    if cols is not None:\n",
    "        features = [c for c in features if c in cols]\n",
    "\n",
    "    out: CanonicalData = {}\n",
    "    for region, grp in df.groupby(region_col):\n",
    "        frame = grp.drop(columns=region_col)\n",
    "        out[region] = frame[features].copy()\n",
    "    return out\n",
    "\n",
    "\n",
    "def _wide_to_dict(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    sep: str = \"-\",\n",
    "    cols: List[str] | None = None,\n",
    ") -> CanonicalData:\n",
    "    \"\"\"\n",
    "    Convert a wide DataFrame with ‘region-feature’ column names into canon dict.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df   : DataFrame whose columns look like \"tokyo-demand\"\n",
    "    sep  : separator between region and feature\n",
    "    cols : optional feature filter\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Canonical dict keyed by region\n",
    "    \"\"\"\n",
    "    parts = df.columns.to_series().str.split(sep, n=1, expand=True)\n",
    "    if parts.isna().any().any():\n",
    "        raise ValueError(\n",
    "            f\"Column names do not all match <region>{sep}<feature> pattern.\"\n",
    "        )\n",
    "\n",
    "    out: CanonicalData = {}\n",
    "    for region in parts[0].unique():\n",
    "        mask      = parts[0] == region\n",
    "        sub_cols  = df.columns[mask]\n",
    "        features  = parts[1][mask]\n",
    "        sub_frame = df[sub_cols].copy()\n",
    "        sub_frame.columns = features\n",
    "        if cols is not None:\n",
    "            sub_frame = sub_frame[[c for c in cols if c in sub_frame.columns]]\n",
    "        out[region] = sub_frame\n",
    "    return out\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# ─── Converters: CANONICAL ➜ DISK ────────────────────────────────────────────\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def _dict_to_multi(data: CanonicalData) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combine a canonical dict into a Multi-Index column DataFrame.\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    for region, df in data.items():\n",
    "        tmp = df.copy()\n",
    "        tmp.columns = pd.MultiIndex.from_product([[region], tmp.columns])\n",
    "        frames.append(tmp)\n",
    "    return pd.concat(frames, axis=1).sort_index(axis=1)\n",
    "\n",
    "\n",
    "def _dict_to_long(\n",
    "    data: CanonicalData,\n",
    "    *,\n",
    "    region_col: str = \"region\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combine a canonical dict into a long (tidy) DataFrame.\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    for region, df in data.items():\n",
    "        tmp = df.copy()\n",
    "        tmp[region_col] = region\n",
    "        frames.append(tmp)\n",
    "    combined = pd.concat(frames)\n",
    "    order = [c for c in combined.columns if c != region_col] + [region_col]\n",
    "    return combined[order]\n",
    "\n",
    "\n",
    "def _dict_to_wide(\n",
    "    data: CanonicalData,\n",
    "    *,\n",
    "    sep: str = \"-\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combine a canonical dict into wide ‘region-feature’ columns.\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    for region, df in data.items():\n",
    "        tmp = df.copy()\n",
    "        tmp.columns = [f\"{region}{sep}{c}\" for c in tmp.columns]\n",
    "        frames.append(tmp)\n",
    "    return pd.concat(frames, axis=1)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# ─── Public helpers: one-file load / convert ────────────────────────────────\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def load_parquet_as_canonical(\n",
    "    path: str | Path,\n",
    "    *,\n",
    "    layout: Layout,\n",
    "    region_col: str = \"region\",\n",
    "    sep: str = \"-\",\n",
    "    cols: List[str] | None = None,\n",
    ") -> CanonicalData:\n",
    "    \"\"\"\n",
    "    Read a parquet file of known *layout* and return canonical dict form.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path       : file path\n",
    "    layout     : \"multi\", \"long\", or \"wide\" (must match the file)\n",
    "    region_col : name of the region column for *long* layout\n",
    "    sep        : region-feature separator for *wide* layout\n",
    "    cols       : optional feature subset\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    CanonicalData\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(path)\n",
    "    if layout == \"multi\":\n",
    "        return _multi_to_dict(df, cols=cols)\n",
    "    if layout == \"long\":\n",
    "        return _long_to_dict(df, region_col=region_col, cols=cols)\n",
    "    if layout == \"wide\":\n",
    "        return _wide_to_dict(df, sep=sep, cols=cols)\n",
    "    raise ValueError(f\"Unsupported layout '{layout}'.\")\n",
    "\n",
    "\n",
    "def canonical_to_layout(\n",
    "    data: CanonicalData,\n",
    "    layout: Layout,\n",
    "    *,\n",
    "    region_col: str = \"region\",\n",
    "    sep: str = \"-\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert *canonical* dict back to the requested layout format.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data       : canonical dict\n",
    "    layout     : target layout (\"multi\", \"long\", \"wide\")\n",
    "    region_col : name of region column for long layout\n",
    "    sep        : separator for wide layout\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame in the specified layout\n",
    "    \"\"\"\n",
    "    if layout == \"multi\":\n",
    "        return _dict_to_multi(data)\n",
    "    if layout == \"long\":\n",
    "        return _dict_to_long(data, region_col=region_col)\n",
    "    if layout == \"wide\":\n",
    "        return _dict_to_wide(data, sep=sep)\n",
    "    raise ValueError(f\"Unsupported target layout '{layout}'.\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# ─── Helpers for multi-file ingestion ───────────────────────────────────────\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def _collect_by_layout(\n",
    "    paths: List[str] | None,\n",
    "    layout: Layout,\n",
    "    *,\n",
    "    cols: List[str] | None,\n",
    "    region_col: str,\n",
    "    sep: str,\n",
    ") -> Dict[str, List[pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Internal helper: load every file in `paths` (given its layout)\n",
    "    and collate DataFrames by region.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[region, list[DataFrame]]\n",
    "    \"\"\"\n",
    "    bucket: Dict[str, List[pd.DataFrame]] = {}\n",
    "    for p in paths or []:\n",
    "        data = load_parquet_as_canonical(\n",
    "            p, layout=layout, region_col=region_col, sep=sep, cols=cols\n",
    "        )\n",
    "        for region, df in data.items():\n",
    "            bucket.setdefault(region, []).append(df)\n",
    "    return bucket\n",
    "\n",
    "\n",
    "def load_region_dataset(\n",
    "    *,\n",
    "    region: str,\n",
    "    multi_paths: List[str] | None = None,\n",
    "    long_paths: List[str] | None = None,\n",
    "    wide_paths: List[str] | None = None,\n",
    "    cols: List[str] | None = None,\n",
    "    region_col: str = \"region\",\n",
    "    sep: str = \"-\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate **one** region’s data from any number of parquet files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    region      : name of the region to extract\n",
    "    multi_paths : list of files in *multi* layout\n",
    "    long_paths  : list of files in *long* layout\n",
    "    wide_paths  : list of files in *wide* layout\n",
    "    cols        : optional feature subset\n",
    "    region_col  : region column (long layout)\n",
    "    sep         : separator (wide layout)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame with union of timestamps & features for that region\n",
    "    (duplicate columns are de-duplicated, keeping last-read version).\n",
    "    \"\"\"\n",
    "    frames: List[pd.DataFrame] = []\n",
    "    for layout, paths in (\n",
    "        (\"multi\", multi_paths),\n",
    "        (\"long\", long_paths),\n",
    "        (\"wide\", wide_paths),\n",
    "    ):\n",
    "        for p in paths or []:\n",
    "            data = load_parquet_as_canonical(\n",
    "                p, layout=layout, region_col=region_col, sep=sep, cols=cols\n",
    "            )\n",
    "            if region in data:\n",
    "                frames.append(data[region])\n",
    "\n",
    "    if not frames:\n",
    "        raise KeyError(f\"Region '{region}' not found in supplied paths.\")\n",
    "\n",
    "    combined = pd.concat(frames, axis=1)\n",
    "    combined = combined.loc[:, ~combined.columns.duplicated()]  # keep last duplicate\n",
    "    combined.sort_index(inplace=True)\n",
    "    return combined\n",
    "\n",
    "\n",
    "def load_all_regions_dataset(\n",
    "    *,\n",
    "    multi_paths: List[str] | None = None,\n",
    "    long_paths: List[str] | None = None,\n",
    "    wide_paths: List[str] | None = None,\n",
    "    regions: List[str] | None = None,\n",
    "    cols: List[str] | None = None,\n",
    "    region_col: str = \"region\",\n",
    "    sep: str = \"-\",\n",
    ") -> CanonicalData:\n",
    "    \"\"\"\n",
    "    Load **every** region (or a specified subset) from the supplied file lists.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    multi_paths : parquet files in *multi* layout\n",
    "    long_paths  : parquet files in *long* layout\n",
    "    wide_paths  : parquet files in *wide* layout\n",
    "    regions     : optional subset of region names to return\n",
    "    cols        : optional feature subset\n",
    "    region_col  : region column (long layout)\n",
    "    sep         : separator (wide layout)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    CanonicalData containing one DataFrame per region.\n",
    "    \"\"\"\n",
    "    # Read & collate per-layout\n",
    "    bucket: Dict[str, List[pd.DataFrame]] = {}\n",
    "\n",
    "    for paths, layout in (\n",
    "        (multi_paths, \"multi\"),\n",
    "        (long_paths, \"long\"),\n",
    "        (wide_paths, \"wide\"),\n",
    "    ):\n",
    "        part = _collect_by_layout(\n",
    "            paths,\n",
    "            layout,\n",
    "            cols=cols,\n",
    "            region_col=region_col,\n",
    "            sep=sep,\n",
    "        )\n",
    "        for region, frames in part.items():\n",
    "            bucket.setdefault(region, []).extend(frames)\n",
    "\n",
    "    # Filter to requested subset (if any)\n",
    "    if regions is not None:\n",
    "        bucket = {r: bucket[r] for r in regions if r in bucket}\n",
    "\n",
    "    # Merge frames per region\n",
    "    out: CanonicalData = {}\n",
    "    for region, frames in bucket.items():\n",
    "        df = pd.concat(frames, axis=1)\n",
    "        df = df.loc[:, ~df.columns.duplicated()]\n",
    "        df.sort_index(inplace=True)\n",
    "        out[region] = df\n",
    "\n",
    "    # Handle missing regions gracefully\n",
    "    if regions is not None:\n",
    "        missing = set(regions) - set(out)\n",
    "        if missing:\n",
    "            raise KeyError(\n",
    "                f\"The following requested regions were not found in any file: \"\n",
    "                f\"{', '.join(sorted(missing))}\"\n",
    "            )\n",
    "\n",
    "    return out\n",
    "#88888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Literal\n",
    "import pandas as pd\n",
    "\n",
    "# -------------  Canonical type & layout tags  --------------------------------\n",
    "CanonicalData = Dict[str, pd.DataFrame]\n",
    "Layout        = Literal[\"multi\", \"long\", \"wide\"]\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# ═════════════════════════════ I/O CONVERSION ════════════════════════════════\n",
    "# (unchanged from earlier — trimmed for brevity; keep the same helper fns:\n",
    "#     _multi_to_dict, _long_to_dict, _wide_to_dict,\n",
    "#     _dict_to_multi,  _dict_to_long,  _dict_to_wide,\n",
    "#     load_parquet_as_canonical, canonical_to_layout)\n",
    "# ----------------------------------------------------------------------------- \n",
    "\n",
    "# ...  <keep earlier converter code here exactly as before> ...\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# ══════════════════════════ PRE-MERGE FEATURE PASS ═══════════════════════════\n",
    "# ----------------------------------------------------------------------------- \n",
    "\n",
    "from jp_da_imb.utils.time import construct_time_features   # ← your existing util\n",
    "\n",
    "\n",
    "def preprocess_region_df(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    start_date: str | pd.Timestamp | None = None,\n",
    "    end_date:   str | pd.Timestamp | None = None,\n",
    "    freq: str | None = \"30T\",\n",
    "    na_removal: bool = True,\n",
    "    add_time_feats: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply *per-file* hygiene & feature engineering **before** merging with other\n",
    "    DataFrames for the same region.\n",
    "\n",
    "    Steps (all optional, controlled by kwargs)\n",
    "    -----------------------------------------\n",
    "    1. Clip to [start_date, end_date] (if provided)\n",
    "    2. Resample to `freq` (mean aggregation)       — skip if `freq=None`\n",
    "    3. Drop rows with any NA                       — only if `na_removal=True`\n",
    "    4. Add calendar/time features (and cast them\n",
    "       to `category`) using your `construct_time_features`\n",
    "       — only if `add_time_feats=True`\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    # 1. Clip date range -------------------------------------------------------\n",
    "    if start_date is not None:\n",
    "        out = out.loc[out.index >= pd.to_datetime(start_date)]\n",
    "    if end_date is not None:\n",
    "        out = out.loc[out.index <= pd.to_datetime(end_date)]\n",
    "\n",
    "    # 2. Resample --------------------------------------------------------------\n",
    "    if freq is not None:\n",
    "        out = out.resample(freq).mean()\n",
    "\n",
    "    # 3. Remove NAs ------------------------------------------------------------\n",
    "    if na_removal:\n",
    "        out = out.dropna()\n",
    "\n",
    "    # 4. Calendar / categorical time features ---------------------------------\n",
    "    if add_time_feats:\n",
    "        construct_time_features(out)     # mutates in place, adds columns\n",
    "        time_cols = [\n",
    "            \"weekday\", \"hour\", \"month\", \"quarter\",\n",
    "            \"koma\", \"koma_week\", \"is_holiday\",\n",
    "            \"is_peak\", \"is_weekend\",\n",
    "        ]\n",
    "        for c in time_cols:\n",
    "            if c in out.columns and not pd.api.types.is_categorical_dtype(out[c]):\n",
    "                out[c] = pd.Categorical(out[c])\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# ═══════════════  MULTI-FILE INGEST, NOW WITH PRE-PASS  ══════════════════════\n",
    "# ----------------------------------------------------------------------------- \n",
    "\n",
    "def _collect_by_layout(\n",
    "    paths: List[str] | None,\n",
    "    layout: Layout,\n",
    "    *,\n",
    "    cols: List[str] | None,\n",
    "    region_col: str,\n",
    "    sep: str,\n",
    "    preprocess_kwargs: dict,\n",
    ") -> Dict[str, List[pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Load every parquet file in `paths` -> canonical dict,\n",
    "    run `preprocess_region_df` on each region DataFrame,\n",
    "    and bucket them by region.\n",
    "    \"\"\"\n",
    "    bucket: Dict[str, List[pd.DataFrame]] = {}\n",
    "    for p in paths or []:\n",
    "        data = load_parquet_as_canonical(\n",
    "            p, layout=layout, region_col=region_col, sep=sep, cols=cols\n",
    "        )\n",
    "        for region, df in data.items():\n",
    "            df_proc = preprocess_region_df(df, **preprocess_kwargs)\n",
    "            bucket.setdefault(region, []).append(df_proc)\n",
    "    return bucket\n",
    "\n",
    "\n",
    "def load_region_dataset(\n",
    "    *,\n",
    "    region: str,\n",
    "    multi_paths: List[str] | None = None,\n",
    "    long_paths:  List[str] | None = None,\n",
    "    wide_paths:  List[str] | None = None,\n",
    "    cols:       List[str] | None = None,\n",
    "    region_col: str = \"region\",\n",
    "    sep: str = \"-\",\n",
    "    # --- pre-processing options ----------------------------------------------\n",
    "    start_date: str | pd.Timestamp | None = None,\n",
    "    end_date:   str | pd.Timestamp | None = None,\n",
    "    freq: str | None = \"30T\",\n",
    "    na_removal: bool = True,\n",
    "    add_time_feats: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a **single-region** DataFrame by reading any combination of\n",
    "    *multi*, *long*, and *wide* parquet files, applying the pre-processing\n",
    "    steps to each file **before** they are merged together.\n",
    "\n",
    "    Other parameters are identical to the earlier version, plus the\n",
    "    pre-processing kwargs shown above.\n",
    "    \"\"\"\n",
    "    preprocess_kwargs = dict(\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        freq=freq,\n",
    "        na_removal=na_removal,\n",
    "        add_time_feats=add_time_feats,\n",
    "    )\n",
    "\n",
    "    frames: List[pd.DataFrame] = []\n",
    "    for layout, paths in (\n",
    "        (\"multi\", multi_paths),\n",
    "        (\"long\",  long_paths),\n",
    "        (\"wide\",  wide_paths),\n",
    "    ):\n",
    "        for p in paths or []:\n",
    "            data = load_parquet_as_canonical(\n",
    "                p, layout=layout, region_col=region_col, sep=sep, cols=cols\n",
    "            )\n",
    "            if region in data:\n",
    "                frames.append(preprocess_region_df(data[region], **preprocess_kwargs))\n",
    "\n",
    "    if not frames:\n",
    "        raise KeyError(f\"Region '{region}' not found in supplied paths.\")\n",
    "\n",
    "    combined = pd.concat(frames, axis=1)\n",
    "    combined = combined.loc[:, ~combined.columns.duplicated()]  # keep last dup\n",
    "    combined.sort_index(inplace=True)\n",
    "    return combined\n",
    "\n",
    "\n",
    "def load_all_regions_dataset(\n",
    "    *,\n",
    "    multi_paths: List[str] | None = None,\n",
    "    long_paths:  List[str] | None = None,\n",
    "    wide_paths:  List[str] | None = None,\n",
    "    regions:     List[str] | None = None,\n",
    "    cols:       List[str] | None = None,\n",
    "    region_col: str = \"region\",\n",
    "    sep: str = \"-\",\n",
    "    # --- pre-processing options ----------------------------------------------\n",
    "    start_date: str | pd.Timestamp | None = None,\n",
    "    end_date:   str | pd.Timestamp | None = None,\n",
    "    freq: str | None = \"30T\",\n",
    "    na_removal: bool = True,\n",
    "    add_time_feats: bool = True,\n",
    ") -> CanonicalData:\n",
    "    \"\"\"\n",
    "    Load **all** regions (or a given subset) from the provided file lists,\n",
    "    applying the pre-merge feature-creation steps to every individual file.\n",
    "    \"\"\"\n",
    "    preprocess_kwargs = dict(\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        freq=freq,\n",
    "        na_removal=na_removal,\n",
    "        add_time_feats=add_time_feats,\n",
    "    )\n",
    "\n",
    "    bucket: Dict[str, List[pd.DataFrame]] = {}\n",
    "\n",
    "    for paths, layout in (\n",
    "        (multi_paths, \"multi\"),\n",
    "        (long_paths,  \"long\"),\n",
    "        (wide_paths,  \"wide\"),\n",
    "    ):\n",
    "        part = _collect_by_layout(\n",
    "            paths,\n",
    "            layout,\n",
    "            cols=cols,\n",
    "            region_col=region_col,\n",
    "            sep=sep,\n",
    "            preprocess_kwargs=preprocess_kwargs,\n",
    "        )\n",
    "        for region, frames in part.items():\n",
    "            bucket.setdefault(region, []).extend(frames)\n",
    "\n",
    "    # subset filter\n",
    "    if regions is not None:\n",
    "        bucket = {r: bucket[r] for r in regions if r in bucket}\n",
    "\n",
    "    # merge frames per region\n",
    "    out: CanonicalData = {}\n",
    "    for region, frames in bucket.items():\n",
    "        df = pd.concat(frames, axis=1)\n",
    "        df = df.loc[:, ~df.columns.duplicated()]\n",
    "        df.sort_index(inplace=True)\n",
    "        out[region] = df\n",
    "\n",
    "    # raise on missing explicit requests\n",
    "    if regions is not None:\n",
    "        missing = set(regions) - set(out)\n",
    "        if missing:\n",
    "            raise KeyError(\n",
    "                f\"The following requested regions were not found in any file: \"\n",
    "                f\\\"{', '.join(sorted(missing))}\\\"\n",
    "            )\n",
    "\n",
    "    return out\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
