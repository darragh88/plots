{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ebd33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# gnn_experiment.py\n",
    "# ======================================================================\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import pathlib\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import Dict, Any, Optional, Union, List\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# ------------------------------------------------------------- optional\n",
    "try:\n",
    "    import yaml\n",
    "except ImportError:   # YAML is optional; JSON works out-of-the-box\n",
    "    yaml = None\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1. Configuration object\n",
    "# ----------------------------------------------------------------------\n",
    "@dataclass\n",
    "class GNNConfig:\n",
    "    # --- task / model --------------------------------------------------\n",
    "    task: str            = \"node_reg\"               # {\"node_reg\", \"node_clf\", \"edge_clf\"}\n",
    "    model_name: str      = \"gcn\"                    # {\"gcn\", \"graphsage\", \"gat\", \"gatv2\"}\n",
    "    num_layers: int      = 2\n",
    "    hidden_dim: int      = 128\n",
    "    heads: int           = 8                        # for attention models\n",
    "    norm: str            = \"batch\"                  # {\"batch\", \"layer\", None}\n",
    "\n",
    "    # --- dataset & target ---------------------------------------------\n",
    "    target_col: str      = \"target\"                 # node column to predict\n",
    "\n",
    "    # --- data splitting -----------------------------------------------\n",
    "    split_mode: str      = \"date\"                   # {\"date\", \"ratio\"}\n",
    "    cutoff_date: Optional[str] = None               # required if split_mode == \"date\"\n",
    "    val_ratio: float     = 0.10                     # used if split_mode == \"ratio\"\n",
    "    test_ratio: float    = 0.10                     # used if split_mode == \"ratio\"\n",
    "    shuffle_in_split: bool = False                  # shuffle batches inside DataLoader?\n",
    "\n",
    "    # --- optimisation --------------------------------------------------\n",
    "    batch_size: int      = 32\n",
    "    lr: float            = 1e-3\n",
    "    epochs: int          = 100\n",
    "    patience: int        = 10\n",
    "    grad_clip: float     = 1.0\n",
    "\n",
    "    # --- loss / optimiser ---------------------------------------------\n",
    "    loss_fn: str         = \"mse\"                    # {\"mse\", \"mae\", \"bce\", \"cross_entropy\"}\n",
    "    class_weights: Optional[list] = None            # for BCE / CE\n",
    "    optimiser: str       = \"adam\"                   # {\"adam\", \"adamw\", \"sgd\"}\n",
    "    optimiser_kwargs: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "    # --- misc ----------------------------------------------------------\n",
    "    device: str          = \"cuda\"                   # fallback to cpu if unavailable\n",
    "    run_name: str        = \"default_run\"\n",
    "    seed: int            = 42\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # helper constructor\n",
    "    @staticmethod\n",
    "    def load(cfg: Union[\"GNNConfig\", str, Dict[str, Any]]) -> \"GNNConfig\":\n",
    "        \"\"\"\n",
    "        Accept a GNNConfig, a dict, or a path to JSON/YAML and return a GNNConfig.\n",
    "        \"\"\"\n",
    "        if isinstance(cfg, GNNConfig):\n",
    "            return cfg\n",
    "        if isinstance(cfg, dict):\n",
    "            return GNNConfig(**cfg)\n",
    "        if isinstance(cfg, (str, pathlib.Path)):\n",
    "            path = pathlib.Path(cfg)\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                if path.suffix == \".json\":\n",
    "                    data = json.load(f)\n",
    "                elif path.suffix in {\".yml\", \".yaml\"} and yaml is not None:\n",
    "                    data = yaml.safe_load(f)\n",
    "                else:\n",
    "                    raise ValueError(\"Unsupported file type for config path\")\n",
    "            return GNNConfig(**data)\n",
    "        raise TypeError(f\"Unsupported cfg type: {type(cfg)}\")\n",
    "\n",
    "    # pretty-print helper\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return asdict(self)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2. Core experiment wrapper\n",
    "# ----------------------------------------------------------------------\n",
    "class GNNExperiment:\n",
    "    \"\"\"\n",
    "    End-to-end wrapper around:\n",
    "       raw node/edge time-series  →  PyG snapshots  →  loaders  →  model\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------ constructor -------------------------------------------------\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_frames: Dict[str, pd.DataFrame],\n",
    "        edge_frames: Dict[str, pd.DataFrame],\n",
    "        graph: nx.DiGraph,\n",
    "        cfg: Union[GNNConfig, str, Dict[str, Any]] = GNNConfig(),\n",
    "    ):\n",
    "        # raw inputs\n",
    "        self.node_frames = node_frames     # {\"Tokyo\": df, ...}\n",
    "        self.edge_frames = edge_frames     # {\"tokyo-chubu\": df, ...}\n",
    "        self.graph       = graph\n",
    "\n",
    "        # config\n",
    "        self.cfg: GNNConfig = GNNConfig.load(cfg)\n",
    "\n",
    "        # runtime placeholders\n",
    "        self.reg_order:  List[str]     = []   # alphabetical node order\n",
    "        self.edge_order: List[tuple]   = []   # (src_idx, dst_idx)\n",
    "        self.snapshots   = None               # list[Data]\n",
    "\n",
    "        self.train_dl = self.val_dl = self.test_dl = None\n",
    "\n",
    "        self.model = None\n",
    "        self.optimizer = None\n",
    "        self.loss_fn = None\n",
    "        self.history: Dict[str, list] = {}\n",
    "\n",
    "        # device handling\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if (self.cfg.device == \"cuda\" and torch.cuda.is_available()) else \"cpu\"\n",
    "        )\n",
    "\n",
    "    # =========================== helpers =====================================\n",
    "    @staticmethod\n",
    "    def _edge_key(src: str, dst: str) -> str:\n",
    "        \"\"\"Convert (src, dst) into canonical 'src-dst' lowercase key.\"\"\"\n",
    "        return f\"{src}-{dst}\".lower()\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_same_index(idxs: List[pd.DatetimeIndex]) -> List[pd.Timestamp]:\n",
    "        \"\"\"Return sorted intersection of all DatetimeIndex objects.\"\"\"\n",
    "        common = sorted(set.intersection(*(set(i) for i in idxs)))\n",
    "        if not common:\n",
    "            raise ValueError(\"No common timestamps across supplied DataFrames\")\n",
    "        return common\n",
    "\n",
    "    # =================== 1. snapshot creation ================================\n",
    "    def prepare_snapshots(self) -> \"GNNExperiment\":\n",
    "        \"\"\"\n",
    "        Build `torch_geometric.data.Data` objects for each shared timestamp.\n",
    "        Stores them in `self.snapshots` and returns self.\n",
    "        \"\"\"\n",
    "        # --- deterministic node order ----------------------------------------\n",
    "        self.reg_order = sorted(self.graph.nodes)\n",
    "        node_pos = {n: i for i, n in enumerate(self.reg_order)}\n",
    "\n",
    "        # --- deterministic edge order ----------------------------------------\n",
    "        self.edge_order = [(node_pos[src], node_pos[dst])\n",
    "                           for (src, dst) in self.graph.edges]\n",
    "\n",
    "        # --- intersect timestamps --------------------------------------------\n",
    "        node_idxs = [df.index for df in self.node_frames.values()]\n",
    "        edge_idxs = [df.index for df in self.edge_frames.values()]\n",
    "        ts_common = self._ensure_same_index(node_idxs + edge_idxs)\n",
    "\n",
    "        # --- construct snapshot objects --------------------------------------\n",
    "        snapshots: List[Data] = []\n",
    "        for ts in ts_common:\n",
    "            # ---- node matrix & targets\n",
    "            feats, tgts = [], []\n",
    "            for region in self.reg_order:\n",
    "                row = self.node_frames[region].loc[ts]\n",
    "                tgts.append(row[self.cfg.target_col])\n",
    "                feats.append(row.drop(self.cfg.target_col).to_numpy(dtype=np.float32))\n",
    "            x = torch.tensor(np.vstack(feats), dtype=torch.float32)\n",
    "            y = torch.tensor(tgts, dtype=torch.float32)\n",
    "\n",
    "            # ---- edge attribute matrix\n",
    "            edge_rows = []\n",
    "            for src_idx, dst_idx in self.edge_order:\n",
    "                src_name, dst_name = self.reg_order[src_idx], self.reg_order[dst_idx]\n",
    "                key = self._edge_key(src_name, dst_name)\n",
    "                row = self.edge_frames[key].loc[ts]\n",
    "                edge_rows.append(row.to_numpy(dtype=np.float32))\n",
    "            edge_attr = torch.tensor(np.vstack(edge_rows), dtype=torch.float32)\n",
    "\n",
    "            # ---- edge index tensor (shape [2, E])\n",
    "            edge_index = torch.tensor(np.vstack(self.edge_order), dtype=torch.long)\n",
    "\n",
    "            snapshots.append(\n",
    "                Data(\n",
    "                    x=x,\n",
    "                    edge_index=edge_index,\n",
    "                    edge_attr=edge_attr,\n",
    "                    y=y,\n",
    "                    snap_time=torch.tensor([pd.Timestamp(ts).value]),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.snapshots = snapshots\n",
    "        return self\n",
    "\n",
    "    # =================== 2. build loaders ====================================\n",
    "    def build_loaders(self) -> \"GNNExperiment\":\n",
    "        \"\"\"\n",
    "        Create `train_dl`, `val_dl`, `test_dl` according to `self.cfg`.\n",
    "        Must be called **after** `prepare_snapshots`.\n",
    "        \"\"\"\n",
    "        if self.snapshots is None:\n",
    "            raise RuntimeError(\"Call prepare_snapshots() before build_loaders()\")\n",
    "\n",
    "        # ---- chronological sort (snap_time is 1-elem tensor) -----------------\n",
    "        snaps_sorted = sorted(self.snapshots, key=lambda g: g.snap_time.item())\n",
    "\n",
    "        if self.cfg.split_mode == \"date\":\n",
    "            if self.cfg.cutoff_date is None:\n",
    "                raise ValueError(\"`cutoff_date` must be set in cfg for date split\")\n",
    "\n",
    "            cutoff_int = pd.Timestamp(self.cfg.cutoff_date).value\n",
    "            train_set = [g for g in snaps_sorted if g.snap_time.item() <= cutoff_int]\n",
    "            holdout   = [g for g in snaps_sorted if g.snap_time.item() >  cutoff_int]\n",
    "\n",
    "            if not holdout:\n",
    "                raise ValueError(\"No snapshots after cutoff_date for val/test split\")\n",
    "\n",
    "            # simple 50-50 val/test split on holdout\n",
    "            mid = len(holdout) // 2\n",
    "            val_set, test_set = holdout[:mid], holdout[mid:]\n",
    "\n",
    "        elif self.cfg.split_mode == \"ratio\":\n",
    "            n_total = len(snaps_sorted)\n",
    "            n_test  = int(n_total * self.cfg.test_ratio)\n",
    "            n_val   = int(n_total * self.cfg.val_ratio)\n",
    "            n_train = n_total - n_val - n_test\n",
    "\n",
    "            train_set = snaps_sorted[:n_train]\n",
    "            val_set   = snaps_sorted[n_train:n_train + n_val]\n",
    "            test_set  = snaps_sorted[n_train + n_val:]\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown split_mode '{self.cfg.split_mode}'\")\n",
    "\n",
    "        # ---- DataLoaders ------------------------------------------------------\n",
    "        self.train_dl = DataLoader(\n",
    "            train_set,\n",
    "            batch_size=self.cfg.batch_size,\n",
    "            shuffle=self.cfg.shuffle_in_split,\n",
    "        )\n",
    "        self.val_dl = DataLoader(\n",
    "            val_set,\n",
    "            batch_size=self.cfg.batch_size,\n",
    "            shuffle=False,\n",
    "        )\n",
    "        self.test_dl = DataLoader(\n",
    "            test_set,\n",
    "            batch_size=self.cfg.batch_size,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "        return self\n",
    "\n",
    "    # =================== placeholders for later ==============================\n",
    "    def init_model(self) -> \"GNNExperiment\":\n",
    "        \"\"\"Instantiate the requested GNN backbone + head (TODO).\"\"\"\n",
    "        # TODO\n",
    "        return self\n",
    "\n",
    "    def compile(self) -> \"GNNExperiment\":\n",
    "        \"\"\"Attach loss fn, optimiser, schedulers, etc. (TODO).\"\"\"\n",
    "        # TODO\n",
    "        return self\n",
    "\n",
    "    def train(self, debug: bool = False):\n",
    "        \"\"\"Main training loop (TODO).\"\"\"\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def evaluate(self, split: str = \"val\") -> Dict[str, float]:\n",
    "        \"\"\"Evaluate on a chosen split (TODO).\"\"\"\n",
    "        # TODO\n",
    "        return {}\n",
    "\n",
    "    def predict(self, node_frames_new, edge_frames_new, timestamps):\n",
    "        \"\"\"Predict on unseen timestamps (TODO).\"\"\"\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def plot_history(self, metric: str = \"loss\"):\n",
    "        \"\"\"Matplotlib learning-curve plot (TODO).\"\"\"\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def save(self, run_dir: Union[str, pathlib.Path]):\n",
    "        \"\"\"Save config, weights, scaler, etc. (TODO).\"\"\"\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, run_dir: Union[str, pathlib.Path]) -> \"GNNExperiment\":\n",
    "        \"\"\"Reload a previously saved experiment (TODO).\"\"\"\n",
    "        # TODO\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b1ee29",
   "metadata": {},
   "source": [
    "### Model zoo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a57974",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNModelSimple(torch.nn.Module):\n",
    "    def __init__(self, d_in, hidden, d_out):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(d_in, hidden)\n",
    "        self.conv2 = GCNConv(hidden, hidden)\n",
    "        self.fc    = torch.nn.Linear(hidden, d_out)\n",
    "    def forward(self, data):\n",
    "        x, ei, batch = data.x, data.edge_index, data.batch\n",
    "        x = torch.relu(self.conv1(x, ei))\n",
    "        x = torch.relu(self.conv2(x, ei))\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return self.fc(x)\n",
    "\n",
    "class GNNConv(torch.nn.Module):\n",
    "    def __init__(self, d_in, hidden, d_out, layers):\n",
    "        super().__init__()\n",
    "        self.body = GCN(d_in, hidden, layers, norm='batch', act='relu')\n",
    "        self.fc   = torch.nn.Linear(hidden, d_out)\n",
    "    def forward(self, data):\n",
    "        x = self.body(data.x, data.edge_index)\n",
    "        return self.fc(x)\n",
    "\n",
    "class GNNSage(torch.nn.Module):\n",
    "    def __init__(self, d_in, hidden, d_out, layers):\n",
    "        super().__init__()\n",
    "        self.body = GraphSAGE(d_in, hidden, layers, norm='batch', act='relu')\n",
    "        self.fc   = torch.nn.Linear(hidden, d_out)\n",
    "    def forward(self, data):\n",
    "        x = self.body(data.x, data.edge_index)\n",
    "        return self.fc(x)\n",
    "\n",
    "class GNNGAT(torch.nn.Module):\n",
    "    def __init__(self, d_in, hidden, d_out, layers, heads):\n",
    "        super().__init__()\n",
    "        self.body = GAT(d_in, hidden, layers, heads=heads, norm='batch', act='relu')\n",
    "        self.fc   = torch.nn.Linear(hidden, d_out)\n",
    "    def forward(self, data):\n",
    "        x = self.body(data.x, data.edge_index)\n",
    "        return self.fc(x)\n",
    "\n",
    "class GNNGAT2(torch.nn.Module):\n",
    "    def __init__(self, d_in, hidden, d_out, layers, heads, edge_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATv2Conv(d_in, hidden, heads=heads, concat=True,  edge_dim=edge_dim)\n",
    "        self.conv2 = GATv2Conv(hidden*heads, hidden, heads=1,  concat=False, edge_dim=edge_dim)\n",
    "        self.fc    = torch.nn.Linear(hidden, d_out)\n",
    "    def forward(self, data):\n",
    "        x = torch.relu(self.conv1(data.x, data.edge_index, data.edge_attr))\n",
    "        x = torch.relu(self.conv2(x,       data.edge_index, data.edge_attr))\n",
    "        x = global_mean_pool(x, data.batch)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c44792",
   "metadata": {},
   "source": [
    "### Initialize Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4205d8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.  EXPERIMENT  (unchanged until indicated)\n",
    "class GNNExperiment:\n",
    "    # (constructor, helpers, prepare_snapshots, build_loaders as in previous post)\n",
    "    #  …  previous code unchanged …\n",
    "\n",
    "    # ---------- NEW: model factory ------------------------------------\n",
    "    def init_model(self) -> \"GNNExperiment\":\n",
    "        if self.snapshots is None:\n",
    "            raise RuntimeError(\"Call prepare_snapshots() first.\")\n",
    "\n",
    "        d_in  = self.snapshots[0].x.shape[1]\n",
    "        d_out = 1                               # can be generalised later\n",
    "        edge_dim = (\n",
    "            self.snapshots[0].edge_attr.shape[1]\n",
    "            if self.snapshots[0].edge_attr is not None else None\n",
    "        )\n",
    "\n",
    "        name = self.cfg.model_name.lower()\n",
    "        if name in {\"simple\", \"baseline\"}:\n",
    "            model = GNNModelSimple(d_in, self.cfg.hidden_dim, d_out)\n",
    "        elif name in {\"gcn\"}:\n",
    "            model = GNNConv(d_in, self.cfg.hidden_dim, d_out, self.cfg.num_layers)\n",
    "        elif name in {\"graphsage\", \"sage\"}:\n",
    "            model = GNNSage(d_in, self.cfg.hidden_dim, d_out, self.cfg.num_layers)\n",
    "        elif name == \"gat\":\n",
    "            model = GNNGAT(d_in, self.cfg.hidden_dim, d_out,\n",
    "                           self.cfg.num_layers, self.cfg.heads)\n",
    "        elif name in {\"gatv2\", \"gat2\"}:\n",
    "            if edge_dim is None:\n",
    "                raise ValueError(\"GATv2 selected but snapshots contain no edge features.\")\n",
    "            model = GNNGAT2(d_in, self.cfg.hidden_dim, d_out,\n",
    "                            self.cfg.num_layers, self.cfg.heads, edge_dim)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model_name '{self.cfg.model_name}'\")\n",
    "\n",
    "        self.model = model.to(self.device)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac04a87",
   "metadata": {},
   "source": [
    "### Compile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340db1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile(self) -> \"GNNExperiment\":\n",
    "    \"\"\"\n",
    "    Attach loss function, optimiser (and later scheduler) to the experiment.\n",
    "    Must be called **after** init_model().\n",
    "    \"\"\"\n",
    "    if self.model is None:\n",
    "        raise RuntimeError(\"Call init_model() before compile()\")\n",
    "\n",
    "    # ---------- LOSS ------------------------------------------------\n",
    "    loss_name = self.cfg.loss_fn.lower()\n",
    "    if loss_name == \"mse\":\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "    elif loss_name in {\"mae\", \"l1\"}:\n",
    "        self.loss_fn = torch.nn.L1Loss()\n",
    "    elif loss_name == \"bce\":\n",
    "        pos_w = (\n",
    "            torch.tensor(self.cfg.class_weights, dtype=torch.float32, device=self.device)\n",
    "            if self.cfg.class_weights else None\n",
    "        )\n",
    "        self.loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=pos_w)\n",
    "    elif loss_name in {\"cross_entropy\", \"ce\"}:\n",
    "        w = (\n",
    "            torch.tensor(self.cfg.class_weights, dtype=torch.float32, device=self.device)\n",
    "            if self.cfg.class_weights else None\n",
    "        )\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss(weight=w)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown loss_fn '{self.cfg.loss_fn}'\")\n",
    "\n",
    "    # ---------- OPTIMISER ------------------------------------------\n",
    "    opt_name = self.cfg.optimiser.lower()\n",
    "    opt_kwargs = dict(lr=self.cfg.lr, **self.cfg.optimiser_kwargs)\n",
    "\n",
    "    if opt_name == \"adam\":\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), **opt_kwargs)\n",
    "    elif opt_name == \"adamw\":\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), **opt_kwargs)\n",
    "    elif opt_name == \"sgd\":\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), **opt_kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimiser '{self.cfg.optimiser}'\")\n",
    "\n",
    "    # schedulers or other callbacks could be attached here later\n",
    "    return self\n",
    "\n",
    "# ---------- train / evaluate / etc. to be filled later ----------\n",
    "def train(self, debug: bool = False):\n",
    "    pass\n",
    "def evaluate(self, split: str = \"val\") -> Dict[str, float]:\n",
    "    return {}\n",
    "def predict(self, node_frames_new, edge_frames_new, timestamps):\n",
    "    pass\n",
    "def plot_history(self, metric: str = \"loss\"): pass\n",
    "def save(self, run_dir: Union[str, pathlib.Path]): pass\n",
    "@classmethod\n",
    "def load(cls, run_dir: Union[str, pathlib.Path]) -> \"GNNExperiment\": pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e88958",
   "metadata": {},
   "source": [
    "### Extra Loss functionality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3223a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.  SMALL UTILS\n",
    "# ----------------------------------------------------------------------\n",
    "def mean_absolute_error(preds: torch.Tensor, targets: torch.Tensor) -> float:\n",
    "    \"\"\"MAE helper (tensor → float CPU).\"\"\"\n",
    "    return torch.mean(torch.abs(preds - targets)).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c8c50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# gnn_experiment.py\n",
    "# ======================================================================\n",
    "from __future__ import annotations\n",
    "import json, pathlib, numpy as np, pandas as pd, torch, networkx as nx\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import Dict, Any, Optional, Union, List\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import (\n",
    "    GCNConv, GATv2Conv, GCN, GraphSAGE, GAT, global_mean_pool\n",
    ")\n",
    "\n",
    "try:  import yaml\n",
    "except ImportError: yaml = None\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 0.  SMALL UTILS\n",
    "# ----------------------------------------------------------------------\n",
    "def mean_absolute_error(preds: torch.Tensor, targets: torch.Tensor) -> float:\n",
    "    return torch.mean(torch.abs(preds - targets)).item()\n",
    "\n",
    "def has_nan(t: torch.Tensor) -> bool:\n",
    "    return torch.isnan(t).any() or torch.isinf(t).any()\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1.  CONFIG\n",
    "# ----------------------------------------------------------------------\n",
    "@dataclass\n",
    "class GNNConfig:\n",
    "    # --- task / model --------------------------------------------------\n",
    "    task: str            = \"node_reg\"              # {\"node_reg\", \"node_clf\", \"edge_clf\"}\n",
    "    model_name: str      = \"gcn\"                   # {\"simple\",\"gcn\",\"graphsage\",\"gat\",\"gatv2\"}\n",
    "    num_layers: int      = 2\n",
    "    hidden_dim: int      = 128\n",
    "    heads: int           = 8\n",
    "    norm: str            = \"batch\"\n",
    "\n",
    "    # --- dataset & target ---------------------------------------------\n",
    "    target_col: str      = \"target\"\n",
    "\n",
    "    # --- splitting -----------------------------------------------------\n",
    "    split_mode: str      = \"date\"                  # {\"date\",\"ratio\"}\n",
    "    cutoff_date: Optional[str] = None\n",
    "    val_ratio: float     = 0.10\n",
    "    test_ratio: float    = 0.10\n",
    "    shuffle_in_split: bool = False\n",
    "\n",
    "    # --- optimisation --------------------------------------------------\n",
    "    batch_size: int      = 32\n",
    "    lr: float            = 1e-3\n",
    "    epochs: int          = 100\n",
    "    patience: int        = 10\n",
    "    grad_clip: float     = 1.0\n",
    "\n",
    "    # --- loss / optimiser ---------------------------------------------\n",
    "    loss_fn: str         = \"mse\"                   # {\"mse\",\"mae\",\"bce\",\"cross_entropy\"}\n",
    "    class_weights: Optional[list] = None\n",
    "    optimiser: str       = \"adam\"                  # {\"adam\",\"adamw\",\"sgd\"}\n",
    "    optimiser_kwargs: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "    # --- misc ----------------------------------------------------------\n",
    "    device: str          = \"cuda\"\n",
    "    run_name: str        = \"default_run\"\n",
    "    seed: int            = 42\n",
    "\n",
    "    # ---------- helper constructor ----------\n",
    "    @staticmethod\n",
    "    def load(cfg: Union[\"GNNConfig\", str, Dict[str, Any]]) -> \"GNNConfig\":\n",
    "        if isinstance(cfg, GNNConfig):\n",
    "            return cfg\n",
    "        if isinstance(cfg, dict):\n",
    "            return GNNConfig(**cfg)\n",
    "        path = pathlib.Path(cfg)\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = (\n",
    "                json.load(f)\n",
    "                if path.suffix == \".json\"\n",
    "                else yaml.safe_load(f) if yaml else\n",
    "                (_ for _ in ()).throw(RuntimeError(\"PyYAML not installed\"))\n",
    "            )\n",
    "        return GNNConfig(**data)\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return asdict(self)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2.  MODEL ZOO  (shortened comments)\n",
    "# ----------------------------------------------------------------------\n",
    "class GNNModelSimple(torch.nn.Module):\n",
    "    def __init__(self, d_in, hidden, d_out):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(d_in, hidden)\n",
    "        self.conv2 = GCNConv(hidden, hidden)\n",
    "        self.fc    = torch.nn.Linear(hidden, d_out)\n",
    "    def forward(self, data):\n",
    "        x = torch.relu(self.conv1(data.x, data.edge_index))\n",
    "        x = torch.relu(self.conv2(x,      data.edge_index))\n",
    "        return self.fc(global_mean_pool(x, data.batch))\n",
    "\n",
    "class GNNConv(torch.nn.Module):\n",
    "    def __init__(self, d_in, hidden, d_out, layers):\n",
    "        super().__init__()\n",
    "        self.body = GCN(d_in, hidden, layers, norm='batch', act='relu')\n",
    "        self.fc   = torch.nn.Linear(hidden, d_out)\n",
    "    def forward(self, data):\n",
    "        return self.fc(self.body(data.x, data.edge_index))\n",
    "\n",
    "class GNNSage(torch.nn.Module):\n",
    "    def __init__(self, d_in, hidden, d_out, layers):\n",
    "        super().__init__()\n",
    "        self.body = GraphSAGE(d_in, hidden, layers, norm='batch', act='relu')\n",
    "        self.fc   = torch.nn.Linear(hidden, d_out)\n",
    "    def forward(self, data):\n",
    "        return self.fc(self.body(data.x, data.edge_index))\n",
    "\n",
    "class GNNGAT(torch.nn.Module):\n",
    "    def __init__(self, d_in, hidden, d_out, layers, heads):\n",
    "        super().__init__()\n",
    "        self.body = GAT(d_in, hidden, layers, heads=heads, norm='batch', act='relu')\n",
    "        self.fc   = torch.nn.Linear(hidden, d_out)\n",
    "    def forward(self, data):\n",
    "        return self.fc(self.body(data.x, data.edge_index))\n",
    "\n",
    "class GNNGAT2(torch.nn.Module):\n",
    "    def __init__(self, d_in, hidden, d_out, layers, heads, edge_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATv2Conv(d_in, hidden, heads=heads, concat=True, edge_dim=edge_dim)\n",
    "        self.conv2 = GATv2Conv(hidden*heads, hidden, heads=1, concat=False, edge_dim=edge_dim)\n",
    "        self.fc    = torch.nn.Linear(hidden, d_out)\n",
    "    def forward(self, data):\n",
    "        x = torch.relu(self.conv1(data.x, data.edge_index, data.edge_attr))\n",
    "        x = torch.relu(self.conv2(x,      data.edge_index, data.edge_attr))\n",
    "        return self.fc(global_mean_pool(x, data.batch))\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3.  EXPERIMENT (prepare_snapshots + loaders + model/compile as before)\n",
    "# ----------------------------------------------------------------------\n",
    "class GNNExperiment:\n",
    "    # ------- constructor & helpers (unchanged until train section) -----\n",
    "    def __init__(self, node_frames, edge_frames, graph, cfg=GNNConfig()):\n",
    "        self.node_frames, self.edge_frames, self.graph = node_frames, edge_frames, graph\n",
    "        self.cfg: GNNConfig = GNNConfig.load(cfg)\n",
    "\n",
    "        self.reg_order:  List[str]   = []\n",
    "        self.edge_order: List[tuple] = []\n",
    "        self.snapshots = None\n",
    "\n",
    "        self.train_dl = self.val_dl = self.test_dl = None\n",
    "        self.model = self.optimizer = self.loss_fn = None\n",
    "        self.metric_fn = mean_absolute_error\n",
    "        self.history: Dict[str, list] = {\"train_loss\": [], \"val_loss\": [], \"val_metric\": []}\n",
    "        self.best_val_loss = float(\"inf\")\n",
    "        self.patience_counter = 0\n",
    "        self.best_ckpt = None\n",
    "\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if (self.cfg.device == \"cuda\" and torch.cuda.is_available()) else \"cpu\"\n",
    "        )\n",
    "\n",
    "    # -- (prepare_snapshots & build_loaders identical to earlier version) --\n",
    "\n",
    "    def _edge_key(self, s, d): return f\"{s}-{d}\".lower()\n",
    "    @staticmethod\n",
    "    def _ensure_same_index(idxs): return sorted(set.intersection(*(set(i) for i in idxs)))\n",
    "\n",
    "    def prepare_snapshots(self):\n",
    "        self.reg_order = sorted(self.graph.nodes)\n",
    "        pos = {n: i for i, n in enumerate(self.reg_order)}\n",
    "        self.edge_order = [(pos[s], pos[d]) for (s, d) in self.graph.edges]\n",
    "\n",
    "        ts_common = self._ensure_same_index(\n",
    "            [df.index for df in self.node_frames.values()]\n",
    "            + [df.index for df in self.edge_frames.values()]\n",
    "        )\n",
    "\n",
    "        snaps = []\n",
    "        for ts in ts_common:\n",
    "            feats, tgts = [], []\n",
    "            for region in self.reg_order:\n",
    "                row = self.node_frames[region].loc[ts]\n",
    "                tgts.append(row[self.cfg.target_col])\n",
    "                feats.append(row.drop(self.cfg.target_col).to_numpy(dtype=np.float32))\n",
    "            x = torch.tensor(np.vstack(feats), dtype=torch.float32)\n",
    "            y = torch.tensor(tgts, dtype=torch.float32)\n",
    "\n",
    "            edge_rows = []\n",
    "            for s_i, d_i in self.edge_order:\n",
    "                s, d = self.reg_order[s_i], self.reg_order[d_i]\n",
    "                edge_rows.append(\n",
    "                    self.edge_frames[self._edge_key(s, d)].loc[ts].to_numpy(np.float32)\n",
    "                )\n",
    "            edge_attr  = torch.tensor(np.vstack(edge_rows), dtype=torch.float32)\n",
    "            edge_index = torch.tensor(np.vstack(self.edge_order), dtype=torch.long)\n",
    "\n",
    "            snaps.append(\n",
    "                Data(x=x, edge_index=edge_index, edge_attr=edge_attr,\n",
    "                     y=y, snap_time=torch.tensor([pd.Timestamp(ts).value]))\n",
    "            )\n",
    "        self.snapshots = snaps\n",
    "        return self\n",
    "\n",
    "    def build_loaders(self):\n",
    "        if self.snapshots is None:\n",
    "            raise RuntimeError(\"Call prepare_snapshots() first\")\n",
    "\n",
    "        snaps_sorted = sorted(self.snapshots, key=lambda g: g.snap_time.item())\n",
    "        if self.cfg.split_mode == \"date\":\n",
    "            cut = pd.Timestamp(self.cfg.cutoff_date).value\n",
    "            train_set = [g for g in snaps_sorted if g.snap_time.item() <= cut]\n",
    "            hold      = [g for g in snaps_sorted if g.snap_time.item() >  cut]\n",
    "            mid = len(hold) // 2\n",
    "            val_set, test_set = hold[:mid], hold[mid:]\n",
    "        else:\n",
    "            n = len(snaps_sorted)\n",
    "            n_test = int(n * self.cfg.test_ratio)\n",
    "            n_val  = int(n * self.cfg.val_ratio)\n",
    "            n_train = n - n_val - n_test\n",
    "            train_set = snaps_sorted[:n_train]\n",
    "            val_set   = snaps_sorted[n_train:n_train+n_val]\n",
    "            test_set  = snaps_sorted[n_train+n_val:]\n",
    "\n",
    "        self.train_dl = DataLoader(train_set, batch_size=self.cfg.batch_size,\n",
    "                                   shuffle=self.cfg.shuffle_in_split)\n",
    "        self.val_dl   = DataLoader(val_set,   batch_size=self.cfg.batch_size)\n",
    "        self.test_dl  = DataLoader(test_set,  batch_size=self.cfg.batch_size)\n",
    "        return self\n",
    "\n",
    "    def update_config(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            if not hasattr(self.cfg, k): raise AttributeError(k)\n",
    "            setattr(self.cfg, k, v)\n",
    "        return self\n",
    "\n",
    "    def init_model(self):\n",
    "        d_in  = self.snapshots[0].x.shape[1]\n",
    "        d_out = 1\n",
    "        edge_dim = self.snapshots[0].edge_attr.shape[1]\n",
    "\n",
    "        name = self.cfg.model_name.lower()\n",
    "        if name in {\"simple\", \"baseline\"}:\n",
    "            self.model = GNNModelSimple(d_in, self.cfg.hidden_dim, d_out)\n",
    "        elif name == \"gcn\":\n",
    "            self.model = GNNConv(d_in, self.cfg.hidden_dim, d_out, self.cfg.num_layers)\n",
    "        elif name in {\"graphsage\", \"sage\"}:\n",
    "            self.model = GNNSage(d_in, self.cfg.hidden_dim, d_out, self.cfg.num_layers)\n",
    "        elif name == \"gat\":\n",
    "            self.model = GNNGAT(d_in, self.cfg.hidden_dim, d_out,\n",
    "                                self.cfg.num_layers, self.cfg.heads)\n",
    "        elif name in {\"gatv2\", \"gat2\"}:\n",
    "            self.model = GNNGAT2(d_in, self.cfg.hidden_dim, d_out,\n",
    "                                 self.cfg.num_layers, self.cfg.heads, edge_dim)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model '{self.cfg.model_name}'\")\n",
    "\n",
    "        self.model = self.model.to(self.device)\n",
    "        return self\n",
    "\n",
    "    def compile(self):\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"Call init_model() before compile()\")\n",
    "\n",
    "        loss = self.cfg.loss_fn.lower()\n",
    "        if loss == \"mse\":\n",
    "            self.loss_fn = torch.nn.MSELoss()\n",
    "        elif loss in {\"mae\", \"l1\"}:\n",
    "            self.loss_fn = torch.nn.L1Loss()\n",
    "        elif loss == \"bce\":\n",
    "            w = (torch.tensor(self.cfg.class_weights, dtype=torch.float32, device=self.device)\n",
    "                 if self.cfg.class_weights else None)\n",
    "            self.loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=w)\n",
    "        elif loss in {\"cross_entropy\", \"ce\"}:\n",
    "            w = (torch.tensor(self.cfg.class_weights, dtype=torch.float32, device=self.device)\n",
    "                 if self.cfg.class_weights else None)\n",
    "            self.loss_fn = torch.nn.CrossEntropyLoss(weight=w)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown loss_fn '{self.cfg.loss_fn}'\")\n",
    "\n",
    "        opt_name = self.cfg.optimiser.lower()\n",
    "        opt_kwargs = dict(lr=self.cfg.lr, **self.cfg.optimiser_kwargs)\n",
    "        if opt_name == \"adam\":\n",
    "            self.optimizer = torch.optim.Adam(self.model.parameters(), **opt_kwargs)\n",
    "        elif opt_name == \"adamw\":\n",
    "            self.optimizer = torch.optim.AdamW(self.model.parameters(), **opt_kwargs)\n",
    "        elif opt_name == \"sgd\":\n",
    "            self.optimizer = torch.optim.SGD(self.model.parameters(), **opt_kwargs)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown optimiser '{self.cfg.optimiser}'\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    # ==================================================================\n",
    "    # ### 4. TRAIN / EVALUATE ###########################################\n",
    "    # ==================================================================\n",
    "    def _train_epoch(self, debug: bool = False) -> float:\n",
    "        self.model.train()\n",
    "        total = 0\n",
    "        for step, data in enumerate(self.train_dl):\n",
    "            data = data.to(self.device)\n",
    "            data.x = data.x.float()\n",
    "            if data.edge_attr is not None:\n",
    "                data.edge_attr = data.edge_attr.float()\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            out = self.model(data)\n",
    "            loss = self.loss_fn(out.squeeze(-1), data.y)\n",
    "            if debug and (has_nan(loss) or has_nan(out)):\n",
    "                raise RuntimeError(f\"NaN detected at step {step}\")\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.cfg.grad_clip)\n",
    "            self.optimizer.step()\n",
    "            total += loss.item()\n",
    "        return total / len(self.train_dl)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _eval_loader(self, loader: DataLoader) -> tuple[float, float]:\n",
    "        self.model.eval()\n",
    "        total_loss = total_metric = 0\n",
    "        for data in loader:\n",
    "            data = data.to(self.device)\n",
    "            data.x = data.x.float()\n",
    "            if data.edge_attr is not None:\n",
    "                data.edge_attr = data.edge_attr.float()\n",
    "            out = self.model(data)\n",
    "            loss = self.loss_fn(out.squeeze(-1), data.y)\n",
    "            metric = self.metric_fn(out.squeeze(-1).cpu(), data.y.cpu())\n",
    "            total_loss   += loss.item()\n",
    "            total_metric += metric\n",
    "        n = len(loader)\n",
    "        return total_loss / n, total_metric / n\n",
    "\n",
    "    def train(self, debug: bool = False):\n",
    "        if any(v is None for v in (self.model, self.optimizer, self.loss_fn)):\n",
    "            raise RuntimeError(\"Call init_model() and compile() before train()\")\n",
    "\n",
    "        for epoch in range(1, self.cfg.epochs + 1):\n",
    "            tr_loss = self._train_epoch(debug)\n",
    "            val_loss, val_metric = self._eval_loader(self.val_dl)\n",
    "\n",
    "            self.history[\"train_loss\"].append(tr_loss)\n",
    "            self.history[\"val_loss\"].append(val_loss)\n",
    "            self.history[\"val_metric\"].append(val_metric)\n",
    "\n",
    "            print(f\"[{epoch:03d}/{self.cfg.epochs}] \"\n",
    "                  f\"train={tr_loss:.4f}  val={val_loss:.4f}  metric={val_metric:.4f}\")\n",
    "\n",
    "            # ----- early stopping -----\n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "                self.patience_counter = 0\n",
    "                self.best_ckpt = {k: v.cpu() for k, v in self.model.state_dict().items()}\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "                if self.patience_counter >= self.cfg.patience:\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "        return self\n",
    "\n",
    "    def evaluate(self, split: str = \"test\") -> Dict[str, float]:\n",
    "        loader = {\"train\": self.train_dl, \"val\": self.val_dl, \"test\": self.test_dl}.get(split)\n",
    "        if loader is None:\n",
    "            raise ValueError(\"split must be one of {'train','val','test'}\")\n",
    "\n",
    "        loss, metric = self._eval_loader(loader)\n",
    "        return {\"loss\": loss, \"metric\": metric}\n",
    "\n",
    "    # -------- stubs for predict / save / load / plot (later) ----------\n",
    "    def predict(self, node_frames_new, edge_frames_new, timestamps): ...\n",
    "    def plot_history(self, metric: str = \"loss\"): ...\n",
    "    def save(self, run_dir): ...\n",
    "    @classmethod\n",
    "    def load(cls, run_dir): ...\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# USAGE EXAMPLE (not executed here)\n",
    "# ======================================================================\n",
    "# exp = (GNNExperiment(nodes_dict, edges_dict, G, {\"model_name\":\"gatv2\", \"cutoff_date\":\"2023-06-01\"})\n",
    "#        .prepare_snapshots()\n",
    "#        .build_loaders()\n",
    "#        .init_model()\n",
    "#        .compile()\n",
    "#        .train(debug=False))\n",
    "#\n",
    "# print(exp.evaluate(\"test\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735433e6",
   "metadata": {},
   "source": [
    "### Predictions and plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d36680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# inside class GNNExperiment  (add/replace these two methods)\n",
    "# =====================================================================\n",
    "# --------------------------------------------------\n",
    "# PREDICT on *unseen* data\n",
    "# --------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def predict(\n",
    "    self,\n",
    "    node_frames_new: Dict[str, pd.DataFrame],\n",
    "    edge_frames_new: Dict[str, pd.DataFrame],\n",
    "    timestamps: Optional[List[pd.Timestamp]] = None,\n",
    "    return_df: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Forward-pass the fitted model on future (or out-of-sample) snapshots.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    node_frames_new : dict[str → pd.DataFrame]\n",
    "        Same structure / columns as training node_frames.\n",
    "    edge_frames_new : dict[str → pd.DataFrame]\n",
    "        Same structure / columns as training edge_frames.\n",
    "    timestamps      : iterable of pandas Timestamps (optional)\n",
    "        If None, use *intersection* of all node/edge frames.\n",
    "    return_df       : if True, returns a tidy DataFrame; else raw torch.Tensor.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame or torch.Tensor\n",
    "    \"\"\"\n",
    "    if self.model is None:\n",
    "        raise RuntimeError(\"Train or load a model before calling predict()\")\n",
    "\n",
    "    # 1. decide which timestamps to score ---------------------------------\n",
    "    if timestamps is None:\n",
    "        idxs = [df.index for df in node_frames_new.values()] + \\\n",
    "               [df.index for df in edge_frames_new.values()]\n",
    "        timestamps = sorted(set.intersection(*(set(ix) for ix in idxs)))\n",
    "    else:\n",
    "        timestamps = [pd.Timestamp(ts) for ts in timestamps]\n",
    "\n",
    "    # 2. build snapshots *in the same node/edge order* --------------------\n",
    "    snaps = []\n",
    "    for ts in timestamps:\n",
    "        feats = []\n",
    "        for region in self.reg_order:\n",
    "            feats.append(\n",
    "                node_frames_new[region].loc[ts]\n",
    "                .drop(self.cfg.target_col)\n",
    "                .to_numpy(dtype=np.float32)\n",
    "            )\n",
    "        x = torch.tensor(np.vstack(feats), dtype=torch.float32)\n",
    "\n",
    "        edge_rows = []\n",
    "        for s_idx, d_idx in self.edge_order:\n",
    "            s, d = self.reg_order[s_idx], self.reg_order[d_idx]\n",
    "            edge_rows.append(\n",
    "                edge_frames_new[f\"{s}-{d}\".lower()].loc[ts].to_numpy(np.float32)\n",
    "            )\n",
    "        edge_attr  = torch.tensor(np.vstack(edge_rows), dtype=torch.float32)\n",
    "        edge_index = torch.tensor(np.array(self.edge_order).T, dtype=torch.long)\n",
    "\n",
    "        snaps.append(Data(x=x, edge_index=edge_index, edge_attr=edge_attr))\n",
    "\n",
    "    loader = DataLoader(snaps, batch_size=self.cfg.batch_size, shuffle=False)\n",
    "\n",
    "    # 3. forward pass ------------------------------------------------------\n",
    "    self.model.eval()\n",
    "    preds = []\n",
    "    for data in loader:\n",
    "        data = data.to(self.device)\n",
    "        data.x = data.x.float()\n",
    "        if data.edge_attr is not None:\n",
    "            data.edge_attr = data.edge_attr.float()\n",
    "        out = self.model(data).squeeze(-1).cpu()\n",
    "        preds.append(out)\n",
    "\n",
    "    y_hat = torch.cat(preds, dim=0)          # [len(timestamps)*N]\n",
    "    y_hat = y_hat.reshape(len(timestamps), len(self.reg_order))\n",
    "\n",
    "    if return_df:\n",
    "        return pd.DataFrame(\n",
    "            y_hat.numpy(),\n",
    "            index=pd.to_datetime(timestamps),\n",
    "            columns=self.reg_order,\n",
    "        )\n",
    "    return y_hat                                     # raw tensor\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# PLOT learning curves\n",
    "# --------------------------------------------------\n",
    "def plot_history(self, metric: str = \"loss\"):\n",
    "    \"\"\"\n",
    "    Plot training vs validation curves.\n",
    "\n",
    "    metric : \"loss\" (train/val) | \"metric\" (val MAE or CE etc.)\n",
    "    \"\"\"\n",
    "    if not self.history[\"train_loss\"]:\n",
    "        raise RuntimeError(\"Nothing in history — did you call train()?\")\n",
    "\n",
    "    if metric == \"loss\":\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.plot(self.history[\"train_loss\"], label=\"Train Loss\")\n",
    "        plt.plot(self.history[\"val_loss\"],   label=\"Val Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training / Validation Loss\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    elif metric in {\"metric\", \"mae\", \"accuracy\"}:\n",
    "        if not self.history[\"val_metric\"]:\n",
    "            raise ValueError(\"Metric history empty; choose 'loss' instead.\")\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.plot(self.history[\"val_metric\"], label=f\"Val {metric.upper()}\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(metric.upper())\n",
    "        plt.title(f\"Validation {metric.upper()} Curve\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    else:\n",
    "        raise ValueError(\"metric must be 'loss' or 'metric'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7ea7ca",
   "metadata": {},
   "source": [
    "### Loading and prediciting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6150fbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# inside class GNNExperiment  (add or replace the stubs)\n",
    "# =====================================================================\n",
    "    # --------------------------------------------------\n",
    "    # SAVE everything needed to reproduce / predict\n",
    "    # --------------------------------------------------\n",
    "    def save(self, run_dir: Union[str, pathlib.Path]):\n",
    "        \"\"\"\n",
    "        Persist config, model weights, and run metadata into `run_dir/`.\n",
    "\n",
    "        Creates:\n",
    "            run_dir/\n",
    "              ├─ cfg.json\n",
    "              ├─ meta.json             (node & edge ordering, dims)\n",
    "              ├─ model.pt              (best weights if available else current)\n",
    "              └─ history.json          (training curves)\n",
    "        \"\"\"\n",
    "        run_dir = pathlib.Path(run_dir)\n",
    "        run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # 1) config ------------------------------------------------------\n",
    "        (run_dir / \"cfg.json\").write_text(\n",
    "            json.dumps(self.cfg.to_dict(), indent=2)\n",
    "        )\n",
    "\n",
    "        # 2) metadata we need to rebuild the model without original data --\n",
    "        meta = {\n",
    "            \"reg_order\":  self.reg_order,\n",
    "            \"edge_order\": self.edge_order,\n",
    "            \"input_dim\":  int(self.snapshots[0].x.shape[1]) if self.snapshots else None,\n",
    "            \"edge_dim\": (\n",
    "                int(self.snapshots[0].edge_attr.shape[1])\n",
    "                if self.snapshots and self.snapshots[0].edge_attr is not None\n",
    "                else None\n",
    "            ),\n",
    "        }\n",
    "        (run_dir / \"meta.json\").write_text(json.dumps(meta, indent=2))\n",
    "\n",
    "        # 3) model weights ----------------------------------------------\n",
    "        state = self.best_ckpt if self.best_ckpt is not None else self.model.state_dict()\n",
    "        torch.save(state, run_dir / \"model.pt\")\n",
    "\n",
    "        # 4) training curves --------------------------------------------\n",
    "        (run_dir / \"history.json\").write_text(json.dumps(self.history, indent=2))\n",
    "\n",
    "        print(f\"✨  Run saved to: {run_dir.resolve()}\")\n",
    "\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # LOAD a previously saved run\n",
    "    # --------------------------------------------------\n",
    "    @classmethod\n",
    "    def load(\n",
    "        cls,\n",
    "        run_dir: Union[str, pathlib.Path],\n",
    "        node_frames: Optional[Dict[str, pd.DataFrame]] = None,\n",
    "        edge_frames: Optional[Dict[str, pd.DataFrame]] = None,\n",
    "        graph: Optional[nx.DiGraph] = None,\n",
    "    ) -> \"GNNExperiment\":\n",
    "        \"\"\"\n",
    "        Recreate an experiment from disk.  If you plan to *predict*, you must\n",
    "        also supply `node_frames`, `edge_frames`, and `graph` so the snapshot\n",
    "        builder can run; otherwise they can be left None.\n",
    "\n",
    "        Usage\n",
    "        -----\n",
    "        exp = GNNExperiment.load(\"runs/my_run\", node_frames, edge_frames, G)\n",
    "        ŷ = exp.predict(...)\n",
    "        \"\"\"\n",
    "        run_dir = pathlib.Path(run_dir)\n",
    "        cfg  = GNNConfig.load(json.loads((run_dir / \"cfg.json\").read_text()))\n",
    "        meta = json.loads((run_dir / \"meta.json\").read_text())\n",
    "\n",
    "        # --------------------------------------------------------------\n",
    "        # 1. Create (possibly empty) experiment instance\n",
    "        # --------------------------------------------------------------\n",
    "        if node_frames is None:  node_frames = {}\n",
    "        if edge_frames is None:  edge_frames = {}\n",
    "        if graph is None:        graph = nx.DiGraph()\n",
    "\n",
    "        exp = cls(node_frames, edge_frames, graph, cfg)\n",
    "\n",
    "        # restore deterministic ordering\n",
    "        exp.reg_order  = meta[\"reg_order\"]\n",
    "        exp.edge_order = [tuple(t) for t in meta[\"edge_order\"]]\n",
    "\n",
    "        # --------------------------------------------------------------\n",
    "        # 2. Re-instantiate the model (input & edge dims come from meta)\n",
    "        # --------------------------------------------------------------\n",
    "        d_in, edge_dim = meta[\"input_dim\"], meta[\"edge_dim\"]\n",
    "        name = cfg.model_name.lower()\n",
    "        if name in {\"simple\", \"baseline\"}:\n",
    "            model = GNNModelSimple(d_in, cfg.hidden_dim, 1)\n",
    "        elif name == \"gcn\":\n",
    "            model = GNNConv(d_in, cfg.hidden_dim, 1, cfg.num_layers)\n",
    "        elif name in {\"graphsage\", \"sage\"}:\n",
    "            model = GNNSage(d_in, cfg.hidden_dim, 1, cfg.num_layers)\n",
    "        elif name == \"gat\":\n",
    "            model = GNNGAT(d_in, cfg.hidden_dim, 1, cfg.num_layers, cfg.heads)\n",
    "        elif name in {\"gatv2\", \"gat2\"}:\n",
    "            model = GNNGAT2(d_in, cfg.hidden_dim, 1, cfg.num_layers, cfg.heads, edge_dim)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model '{cfg.model_name}'\")\n",
    "\n",
    "        exp.model = model.to(exp.device)\n",
    "\n",
    "        # load weights\n",
    "        state_dict = torch.load(run_dir / \"model.pt\", map_location=exp.device)\n",
    "        exp.model.load_state_dict(state_dict)\n",
    "\n",
    "        # training curves (optional)\n",
    "        exp.history = json.loads((run_dir / \"history.json\").read_text())\n",
    "\n",
    "        print(f\"🔄  Loaded run from {run_dir.resolve()}\")\n",
    "        return exp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead32467",
   "metadata": {},
   "source": [
    "### Scaling Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a7f654",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 1️⃣  split timestamps once so we reuse it everywhere\n",
    "# -----------------------------------------------------------\n",
    "cutoff = pd.Timestamp(\"2023-06-01\")\n",
    "\n",
    "train_idx = node_frames[\"Tokyo\"].index <= cutoff\n",
    "test_idx  = ~train_idx                # everything after cutoff\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 2️⃣  one scaler *per region*\n",
    "# -----------------------------------------------------------\n",
    "y_scalers = {}          # region -> fitted scaler\n",
    "for region, df in node_frames.items():\n",
    "    # fit on TRAIN part only\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(df.loc[train_idx, [\"target\"]].values)\n",
    "    y_scalers[region] = scaler\n",
    "\n",
    "    # overwrite the column in BOTH splits with scaled values\n",
    "    df.loc[:, \"target\"] = scaler.transform(df[[\"target\"]])\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 3️⃣  build & train your GNNExperiment\n",
    "# -----------------------------------------------------------\n",
    "exp = (\n",
    "    GNNExperiment(node_frames, edge_frames, G,\n",
    "                  {\"cutoff_date\": \"2023-06-01\"})\n",
    "      .prepare_snapshots()\n",
    "      .build_loaders()\n",
    "      .init_model()\n",
    "      .compile()\n",
    "      .train()\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 4️⃣  get scaled predictions\n",
    "# -----------------------------------------------------------\n",
    "scaled_preds = exp.predict(node_frames, edge_frames)   # DataFrame, still scaled\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 5️⃣  inverse-transform for human-readable numbers\n",
    "# -----------------------------------------------------------\n",
    "pred_orig_units = pd.DataFrame(index=scaled_preds.index, columns=scaled_preds.columns)\n",
    "for region in scaled_preds.columns:\n",
    "    pred_orig_units[region] = y_scalers[region].inverse_transform(\n",
    "        scaled_preds[[region]].values\n",
    "    )\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 6️⃣  compute metrics in natural units\n",
    "# -----------------------------------------------------------\n",
    "y_true_orig = pd.DataFrame(index=pred_orig_units.index, columns=pred_orig_units.columns)\n",
    "for region in pred_orig_units.columns:\n",
    "    # remember: the node_frames we stored earlier now contain *scaled* y.\n",
    "    # fetch the *unscaled* version from the original CSV / a backup copy\n",
    "    # or just inverse-transform the scaled column the same way:\n",
    "    y_true_orig[region] = y_scalers[region].inverse_transform(\n",
    "        node_frames[region].loc[test_idx, [\"target\"]].values\n",
    "    )\n",
    "\n",
    "mae_per_region = (pred_orig_units - y_true_orig).abs().mean()\n",
    "print(mae_per_region)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
