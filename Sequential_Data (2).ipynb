{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0f81249",
   "metadata": {},
   "source": [
    "| Model “block”                         | Import (PyG / PyG-Temporal unless noted)                                                                        | Edge-attr aware?                                | Big selling point                                                              | Use when …                                                          |\n",
    "| ------------------------------------- | --------------------------------------------------------------------------------------------------------------- | ----------------------------------------------- | ------------------------------------------------------------------------------ | ------------------------------------------------------------------- |\n",
    "| **TGCN**                              | `from torch_geometric_temporal.nn.recurrent import TGCN`                                                        | **No** (only adjacency)                         | Gated RNN that embeds `A` inside GRU gates.                                    | You want a light baseline; <1 M params.                             |\n",
    "| **STGCN (sandwich)**                  | `from torch_geometric_temporal.nn.stgcn import STGCNBlock`                                                      | **No**                                          | GCN layer between two 1-D temporal convs – the classic traffic-forecast block. | Medium horizon (1–48 h), cheap to run.                              |\n",
    "| **Graph WaveNet**                     | `from torch_geometric_temporal.nn.conv import GraphWaveNet`                                                     | **Yes** (`edge_index`, optional `edge_weight`)  | Dilated causal CNN ⇄ diffusion graph conv; handles 100+ look-back steps.       | You need long receptive field or multihorizon output.               |\n",
    "| **NNConv + GRU**                      | `python<br>from torch_geometric.nn import NNConv<br>from torch_geometric_temporal.nn.recurrent import GConvGRU` | **Yes** (`edge_attr`)                           | Edge-conditioned weights: tie-line capacity/utilisation gates the message.     | Capacities or dynamic line states matter.                           |\n",
    "| **DCRNN**                             | `from torch_geometric_temporal.nn.recurrent import DCRNN`                                                       | **No** (diffusion uses adjacency)               | Diffusion conv inside GRU; softer than STGCN for directed graphs.              | You have directed edges & want diffusion bias.                      |\n",
    "| **ASTGNN / SAN (Adaptive Sp-T Attn)** | `from torch_geometric_temporal.nn.attention import ASTGNNBlock`                                                 | **Yes** (learns adaptive adjacency)             | Learns **extra** soft links via attention on top of physical graph.            | Hidden couplings (e.g. fuel-price link) aren’t in your 9×9 `A`.     |\n",
    "| **GAT-GRU hybrid**                    | `from torch_geometric.nn import GATv2Conv` + wrap in GRU                                                        | **Optional** (`edge_attr` via concat in `attn`) | Attention tells the model which neighbour to trust at each step.               | Graph is tiny → attention cost trivial; you crave interpretability. |\n",
    "| **Temporal Graph Network (TGN)**      | *external repo* `pip install pytorch-tgn` → `from tgn import TemporalFusion`                                    | **Yes** (event stream)                          | Continuous-time memory; handles irregular events.                              | You have true event log, not fixed snapshots.                       |\n",
    "| **Transformer in PyG 2.4**            | `python<br>from torch_geometric.nn.models import GraphTransformer`                                              | **Optional** (`edge_attr` ↦ bias)               | Full-attention over nodes per snapshot, then stack with 1-D temporal SA.       | GPU okay, you want SOTA accuracy / explainability.                  |\n",
    "| **DGL-Spatio-Temporal modules**       | `from dgl.nn.pytorch import STConv, TGATConv …`                                                                 | **Yes**                                         | Same ideas but DGL backend (better for billion-edge later).                    | You ever migrate to huge graphs; code still looks like PyTorch.     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dad593",
   "metadata": {},
   "source": [
    "Quick pick-list\n",
    "Start simple: TGCN or STGCNBlock stack.\n",
    "\n",
    "Edge capacities matter? → switch to NNConv + GRU.\n",
    "\n",
    ">30-step history or multi-horizon? → Graph WaveNet or ASTGNN.\n",
    "\n",
    "Irregular event timestamps? → external TGN package.\n",
    "\n",
    "Want explainable attention & you have GPU slack? → GraphTransformer (PyG 2.4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84917fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jpx_stgnn.py  ─── Minimal end-to-end ST-GNN for nine-region price forecasts\n",
    "# ----------------------------------------------------------\n",
    "import numpy as np, pandas as pd, torch, torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric_temporal.signal import StaticGraphTemporalSignal\n",
    "from torch_geometric_temporal.nn.recurrent import TGCN\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────\n",
    "# 1. Pandas-dict  →  PyG-Temporal dataset\n",
    "# ────────────────────────────────────────────────────────────\n",
    "def build_temporal_graph_dataset(region_dfs: dict[str, pd.DataFrame],\n",
    "                                 adj: np.ndarray) -> StaticGraphTemporalSignal:\n",
    "    regions   = sorted(region_dfs)\n",
    "    timestamps = sorted(\n",
    "        set.intersection(*(set(df.index) for df in region_dfs.values()))\n",
    "    )\n",
    "\n",
    "    feat_seq, tgt_seq = [], []\n",
    "    for ts in timestamps:\n",
    "        feats, tgts = [], []\n",
    "        for r in regions:\n",
    "            row  = region_dfs[r].loc[ts]\n",
    "            tgts.append(row[\"target\"])\n",
    "            feats.append(row.drop(\"target\").to_numpy())\n",
    "        feat_seq.append(torch.tensor(np.vstack(feats), dtype=torch.float32))\n",
    "        tgt_seq .append(torch.tensor(tgts,          dtype=torch.float32))\n",
    "\n",
    "    edge_index  = torch.tensor(np.vstack(np.nonzero(adj)), dtype=torch.long)\n",
    "    edge_weight = torch.tensor(adj[np.nonzero(adj)],        dtype=torch.float32).view(-1)\n",
    "\n",
    "    return StaticGraphTemporalSignal(edge_index=edge_index,\n",
    "                                     edge_weight=edge_weight,\n",
    "                                     features=feat_seq,\n",
    "                                     targets=tgt_seq)\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────\n",
    "# 2. Dataset slicing helpers\n",
    "# ────────────────────────────────────────────────────────────\n",
    "def slice_region_dfs(region_dfs, idx):\n",
    "    return {n: df.loc[idx] for n, df in region_dfs.items()}\n",
    "\n",
    "def make_static_loaders(region_dfs, adj, train_end, batch=1):\n",
    "    all_idx   = sorted(region_dfs[next(iter(region_dfs))].index)\n",
    "    train_idx = pd.Index([t for t in all_idx if t <= pd.Timestamp(train_end)])\n",
    "    test_idx  = pd.Index([t for t in all_idx if t >  pd.Timestamp(train_end)])\n",
    "\n",
    "    train_ds = build_temporal_graph_dataset(slice_region_dfs(region_dfs, train_idx), adj)\n",
    "    test_ds  = build_temporal_graph_dataset(slice_region_dfs(region_dfs,  test_idx), adj)\n",
    "\n",
    "    dl_args = dict(batch_size=batch, shuffle=False)\n",
    "    return DataLoader(train_ds, **dl_args), DataLoader(test_ds, **dl_args)\n",
    "\n",
    "\n",
    "def walk_forward_splits(region_dfs, adj,\n",
    "                        start_train: str,\n",
    "                        step_days=30, horizon_days=30, batch=1):\n",
    "    all_idx = sorted(region_dfs[next(iter(region_dfs))].index)\n",
    "    t_k = pd.Timestamp(start_train)\n",
    "    last_trainable = all_idx[-1] - pd.Timedelta(days=horizon_days)\n",
    "\n",
    "    while t_k <= last_trainable:\n",
    "        train_idx = pd.Index([t for t in all_idx if t <= t_k])\n",
    "        test_idx  = pd.Index([t for t in all_idx\n",
    "                              if t_k < t <= t_k + pd.Timedelta(days=horizon_days)])\n",
    "\n",
    "        train_dl = DataLoader(\n",
    "            build_temporal_graph_dataset(slice_region_dfs(region_dfs, train_idx), adj),\n",
    "            batch_size=batch, shuffle=False)\n",
    "        test_dl  = DataLoader(\n",
    "            build_temporal_graph_dataset(slice_region_dfs(region_dfs, test_idx),  adj),\n",
    "            batch_size=batch, shuffle=False)\n",
    "\n",
    "        yield train_dl, test_dl, t_k\n",
    "        t_k += pd.Timedelta(days=step_days)\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────\n",
    "# 3. Model: one-layer TGCN + linear head\n",
    "#    (swap self.core for NNConv, GraphWaveNet, etc.)\n",
    "# ────────────────────────────────────────────────────────────\n",
    "class PriceTGCN(nn.Module):\n",
    "    def __init__(self, num_nodes, in_feats, hidden=64):\n",
    "        super().__init__()\n",
    "        self.core = TGCN(num_nodes=num_nodes,\n",
    "                         in_channels=in_feats,\n",
    "                         out_channels=hidden)\n",
    "        self.head = nn.Linear(hidden, 1)  # scalar per node\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        h = self.core(x, edge_index, edge_weight)\n",
    "        return self.head(h).squeeze(-1)   # → (N,)\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────\n",
    "# 4. Training & evaluation utilities\n",
    "# ────────────────────────────────────────────────────────────\n",
    "def run_epoch(model, loader, opt=None):\n",
    "    train_mode = opt is not None\n",
    "    model.train() if train_mode else model.eval()\n",
    "    loss_fn = nn.L1Loss()\n",
    "    total = 0.0\n",
    "\n",
    "    for X_t, y_t in loader:\n",
    "        X_t, y_t = X_t.squeeze(0), y_t.squeeze(0)\n",
    "        if train_mode:\n",
    "            opt.zero_grad()\n",
    "        y_hat = model(X_t, loader.dataset.edge_index, loader.dataset.edge_weight)\n",
    "        loss  = loss_fn(y_hat, y_t)\n",
    "        if train_mode:\n",
    "            loss.backward(); opt.step()\n",
    "        total += loss.item()\n",
    "\n",
    "    return total / len(loader)\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────\n",
    "# 5A. STATIC-SPLIT experiment  (comment out if not needed)\n",
    "# ────────────────────────────────────────────────────────────\n",
    "train_dl, test_dl = make_static_loaders(region_dfs, adjacency,\n",
    "                                        train_end=\"2024-06-30\")\n",
    "\n",
    "model = PriceTGCN(num_nodes=adjacency.shape[0],\n",
    "                  in_feats=train_dl.dataset.features[0].shape[1]).to(\"cpu\")\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(10):\n",
    "    mae = run_epoch(model, train_dl, opt)\n",
    "    print(f\"[Static] epoch {epoch:02d} train-MAE = {mae:.4f}\")\n",
    "\n",
    "print(\"Static-split test MAE:\", run_epoch(model, test_dl))\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────\n",
    "# 5B. WALK-FORWARD back-test  (comment out if not needed)\n",
    "# ────────────────────────────────────────────────────────────\n",
    "walk_mae = []\n",
    "for train_dl, test_dl, t_k in walk_forward_splits(region_dfs, adjacency,\n",
    "                                                 start_train=\"2021-01-01\",\n",
    "                                                 step_days=30, horizon_days=30):\n",
    "    model = PriceTGCN(num_nodes=adjacency.shape[0],\n",
    "                      in_feats=train_dl.dataset.features[0].shape[1]).to(\"cpu\")\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for _ in range(5):                          # 5 epochs per window\n",
    "        run_epoch(model, train_dl, opt)\n",
    "\n",
    "    mae = run_epoch(model, test_dl)\n",
    "    walk_mae.append((t_k.strftime(\"%Y-%m-%d\"), mae))\n",
    "    print(f\"[Walk] window end {t_k.date()}  test-MAE = {mae:.4f}\")\n",
    "\n",
    "print(\"Walk-forward MAE series:\", walk_mae)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
