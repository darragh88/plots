{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d357c0da",
   "metadata": {},
   "source": [
    "### Edge classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75e9b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnn_edge_clf.py\n",
    "# --------------------------------------------------------------\n",
    "# Dependencies: torch, torch_geometric, pandas, numpy, networkx\n",
    "# --------------------------------------------------------------\n",
    "import json, pathlib, math, random\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, Any, Optional, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCN, GraphSAGE, GAT, GATv2Conv\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1. Configuration\n",
    "# ---------------------------------------------------------------------\n",
    "@dataclass\n",
    "class GNNConfig:\n",
    "    model_name: str = \"gcn\"         # {\"gcn\",\"graphsage\",\"gat\"}\n",
    "    hidden_dim: int = 128\n",
    "    num_layers: int = 2\n",
    "    heads: int = 8                  # for GAT*\n",
    "    layer_dropout: float = 0.3\n",
    "\n",
    "    # optimisation\n",
    "    batch_size: int = 32\n",
    "    lr: float = 1e-3\n",
    "    epochs: int = 100\n",
    "    patience: int = 15\n",
    "    grad_clip: float = 1.0\n",
    "\n",
    "    # split\n",
    "    split_mode: str = \"date\"        # {\"date\",\"ratio\"}\n",
    "    cutoff_date: Optional[str] = None\n",
    "    val_ratio: float = 0.10\n",
    "    test_ratio: float = 0.10\n",
    "    shuffle_in_split: bool = False\n",
    "\n",
    "    # misc\n",
    "    device: str = \"cuda\"\n",
    "    seed: int = 42\n",
    "\n",
    "    # edge imbalance override\n",
    "    edge_pos_weights: Optional[List[float]] = None\n",
    "\n",
    "    @staticmethod\n",
    "    def load(x):                     # allow json / dict / object\n",
    "        if isinstance(x, GNNConfig):\n",
    "            return x\n",
    "        if isinstance(x, dict):\n",
    "            return GNNConfig(**x)\n",
    "        p = pathlib.Path(x)\n",
    "        with open(p) as f:\n",
    "            return GNNConfig(**json.load(f))\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2. Mini Encoder Zoo  (node → z)\n",
    "# ---------------------------------------------------------------------\n",
    "class EncoderGCN(nn.Module):\n",
    "    def __init__(self, d_in, h, L, p):\n",
    "        super().__init__()\n",
    "        self.body = GCN(d_in, h, L, norm=\"batch\", act=\"relu\")\n",
    "        self.drop = nn.Dropout(p)\n",
    "    def forward(self, x, ei, ea):\n",
    "        return self.drop(self.body(x, ei))\n",
    "\n",
    "class EncoderSAGE(nn.Module):\n",
    "    def __init__(self, d_in, h, L, p):\n",
    "        super().__init__()\n",
    "        self.body = GraphSAGE(d_in, h, L, norm=\"batch\", act=\"relu\")\n",
    "        self.drop = nn.Dropout(p)\n",
    "    def forward(self, x, ei, ea):\n",
    "        return self.drop(self.body(x, ei))\n",
    "\n",
    "class EncoderGAT(nn.Module):\n",
    "    def __init__(self, d_in, h, L, heads, p):\n",
    "        super().__init__()\n",
    "        self.body = GAT(d_in, h, L, heads=heads, norm=\"batch\", act=\"relu\")\n",
    "        self.drop = nn.Dropout(p)\n",
    "    def forward(self, x, ei, ea):\n",
    "        return self.drop(self.body(x, ei))\n",
    "\n",
    "ENCODER_FACTORY = {\n",
    "    \"gcn\":   EncoderGCN,\n",
    "    \"graphsage\": EncoderSAGE,\n",
    "    \"sage\":  EncoderSAGE,\n",
    "    \"gat\":   EncoderGAT,\n",
    "}\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3. Edge decoder  (z_u , z_v) → logit\n",
    "# ---------------------------------------------------------------------\n",
    "class DotDecoder(nn.Module):\n",
    "    def forward(self, z_src, z_dst):\n",
    "        return (z_src * z_dst).sum(-1, keepdim=True)  # (E,1)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4. Experiment wrapper\n",
    "# ---------------------------------------------------------------------\n",
    "class EdgeExperiment:\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_frames: Dict[str, pd.DataFrame],\n",
    "        edge_frames: Dict[str, pd.DataFrame],\n",
    "        graph: nx.DiGraph,\n",
    "        cfg: GNNConfig | Dict[str, Any] | str = GNNConfig(),\n",
    "    ):\n",
    "        self.cfg = GNNConfig.load(cfg)\n",
    "        torch.manual_seed(self.cfg.seed); np.random.seed(self.cfg.seed); random.seed(self.cfg.seed)\n",
    "\n",
    "        self.node_frames = node_frames\n",
    "        self.edge_frames = edge_frames\n",
    "        self.graph = graph\n",
    "\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if self.cfg.device==\"cuda\" and torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "\n",
    "        # placeholders\n",
    "        self.edge_order: List[Tuple[int,int]] = []\n",
    "        self.snapshots: List[Data] = []\n",
    "\n",
    "    # -------------------------------------------- snapshot builder\n",
    "    def prepare_snapshots(self):\n",
    "        nodes = sorted(self.graph.nodes)\n",
    "        n2idx = {n:i for i,n in enumerate(nodes)}\n",
    "        self.edge_order = [(n2idx[u], n2idx[v]) for u,v in self.graph.edges]\n",
    "        E = len(self.edge_order)\n",
    "\n",
    "        # intersect timestamps\n",
    "        idxs = [df.index for df in self.node_frames.values()] \\\n",
    "             + [df.index for df in self.edge_frames.values()]\n",
    "        ts_common = sorted(set.intersection(*map(set, idxs)))\n",
    "\n",
    "        snaps: List[Data] = []\n",
    "        for ts in ts_common:\n",
    "            # node feats\n",
    "            x = torch.tensor(\n",
    "                np.vstack([ self.node_frames[n].loc[ts].to_numpy(dtype=np.float32)\n",
    "                            for n in nodes ]),\n",
    "                dtype=torch.float32)\n",
    "            # edge feats & labels\n",
    "            edge_feat_rows, edge_labels = [], []\n",
    "            for (s_idx,d_idx) in self.edge_order:\n",
    "                s,d = nodes[s_idx], nodes[d_idx]\n",
    "                row = self.edge_frames[f\"{s}-{d}\".lower()].loc[ts]\n",
    "                edge_labels.append(row[\"target\"])\n",
    "                edge_feat_rows.append(row.drop(\"target\").to_numpy(dtype=np.float32))\n",
    "            edge_attr = torch.tensor(np.vstack(edge_feat_rows), dtype=torch.float32)\n",
    "            edge_label = torch.tensor(edge_labels, dtype=torch.float32)\n",
    "            edge_index = torch.tensor(np.array(self.edge_order).T, dtype=torch.long)\n",
    "\n",
    "            snaps.append(Data(\n",
    "                x=x, edge_index=edge_index, edge_attr=edge_attr,\n",
    "                edge_label=edge_label,\n",
    "                snap_time=torch.tensor([pd.Timestamp(ts).value]),\n",
    "            ))\n",
    "        self.snapshots = snaps\n",
    "        return self\n",
    "\n",
    "    # -------------------------------------------- train/val/test split\n",
    "    def build_loaders(self):\n",
    "        snaps_sorted = sorted(self.snapshots, key=lambda g:g.snap_time.item())\n",
    "        if self.cfg.split_mode==\"date\":\n",
    "            cut = pd.Timestamp(self.cfg.cutoff_date).value\n",
    "            train = [g for g in snaps_sorted if g.snap_time.item()<=cut]\n",
    "            hold  = [g for g in snaps_sorted if g.snap_time.item()>cut]\n",
    "            mid = len(hold)//2\n",
    "            val, test = hold[:mid], hold[mid:]\n",
    "        else:\n",
    "            n=len(snaps_sorted)\n",
    "            nv = int(n*self.cfg.val_ratio); nt=int(n*self.cfg.test_ratio)\n",
    "            train, val, test = snaps_sorted[:n-nv-nt], snaps_sorted[n-nv-nt:n-nt], snaps_sorted[n-nt:]\n",
    "        self._compute_edge_weights(train)                     # <- imbalance weights\n",
    "\n",
    "        bs = self.cfg.batch_size\n",
    "        self.train_dl = DataLoader(train, batch_size=bs, shuffle=self.cfg.shuffle_in_split)\n",
    "        self.val_dl   = DataLoader(val,   batch_size=bs, shuffle=False)\n",
    "        self.test_dl  = DataLoader(test,  batch_size=bs, shuffle=False)\n",
    "        return self\n",
    "\n",
    "    # -------------------------------------------- imbalance weights\n",
    "    def _compute_edge_weights(self, train_snaps: List[Data]):\n",
    "        E = len(self.edge_order)\n",
    "        pos = torch.zeros(E); neg = torch.zeros(E)\n",
    "        for g in train_snaps:\n",
    "            pos += g.edge_label\n",
    "            neg += 1 - g.edge_label\n",
    "        w = neg / (pos + 1e-6)\n",
    "        if self.cfg.edge_pos_weights is not None:\n",
    "            w = torch.tensor(self.cfg.edge_pos_weights, dtype=torch.float32)\n",
    "        # attach to every snapshot (clone so grads aren’t tracked)\n",
    "        for g in self.snapshots:\n",
    "            g.edge_weight = w.clone()\n",
    "        self.edge_weight = w             # store for metrics\n",
    "\n",
    "    # -------------------------------------------- model init\n",
    "    def init_model(self):\n",
    "        d_in  = self.snapshots[0].x.size(1)\n",
    "        d_e   = self.snapshots[0].edge_attr.size(1)\n",
    "        enc_cls = ENCODER_FACTORY[self.cfg.model_name.lower()]\n",
    "        enc = enc_cls(d_in, self.cfg.hidden_dim, self.cfg.num_layers,\n",
    "                      self.cfg.layer_dropout, **({\"heads\":self.cfg.heads} if \"gat\" in self.cfg.model_name else {}))\n",
    "        dec = DotDecoder()\n",
    "        self.model = nn.ModuleDict({\"enc\":enc, \"dec\":dec}).to(self.device)\n",
    "        return self\n",
    "\n",
    "    # -------------------------------------------- optimiser / loss\n",
    "    def compile(self):\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "        self.opt = torch.optim.Adam(self.model.parameters(), lr=self.cfg.lr)\n",
    "\n",
    "    # -------------------------------------------- train helpers\n",
    "    def _forward_edges(self, data:Data):\n",
    "        z = self.model[\"enc\"](data.x, data.edge_index, data.edge_attr)\n",
    "        src,dst = data.edge_index        # edges are aligned with labels\n",
    "        logits = self.model[\"dec\"](z[src], z[dst]).squeeze(-1)\n",
    "        return logits\n",
    "\n",
    "    def _run_epoch(self, loader, train:bool):\n",
    "        if train: self.model.train();   torch.set_grad_enabled(True)\n",
    "        else:     self.model.eval();    torch.set_grad_enabled(False)\n",
    "\n",
    "        tot_loss = 0.0\n",
    "        for data in loader:\n",
    "            data = data.to(self.device)\n",
    "            logits = self._forward_edges(data)\n",
    "            loss_vec = self.loss_fn(logits, data.edge_label)\n",
    "            loss = (loss_vec * data.edge_weight.to(loss_vec)).mean()\n",
    "\n",
    "            if train:\n",
    "                self.opt.zero_grad(); loss.backward()\n",
    "                if self.cfg.grad_clip: nn.utils.clip_grad_norm_(self.model.parameters(), self.cfg.grad_clip)\n",
    "                self.opt.step()\n",
    "            tot_loss += loss.item()\n",
    "        return tot_loss/len(loader)\n",
    "\n",
    "    # -------------------------------------------- fit\n",
    "    def train(self):\n",
    "        best,val_best=1e9,0; patience=0\n",
    "        for epoch in range(1, self.cfg.epochs+1):\n",
    "            tr = self._run_epoch(self.train_dl, True)\n",
    "            vl = self._run_epoch(self.val_dl,   False)\n",
    "            print(f\"[{epoch:03d}] train {tr:.4f} | val {vl:.4f}\")\n",
    "            if vl<best: best=vl; patience=0; self.best_state= {k:v.cpu() for k,v in self.model.state_dict().items()}\n",
    "            else:       patience+=1\n",
    "            if patience>=self.cfg.patience:\n",
    "                print(\"Early stop.\"); break\n",
    "        self.model.load_state_dict(self.best_state)\n",
    "\n",
    "    # -------------------------------------------- predict on list of snapshots (loader)\n",
    "    @torch.no_grad()\n",
    "    def predict_loader(self, loader:DataLoader):\n",
    "        self.model.eval()\n",
    "        all_logits=[]\n",
    "        for data in loader:\n",
    "            data=data.to(self.device)\n",
    "            logits=self._forward_edges(data)\n",
    "            all_logits.append(logits.cpu())\n",
    "        return torch.cat(all_logits)    # concatenated over batches/snapshots\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Usage sketch (not executed here):\n",
    "# ---------------------------------------------------------------------\n",
    "#   exp = (EdgeExperiment(node_frames, edge_frames, graph)\n",
    "#            .prepare_snapshots()\n",
    "#            .build_loaders()\n",
    "#            .init_model()\n",
    "#            .compile())\n",
    "#   exp.train()\n",
    "#   test_logits = exp.predict_loader(exp.test_dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb4a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# 0. Imports\n",
    "# ---------------------------------------------------------------------\n",
    "import json, pathlib, random\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, Optional, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCN, GraphSAGE, GAT\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1. Configuration\n",
    "# ---------------------------------------------------------------------\n",
    "@dataclass\n",
    "class GNNConfig:\n",
    "    # encoder\n",
    "    model_name: str = \"gcn\"            # {\"gcn\",\"graphsage\",\"gat\"}\n",
    "    hidden_dim: int = 128\n",
    "    num_layers: int = 2\n",
    "    heads: int = 8                     # only used by GAT\n",
    "    layer_dropout: float = 0.3\n",
    "\n",
    "    # decoder\n",
    "    decoder: str = \"dot\"               # {\"dot\",\"concat_mlp\",\"hadamard_mlp\",\"bilinear\"}\n",
    "    mlp_hidden: int = 128              # hidden dim for *_mlp decoders\n",
    "\n",
    "    # optimisation\n",
    "    batch_size: int = 32\n",
    "    lr: float = 1e-3\n",
    "    epochs: int = 100\n",
    "    patience: int = 15                 # early-stop window\n",
    "    grad_clip: float = 1.0\n",
    "\n",
    "    # split\n",
    "    split_mode: str = \"date\"           # {\"date\",\"ratio\"}\n",
    "    cutoff_date: Optional[str] = None\n",
    "    val_ratio: float = 0.10\n",
    "    test_ratio: float = 0.10\n",
    "    shuffle_in_split: bool = False\n",
    "\n",
    "    # misc\n",
    "    device: str = \"cuda\"\n",
    "    seed: int = 42\n",
    "\n",
    "    # edge imbalance override\n",
    "    edge_pos_weights: Optional[List[float]] = None\n",
    "\n",
    "    # ------------- helper\n",
    "    @staticmethod\n",
    "    def load(x):\n",
    "        if isinstance(x, GNNConfig):\n",
    "            return x\n",
    "        if isinstance(x, dict):\n",
    "            return GNNConfig(**x)\n",
    "        with open(pathlib.Path(x)) as f:\n",
    "            return GNNConfig(**json.load(f))\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2. Mini encoder zoo  (node → z)\n",
    "# ---------------------------------------------------------------------\n",
    "class EncoderGCN(nn.Module):\n",
    "    def __init__(self, d_in, h, L, p):\n",
    "        super().__init__()\n",
    "        self.body = GCN(d_in, h, L, norm=\"batch\", act=\"relu\")\n",
    "        self.drop = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x, ei, ea):\n",
    "        return self.drop(self.body(x, ei))\n",
    "\n",
    "\n",
    "class EncoderSAGE(nn.Module):\n",
    "    def __init__(self, d_in, h, L, p):\n",
    "        super().__init__()\n",
    "        self.body = GraphSAGE(d_in, h, L, norm=\"batch\", act=\"relu\")\n",
    "        self.drop = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x, ei, ea):\n",
    "        return self.drop(self.body(x, ei))\n",
    "\n",
    "\n",
    "class EncoderGAT(nn.Module):\n",
    "    def __init__(self, d_in, h, L, heads, p):\n",
    "        super().__init__()\n",
    "        self.body = GAT(d_in, h, L, heads=heads, norm=\"batch\", act=\"relu\")\n",
    "        self.drop = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x, ei, ea):\n",
    "        return self.drop(self.body(x, ei))\n",
    "\n",
    "\n",
    "ENCODER_FACTORY = {\n",
    "    \"gcn\":       EncoderGCN,\n",
    "    \"graphsage\": EncoderSAGE,\n",
    "    \"sage\":      EncoderSAGE,\n",
    "    \"gat\":       EncoderGAT,\n",
    "}\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3. Decoder zoo  (z_u , z_v) → logit\n",
    "# ---------------------------------------------------------------------\n",
    "class DotDecoder(nn.Module):\n",
    "    \"\"\"〈z_u ⊙ z_v〉\"\"\"\n",
    "    def forward(self, z_src, z_dst):\n",
    "        return (z_src * z_dst).sum(-1, keepdim=True)\n",
    "\n",
    "\n",
    "class ConcatMLPDecoder(nn.Module):\n",
    "    \"\"\"MLP on [z_u || z_v]\"\"\"\n",
    "    def __init__(self, d_in, hidden):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2 * d_in, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, z_src, z_dst):\n",
    "        return self.mlp(torch.cat([z_src, z_dst], -1))\n",
    "\n",
    "\n",
    "class HadamardMLPDecoder(nn.Module):\n",
    "    \"\"\"MLP on (z_u ⊙ z_v)\"\"\"\n",
    "    def __init__(self, d_in, hidden):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_in, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, z_src, z_dst):\n",
    "        return self.mlp(z_src * z_dst)\n",
    "\n",
    "\n",
    "class BilinearDecoder(nn.Module):\n",
    "    \"\"\"z_uᵀ W z_v\"\"\"\n",
    "    def __init__(self, d_in):\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(torch.empty(d_in, d_in))\n",
    "        nn.init.xavier_uniform_(self.W)\n",
    "\n",
    "    def forward(self, z_src, z_dst):\n",
    "        return (z_src @ self.W * z_dst).sum(-1, keepdim=True)\n",
    "\n",
    "\n",
    "DECODER_FACTORY = {\n",
    "    \"dot\":          DotDecoder,\n",
    "    \"concat_mlp\":   ConcatMLPDecoder,\n",
    "    \"hadamard_mlp\": HadamardMLPDecoder,\n",
    "    \"bilinear\":     BilinearDecoder,\n",
    "}\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4. Experiment wrapper\n",
    "# ---------------------------------------------------------------------\n",
    "class EdgeExperiment:\n",
    "    # ------------------------------------------------ constructor\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_frames: Dict[str, pd.DataFrame],\n",
    "        edge_frames: Dict[str, pd.DataFrame],\n",
    "        graph: nx.DiGraph,\n",
    "        cfg: GNNConfig | Dict[str, Any] | str = GNNConfig(),\n",
    "    ):\n",
    "        self.cfg = GNNConfig.load(cfg)\n",
    "        torch.manual_seed(self.cfg.seed)\n",
    "        np.random.seed(self.cfg.seed)\n",
    "        random.seed(self.cfg.seed)\n",
    "\n",
    "        self.node_frames = node_frames\n",
    "        self.edge_frames = edge_frames\n",
    "        self.graph = graph\n",
    "\n",
    "        self.device = torch.device(\n",
    "            \"cuda\"\n",
    "            if self.cfg.device == \"cuda\" and torch.cuda.is_available()\n",
    "            else \"cpu\"\n",
    "        )\n",
    "\n",
    "        # placeholders\n",
    "        self.edge_order: List[Tuple[int, int]] = []\n",
    "        self.snapshots: List[Data] = []\n",
    "        self.history = {\"train\": [], \"val\": []}\n",
    "\n",
    "    # ------------------------------------------------ snapshot builder\n",
    "    def prepare_snapshots(self):\n",
    "        nodes = sorted(self.graph.nodes)\n",
    "        n2idx = {n: i for i, n in enumerate(nodes)}\n",
    "        self.edge_order = [(n2idx[u], n2idx[v]) for u, v in self.graph.edges]\n",
    "\n",
    "        # — intersect timestamps\n",
    "        idxs = [df.index for df in self.node_frames.values()] + [\n",
    "            df.index for df in self.edge_frames.values()\n",
    "        ]\n",
    "        ts_common = sorted(set.intersection(*map(set, idxs)))\n",
    "        if not ts_common:\n",
    "            raise ValueError(\"No common timestamps across all node/edge frames.\")\n",
    "\n",
    "        snaps: List[Data] = []\n",
    "        for ts in ts_common:\n",
    "            # node features\n",
    "            x = torch.tensor(\n",
    "                np.vstack(\n",
    "                    [\n",
    "                        self.node_frames[n].loc[ts].to_numpy(dtype=np.float32)\n",
    "                        for n in nodes\n",
    "                    ]\n",
    "                ),\n",
    "                dtype=torch.float32,\n",
    "            )\n",
    "\n",
    "            # edge feats & labels\n",
    "            edge_feat_rows, edge_labels = [], []\n",
    "            for (s_idx, d_idx) in self.edge_order:\n",
    "                s, d = nodes[s_idx], nodes[d_idx]\n",
    "                row = self.edge_frames[f\"{s}-{d}\".lower()].loc[ts]\n",
    "                edge_labels.append(row[\"target\"])\n",
    "                edge_feat_rows.append(\n",
    "                    row.drop(\"target\").to_numpy(dtype=np.float32)\n",
    "                )\n",
    "\n",
    "            edge_attr = torch.tensor(np.vstack(edge_feat_rows), dtype=torch.float32)\n",
    "            edge_label = torch.tensor(edge_labels, dtype=torch.float32)\n",
    "            edge_index = torch.tensor(\n",
    "                np.array(self.edge_order).T, dtype=torch.long\n",
    "            )\n",
    "\n",
    "            snaps.append(\n",
    "                Data(\n",
    "                    x=x,\n",
    "                    edge_index=edge_index,\n",
    "                    edge_attr=edge_attr,\n",
    "                    edge_label=edge_label,\n",
    "                    snap_time=torch.tensor([pd.Timestamp(ts).value]),\n",
    "                )\n",
    "            )\n",
    "        self.snapshots = snaps\n",
    "        return self\n",
    "\n",
    "    # ------------------------------------------------ train/val/test split\n",
    "    def build_loaders(self):\n",
    "        snaps_sorted = sorted(self.snapshots, key=lambda g: g.snap_time.item())\n",
    "        if self.cfg.split_mode == \"date\":\n",
    "            cut = pd.Timestamp(self.cfg.cutoff_date).value\n",
    "            train = [g for g in snaps_sorted if g.snap_time.item() <= cut]\n",
    "            hold = [g for g in snaps_sorted if g.snap_time.item() > cut]\n",
    "            mid = len(hold) // 2\n",
    "            val, test = hold[:mid], hold[mid:]\n",
    "        else:\n",
    "            n = len(snaps_sorted)\n",
    "            nv = int(n * self.cfg.val_ratio)\n",
    "            nt = int(n * self.cfg.test_ratio)\n",
    "            train = snaps_sorted[: n - nv - nt]\n",
    "            val = snaps_sorted[n - nv - nt : n - nt]\n",
    "            test = snaps_sorted[n - nt :]\n",
    "\n",
    "        self._compute_edge_weights(train)\n",
    "\n",
    "        bs = self.cfg.batch_size\n",
    "        self.train_dl = DataLoader(train, batch_size=bs, shuffle=self.cfg.shuffle_in_split)\n",
    "        self.val_dl = DataLoader(val, batch_size=bs, shuffle=False)\n",
    "        self.test_dl = DataLoader(test, batch_size=bs, shuffle=False)\n",
    "        return self\n",
    "\n",
    "    # ------------------------------------------------ imbalance weights\n",
    "    def _compute_edge_weights(self, train_snaps: List[Data]):\n",
    "        E = len(self.edge_order)\n",
    "        pos = torch.zeros(E)\n",
    "        neg = torch.zeros(E)\n",
    "        for g in train_snaps:\n",
    "            pos += g.edge_label\n",
    "            neg += 1 - g.edge_label\n",
    "        w = neg / (pos + 1)  # +1 for stability\n",
    "\n",
    "        if self.cfg.edge_pos_weights is not None:\n",
    "            w = torch.tensor(self.cfg.edge_pos_weights, dtype=torch.float32)\n",
    "\n",
    "        for g in self.snapshots:  # attach to every snapshot\n",
    "            g.edge_weight = w.clone()\n",
    "        self.edge_weight = w\n",
    "\n",
    "    # ------------------------------------------------ model init\n",
    "    def init_model(self):\n",
    "        d_in = self.snapshots[0].x.size(1)\n",
    "\n",
    "        # ---------- encoder\n",
    "        enc_cls = ENCODER_FACTORY[self.cfg.model_name.lower()]\n",
    "        enc_kw = {\"heads\": self.cfg.heads} if \"gat\" in self.cfg.model_name.lower() else {}\n",
    "        enc = enc_cls(\n",
    "            d_in,\n",
    "            self.cfg.hidden_dim,\n",
    "            self.cfg.num_layers,\n",
    "            self.cfg.layer_dropout,\n",
    "            **enc_kw,\n",
    "        )\n",
    "\n",
    "        # latent dim (GAT stacks heads)\n",
    "        d_lat = (\n",
    "            self.cfg.hidden_dim * self.cfg.heads\n",
    "            if \"gat\" in self.cfg.model_name.lower()\n",
    "            else self.cfg.hidden_dim\n",
    "        )\n",
    "\n",
    "        # ---------- decoder\n",
    "        dec_type = self.cfg.decoder.lower()\n",
    "        dec_cls = DECODER_FACTORY[dec_type]\n",
    "        if dec_type == \"concat_mlp\":\n",
    "            dec = dec_cls(d_lat, self.cfg.mlp_hidden)\n",
    "        elif dec_type == \"hadamard_mlp\":\n",
    "            dec = dec_cls(d_lat, self.cfg.mlp_hidden)\n",
    "        elif dec_type == \"bilinear\":\n",
    "            dec = dec_cls(d_lat)\n",
    "        else:  # dot\n",
    "            dec = dec_cls()\n",
    "\n",
    "        self.model = nn.ModuleDict({\"enc\": enc, \"dec\": dec}).to(self.device)\n",
    "        return self\n",
    "\n",
    "    # ------------------------------------------------ optimiser / loss / scheduler\n",
    "    def compile(self):\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "        self.opt = torch.optim.Adam(self.model.parameters(), lr=self.cfg.lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.opt, mode=\"min\", factor=0.5, patience=5, verbose=False\n",
    "        )\n",
    "        self._prev_lr = self.opt.param_groups[0][\"lr\"]\n",
    "\n",
    "    # ------------------------------------------------ helper: forward edges\n",
    "    def _forward_edges(self, data: Data):\n",
    "        z = self.model[\"enc\"](data.x, data.edge_index, data.edge_attr)\n",
    "        src, dst = data.edge_index  # aligned with labels\n",
    "        logits = self.model[\"dec\"](z[src], z[dst]).squeeze(-1)\n",
    "        return logits\n",
    "\n",
    "    # ------------------------------------------------ run one epoch\n",
    "    def _run_epoch(self, loader: DataLoader, train: bool):\n",
    "        self.model.train(train)\n",
    "        torch.set_grad_enabled(train)\n",
    "\n",
    "        total = 0.0\n",
    "        for data in loader:\n",
    "            data = data.to(self.device)\n",
    "            logits = self._forward_edges(data)\n",
    "            y = data.edge_label\n",
    "            loss_vec = self.loss_fn(logits, y)\n",
    "\n",
    "            # ----- per-edge positive weighting\n",
    "            w_pos = data.edge_weight.to(loss_vec)\n",
    "            weights = 1 + (w_pos - 1) * y  # scale positive terms only\n",
    "            loss = (loss_vec * weights).mean()\n",
    "\n",
    "            if train:\n",
    "                self.opt.zero_grad()\n",
    "                loss.backward()\n",
    "                if self.cfg.grad_clip:\n",
    "                    nn.utils.clip_grad_norm_(self.model.parameters(), self.cfg.grad_clip)\n",
    "                self.opt.step()\n",
    "\n",
    "            total += loss.item()\n",
    "\n",
    "        return total / len(loader)\n",
    "\n",
    "    # ------------------------------------------------ full training loop\n",
    "    def train(self):\n",
    "        best = float(\"inf\")\n",
    "        patience = 0\n",
    "\n",
    "        for epoch in range(1, self.cfg.epochs + 1):\n",
    "            tr_loss = self._run_epoch(self.train_dl, True)\n",
    "            val_loss = self._run_epoch(self.val_dl, False)\n",
    "\n",
    "            # record for plotting\n",
    "            self.history[\"train\"].append(tr_loss)\n",
    "            self.history[\"val\"].append(val_loss)\n",
    "\n",
    "            # LR scheduler\n",
    "            self.scheduler.step(val_loss)\n",
    "            curr_lr = self.opt.param_groups[0][\"lr\"]\n",
    "            if curr_lr != self._prev_lr:\n",
    "                print(\n",
    "                    f\"Epoch {epoch:03d}: LR reduced from {self._prev_lr:.2e} → {curr_lr:.2e}\"\n",
    "                )\n",
    "                self._prev_lr = curr_lr\n",
    "\n",
    "            print(f\"[{epoch:03d}] train {tr_loss:.4f} | val {val_loss:.4f}\")\n",
    "\n",
    "            # early stopping\n",
    "            if val_loss < best:\n",
    "                best, patience = val_loss, 0\n",
    "                self.best_state = {k: v.cpu() for k, v in self.model.state_dict().items()}\n",
    "            else:\n",
    "                patience += 1\n",
    "            if patience >= self.cfg.patience:\n",
    "                print(\"Early stop.\")\n",
    "                break\n",
    "\n",
    "        self.model.load_state_dict(self.best_state)\n",
    "\n",
    "    # ------------------------------------------------ quick-look plot\n",
    "    def plot_history(self, show: bool = True, savepath: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Call after .train() to visualise loss curves.\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(self.history[\"train\"], label=\"Train\")\n",
    "        plt.plot(self.history[\"val\"], label=\"Val\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training vs Validation Loss\")\n",
    "        plt.legend()\n",
    "        if savepath:\n",
    "            plt.savefig(savepath, bbox_inches=\"tight\")\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "    # ------------------------------------------------ inference util\n",
    "    @torch.no_grad()\n",
    "    def predict_loader(self, loader: DataLoader):\n",
    "        self.model.eval()\n",
    "        logits_all = []\n",
    "        for data in loader:\n",
    "            data = data.to(self.device)\n",
    "            logits_all.append(self._forward_edges(data).cpu())\n",
    "        return torch.cat(logits_all)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
