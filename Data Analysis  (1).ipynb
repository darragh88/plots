{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234dfd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def build_feature_dataset(\n",
    "    input_paths: list[str],\n",
    "    output_path: str,\n",
    "    region: str,\n",
    "    cols: list[str],\n",
    "    freq: str = \"30T\",\n",
    "    plot: bool = True\n",
    ") -> tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    Loading Features \n",
    "    Args:\n",
    "        input_paths:    List of Parquet file paths. Each file must load into a DataFrame\n",
    "                        whose columns are a MultiIndex with levels [region, variable_name].\n",
    "        output_path:    File‐path (including filename) where the feature report should be written.\n",
    "        region:         The first‐level column key (region) to subset by after concatenation.\n",
    "        cols:           A list of variable names (second‐level columns) to keep, once we subset to `region`.\n",
    "        freq:           A Pandas offset alias (e.g. \"30T\", \"15T\", \"1H\") used to resample each DataFrame.\n",
    "                        Default is \"30T\".\n",
    "        plot:           If True, calls `generate_feature_report(...)` on the final feature set.\n",
    "\n",
    "    Returns:\n",
    "        feat:   A DataFrame of shape [n_samples × n_features], containing:\n",
    "                • the time‐features (weekday, hour, month, etc.),\n",
    "                • the chosen columns in `cols`,\n",
    "                • and any newly added columns (forward‐filled) for modeling.\n",
    "        tar:    A pd.Series named \"Imbalance_Minus_Spot\", aligned with `feat.index`, \n",
    "                containing the (imbalance_price − spot_price) at each timestamp.\n",
    "    \"\"\"\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # Helpers for timezone‐normalization + resampling\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    def _load_and_resample_one(path: str, freq_rule: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Loads one Parquet file into a DataFrame with a DateTimeIndex, normalizes\n",
    "        its index to Asia/Tokyo, and resamples to `freq_rule` using .mean().\n",
    "        \"\"\"\n",
    "        df = pd.read_parquet(path)\n",
    "\n",
    "        # Ensure index is datetime:\n",
    "        df = df.copy()\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "\n",
    "        # If tz‐naive → assume it's already JST, so localize to Asia/Tokyo.\n",
    "        # If tz‐aware (e.g. UTC or anything), convert to Asia/Tokyo.\n",
    "        if df.index.tz is None:\n",
    "            df.index = df.index.tz_localize(\"Asia/Tokyo\")\n",
    "        else:\n",
    "            df.index = df.index.tz_convert(\"Asia/Tokyo\")\n",
    "\n",
    "        # Resample to the requested frequency, taking the mean of each\n",
    "        # (e.g. if `freq_rule=\"30T\"`, each 30‐minute block is averaged).\n",
    "        df_resampled = df.resample(freq_rule).mean()\n",
    "        return df_resampled\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # 1) Load + resample each input DataFrame; collect start/end times\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    loaded_dfs = []\n",
    "    start_times = []\n",
    "    end_times = []\n",
    "\n",
    "    for path in input_paths:\n",
    "        df_resampled = _load_and_resample_one(path, freq)\n",
    "        loaded_dfs.append(df_resampled)\n",
    "\n",
    "        # Record the new index range\n",
    "        start_times.append(df_resampled.index.min())\n",
    "        end_times.append(df_resampled.index.max())\n",
    "\n",
    "    if not loaded_dfs:\n",
    "        raise ValueError(\"`input_paths` must contain at least one parquet file.\")\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # 2) Find the common date‐range: [latest_start, earliest_end]\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    latest_start = max(start_times)\n",
    "    earliest_end = min(end_times)\n",
    "\n",
    "    if latest_start >= earliest_end:\n",
    "        raise ValueError(\n",
    "            f\"No overlapping time‐range found among the loaded files. \"\n",
    "            f\"latest_start={latest_start}, earliest_end={earliest_end}\"\n",
    "        )\n",
    "\n",
    "    # 3) Truncate each DataFrame to [latest_start : earliest_end]\n",
    "    aligned_dfs = [\n",
    "        df.loc[latest_start : earliest_end] for df in loaded_dfs\n",
    "    ]\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # 4) Concatenate side‐by‐side (axis=1)\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # Since each df had columns = MultiIndex [region, variable], \n",
    "    # the concatenation keeps the same MultiIndex column structure.\n",
    "    concatenated = pd.concat(aligned_dfs, axis=1)\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # 5) Subset by region (first level) and then by `cols` (second level)\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # This picks out one “slice” of the MultiIndex at level=0 == region.\n",
    "    try:\n",
    "        df_region = concatenated[region]\n",
    "    except KeyError:\n",
    "        raise KeyError(f\"Region '{region}' not found in the concatenated columns.\")\n",
    "\n",
    "    # Now df_region’s columns are the second level only. We keep exactly `cols`.\n",
    "    missing = [c for c in cols if c not in df_region.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"The following requested columns are not present for region {region}: {missing}\")\n",
    "\n",
    "    df_region = df_region[cols]\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # 6) Find the first+last index where BOTH spot & imbalance are non‐NaN\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    imb_col = \"pri_imb_down_%_kwh_jst_min30_a\"\n",
    "    spot_col = \"pri_spot_jepx_%_kwh_jst_min30_a\"\n",
    "\n",
    "    # Ensure those two are in `cols` (or else we can’t form the target)\n",
    "    if imb_col not in df_region.columns or spot_col not in df_region.columns:\n",
    "        raise KeyError(\n",
    "            f\"Cannot find both target columns ('{imb_col}' and '{spot_col}') in df_region. \"\n",
    "            f\"Got columns={list(df_region.columns)}\"\n",
    "        )\n",
    "\n",
    "    # Build a mask where both are non‐NaN:\n",
    "    both_valid = (\n",
    "        df_region[imb_col].notna() &\n",
    "        df_region[spot_col].notna()\n",
    "    )\n",
    "    # If there is no timestamp where both are valid, it's an error:\n",
    "    if not both_valid.any():\n",
    "        raise ValueError(\n",
    "            f\"No timestamp exists where both '{imb_col}' and '{spot_col}' are non‐NaN.\"\n",
    "        )\n",
    "\n",
    "    valid_times = df_region.index[both_valid]\n",
    "    crop_start = valid_times.min()\n",
    "    crop_end = valid_times.max()\n",
    "\n",
    "    # Crop the DataFrame so that the first row has both non‐NaN, and the last row has both non‐NaN\n",
    "    df_region = df_region.loc[crop_start : crop_end]\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # 7) Forward‐fill any remaining NaNs (limit=1)\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    df_region = df_region.ffill(limit=1)\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # 8) Construct time‐features\n",
    "    #    (Assumes you already have a function `construct_time_features(df)` defined elsewhere.)\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    construct_time_features(df_region)\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # 9) Create the target series: \"Imbalance_Minus_Spot\"\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    tar = df_region[imb_col] - df_region[spot_col]\n",
    "    tar.name = \"Imbalance_Minus_Spot\"\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # 10) Optionally generate a feature report\n",
    "    #    (Assumes you already have `generate_feature_report(...)` imported.)\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    if plot:\n",
    "        # name=\"Features\" is arbitrary; you can change if you like\n",
    "        generate_feature_report(\n",
    "            features=df_region,\n",
    "            target=tar,\n",
    "            document_name=output_path,\n",
    "            name=\"Features\"\n",
    "        )\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # 11) Return the final feature‐DataFrame and the target‐Series\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    return df_region, tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e4b8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_and_encode_imbalance_cs(\n",
    "    imbalance_path: str,\n",
    "    region: str,\n",
    "    timestamp_col: str = \"timestamp\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the imbalance_cs_train Parquet, filter to the rows where `zone == region`,\n",
    "    and then create eight one-hot (dummy) columns indicating which other zones share\n",
    "    the same `wide_area_category` block code at each timestamp.\n",
    "\n",
    "    Args:\n",
    "        imbalance_path:   Path to the Parquet file containing imbalance_cs_train data.\n",
    "                          It is assumed to have columns:\n",
    "                            - timestamp_col  (DatetimeIndex)\n",
    "                            - \"zone\"         (string: one of the nine region names)\n",
    "                            - \"wide_area_category\" (int: block code)\n",
    "                            - …any number of other features…\n",
    "        region:           The name of the zone you want to keep (e.g. \"tokyo\", \"kansai\", etc.)\n",
    "        timestamp_col:    The name of the timestamp column in the file. After loading,\n",
    "                          this column will be converted to a DateTimeIndex. Default \"timestamp\".\n",
    "                          If you actually have separate \"date\" + \"period\" columns, see note below.\n",
    "\n",
    "    Returns:\n",
    "        df_region:  A DataFrame indexed by timestamp (tz-aware if the file was),\n",
    "                    containing:\n",
    "                      • all original columns from the imbalance file for rows where zone=region\n",
    "                        (EXCEPT \"zone\" and \"wide_area_category\", which we drop once we extract them),\n",
    "                      • plus eight new columns of the form \"is_same_block_<zone_name>\" (int),\n",
    "                        giving 1 if that other zone shares the same wide_area_category code at that time,\n",
    "                        else 0.\n",
    "\n",
    "        Example columns:\n",
    "            [ ... other tokyo features ..., \n",
    "              is_same_block_hokkaido,\n",
    "              is_same_block_tohoku,\n",
    "              is_same_block_chubu,\n",
    "              is_same_block_hokuriku,\n",
    "              is_same_block_kansai,\n",
    "              is_same_block_chugoku,\n",
    "              is_same_block_shikoku,\n",
    "              is_same_block_kyushu,\n",
    "              is_same_block_okinawa\n",
    "            ]\n",
    "    \"\"\"\n",
    "    # ----------------------------------------\n",
    "    # 1) Read the Parquet\n",
    "    # ----------------------------------------\n",
    "    df = pd.read_parquet(imbalance_path)\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # 2) Parse/normalize the timestamp index\n",
    "    #    (If your file truly has a single datetime column:)\n",
    "    # ----------------------------------------\n",
    "    if timestamp_col not in df.columns:\n",
    "        # If instead your file has 'date' + 'period' (30-minute slot):\n",
    "        # uncomment + adjust the following as needed:\n",
    "        #\n",
    "        # df[\"datetime_jst\"] = (\n",
    "        #     pd.to_datetime(df[\"date\"].astype(str))\n",
    "        #     + pd.to_timedelta((df[\"period\"] - 1) * 30, unit=\"m\")\n",
    "        # )\n",
    "        # df[\"datetime_jst\"] = df[\"datetime_jst\"].dt.tz_localize(\"Asia/Tokyo\")\n",
    "        # df = df.set_index(\"datetime_jst\")\n",
    "        # \n",
    "        # In that case, just reassign timestamp_col = \"datetime_jst\":\n",
    "        # timestamp_col = \"datetime_jst\"\n",
    "        #\n",
    "        # For now, I’ll raise an error so you can correct to your actual schema:\n",
    "        raise KeyError(\n",
    "            f\"Column '{timestamp_col}' not found in {imbalance_path}. \"\n",
    "            f\"Either rename your datetime column to '{timestamp_col}', or supply \"\n",
    "            f\"‘date’ + ‘period’ parsing logic above.\"\n",
    "        )\n",
    "    else:\n",
    "        # If tz information is missing, you may need to localize → Asia/Tokyo.\n",
    "        df[timestamp_col] = pd.to_datetime(df[timestamp_col])\n",
    "        if df[timestamp_col].dt.tz is None:\n",
    "            df[timestamp_col] = df[timestamp_col].dt.tz_localize(\"Asia/Tokyo\")\n",
    "        else:\n",
    "            df[timestamp_col] = df[timestamp_col].dt.tz_convert(\"Asia/Tokyo\")\n",
    "\n",
    "        df = df.set_index(timestamp_col)\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # 3) Pivot out the “wide_area_category” codes by zone\n",
    "    #    so we can quickly see “at time t, zone X had code Y.”\n",
    "    # ----------------------------------------\n",
    "    # We only need “zone” and “wide_area_category” for this step.\n",
    "    # If there are multiple rows for (timestamp, zone), you might want to\n",
    "    # take the latest or drop duplicates first. Here, I’ll assume it’s unique.\n",
    "    pivot_block = df[[\"zone\", \"wide_area_category\"]].copy()\n",
    "    # Make sure “zone” is a column, not the index:\n",
    "    pivot_block = pivot_block.reset_index()  \n",
    "\n",
    "    # Create a DataFrame whose index is timestamp, columns are the 9 zone names,\n",
    "    # and values are the wide_area_category for that zone at that timestamp:\n",
    "    block_df = pivot_block.pivot(\n",
    "        index=timestamp_col,\n",
    "        columns=\"zone\",\n",
    "        values=\"wide_area_category\"\n",
    "    )\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # 4) Filter to just the “region” rows\n",
    "    # ----------------------------------------\n",
    "    # This gives us one row per timestamp for our region. If the original file\n",
    "    # had multiple (timestamp, region) rows, you could .drop_duplicates(...) first.\n",
    "    df_region = df[df[\"zone\"] == region].copy()\n",
    "\n",
    "    # If region never appears, we must error:\n",
    "    if df_region.empty:\n",
    "        raise KeyError(f\"No rows found where zone == '{region}' in {imbalance_path}\")\n",
    "\n",
    "    # We’ll want to drop “zone” and “wide_area_category” from df_region once we extract them.\n",
    "    # First, record the region’s block code (so we can compare to others):\n",
    "    df_region[\"region_block_code\"] = df_region[\"wide_area_category\"]\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # 5) Build the dummy columns\n",
    "    # ----------------------------------------\n",
    "    # For each timestamp t, block_df.loc[t] is a row whose columns are the 9 zone names,\n",
    "    # and whose values are that zone’s wide_area_category code at time t.\n",
    "    #\n",
    "    # We want a boolean DataFrame: “is zone Z in the same block as our region at time t?”\n",
    "    # That is: block_df.eq(region_block_code, axis=0).\n",
    "    region_codes = df_region[\"region_block_code\"].rename(\"region_block_code\")\n",
    "\n",
    "    # Align the index of block_df with the index of df_region (some timestamps might not match exactly)\n",
    "    # We'll reindex block_df to only those timestamps where region appears.\n",
    "    block_df_at_region_times = block_df.reindex(df_region.index)\n",
    "\n",
    "    # Now compare: a True wherever block_df code == region_block_code\n",
    "    same_block_bool = block_df_at_region_times.eq(region_codes, axis=0)\n",
    "\n",
    "    # Convert True/False → 1/0\n",
    "    same_block_int = same_block_bool.astype(int)\n",
    "\n",
    "    # We do not need a dummy for the region itself (since it is obviously 1),\n",
    "    # so drop that column if you like, or keep it. I’ll drop it to get exactly 8 columns:\n",
    "    if region in same_block_int.columns:\n",
    "        same_block_int = same_block_int.drop(columns=[region])\n",
    "\n",
    "    # Rename the columns to “is_same_block_<zone>”\n",
    "    same_block_int.columns = [f\"is_same_block_{z}\" for z in same_block_int.columns]\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # 6) Merge these dummy columns back onto df_region\n",
    "    # ----------------------------------------\n",
    "    df_region = pd.concat([df_region, same_block_int], axis=1)\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # 7) Drop the helper columns “zone” + “wide_area_category” + “region_block_code”\n",
    "    #    (unless you want to keep them for reference)\n",
    "    # ----------------------------------------\n",
    "    df_region = df_region.drop(\n",
    "        columns=[\"zone\", \"wide_area_category\", \"region_block_code\"],\n",
    "        errors=\"ignore\"\n",
    "    )\n",
    "\n",
    "    # Now df_region has:\n",
    "    #   • its original features (all columns except we dropped zone/wide_area_category),\n",
    "    #   • plus exactly eight new columns “is_same_block_<other_zone>”.\n",
    "\n",
    "    return df_region"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
