{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d16f471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def build_feature_dataset(\n",
    "    input_paths: list[str],\n",
    "    output_path: str,\n",
    "    region: str,\n",
    "    cols: list[str],\n",
    "    freq: str = \"30T\",\n",
    "    plot: bool = True\n",
    ") -> tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    Loading Features \n",
    "    Args:\n",
    "        input_paths:    List of Parquet file paths. Each file must load into a DataFrame\n",
    "                        whose columns are a MultiIndex with levels [region, variable_name].\n",
    "        output_path:    File‐path (including filename) where the feature report should be written.\n",
    "        region:         The first‐level column key (region) to subset by after concatenation.\n",
    "        cols:           A list of variable names (second‐level columns) to keep, once we subset to `region`.\n",
    "        freq:           A Pandas offset alias (e.g. \"30T\", \"15T\", \"1H\") used to resample each DataFrame.\n",
    "                        Default is \"30T\".\n",
    "        plot:           If True, calls `generate_feature_report(...)` on the final feature set.\n",
    "\n",
    "    Returns:\n",
    "        feat:   A DataFrame of shape [n_samples × n_features], containing:\n",
    "                • the time‐features (weekday, hour, month, etc.),\n",
    "                • the chosen columns in `cols`,\n",
    "                • and any newly added columns (forward‐filled) for modeling.\n",
    "        tar:    A pd.Series named \"Imbalance_Minus_Spot\", aligned with `feat.index`, \n",
    "                containing the (imbalance_price − spot_price) at each timestamp.\n",
    "    \"\"\"\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # Helpers for timezone‐normalization + resampling\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    def _load_and_resample_one(path: str, freq_rule: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Loads one Parquet file into a DataFrame with a DateTimeIndex, normalizes\n",
    "        its index to Asia/Tokyo, and resamples to `freq_rule` using .mean().\n",
    "        \"\"\"\n",
    "        df = pd.read_parquet(path)\n",
    "\n",
    "        # Ensure index is datetime:\n",
    "        df = df.copy()\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "\n",
    "        # If tz‐naive → assume it's already JST, so localize to Asia/Tokyo.\n",
    "        # If tz‐aware (e.g. UTC or anything), convert to Asia/Tokyo.\n",
    "        if df.index.tz is None:\n",
    "            df.index = df.index.tz_localize(\"Asia/Tokyo\")\n",
    "        else:\n",
    "            df.index = df.index.tz_convert(\"Asia/Tokyo\")\n",
    "\n",
    "        # Resample to the requested frequency, taking the mean of each\n",
    "        # (e.g. if `freq_rule=\"30T\"`, each 30‐minute block is averaged).\n",
    "        df_resampled = df.resample(freq_rule).mean()\n",
    "        return df_resampled\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # 1) Load + resample each input DataFrame; collect start/end times\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    loaded_dfs = []\n",
    "    start_times = []\n",
    "    end_times = []\n",
    "\n",
    "    for path in input_paths:\n",
    "        df_resampled = _load_and_resample_one(path, freq)\n",
    "        loaded_dfs.append(df_resampled)\n",
    "\n",
    "        # Record the new index range\n",
    "        start_times.append(df_resampled.index.min())\n",
    "        end_times.append(df_resampled.index.max())\n",
    "\n",
    "    if not loaded_dfs:\n",
    "        raise ValueError(\"`input_paths` must contain at least one parquet file.\")\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # 2) Find the common date‐range: [latest_start, earliest_end]\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    latest_start = max(start_times)\n",
    "    earliest_end = min(end_times)\n",
    "\n",
    "    if latest_start >= earliest_end:\n",
    "        raise ValueError(\n",
    "            f\"No overlapping time‐range found among the loaded files. \"\n",
    "            f\"latest_start={latest_start}, earliest_end={earliest_end}\"\n",
    "        )\n",
    "\n",
    "    # 3) Truncate each DataFrame to [latest_start : earliest_end]\n",
    "    aligned_dfs = [\n",
    "        df.loc[latest_start : earliest_end] for df in loaded_dfs\n",
    "    ]\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # 4) Concatenate side‐by‐side (axis=1)\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # Since each df had columns = MultiIndex [region, variable], \n",
    "    # the concatenation keeps the same MultiIndex column structure.\n",
    "    concatenated = pd.concat(aligned_dfs, axis=1)\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # 5) Subset by region (first level) and then by `cols` (second level)\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # This picks out one “slice” of the MultiIndex at level=0 == region.\n",
    "    try:\n",
    "        df_region = concatenated[region]\n",
    "    except KeyError:\n",
    "        raise KeyError(f\"Region '{region}' not found in the concatenated columns.\")\n",
    "\n",
    "    # Now df_region’s columns are the second level only. We keep exactly `cols`.\n",
    "    missing = [c for c in cols if c not in df_region.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"The following requested columns are not present for region {region}: {missing}\")\n",
    "\n",
    "    df_region = df_region[cols]\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # 6) Find the first+last index where BOTH spot & imbalance are non‐NaN\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    imb_col = \"pri_imb_down_%_kwh_jst_min30_a\"\n",
    "    spot_col = \"pri_spot_jepx_%_kwh_jst_min30_a\"\n",
    "\n",
    "    # Ensure those two are in `cols` (or else we can’t form the target)\n",
    "    if imb_col not in df_region.columns or spot_col not in df_region.columns:\n",
    "        raise KeyError(\n",
    "            f\"Cannot find both target columns ('{imb_col}' and '{spot_col}') in df_region. \"\n",
    "            f\"Got columns={list(df_region.columns)}\"\n",
    "        )\n",
    "\n",
    "    # Build a mask where both are non‐NaN:\n",
    "    both_valid = (\n",
    "        df_region[imb_col].notna() &\n",
    "        df_region[spot_col].notna()\n",
    "    )\n",
    "    # If there is no timestamp where both are valid, it's an error:\n",
    "    if not both_valid.any():\n",
    "        raise ValueError(\n",
    "            f\"No timestamp exists where both '{imb_col}' and '{spot_col}' are non‐NaN.\"\n",
    "        )\n",
    "\n",
    "    valid_times = df_region.index[both_valid]\n",
    "    crop_start = valid_times.min()\n",
    "    crop_end = valid_times.max()\n",
    "\n",
    "    # Crop the DataFrame so that the first row has both non‐NaN, and the last row has both non‐NaN\n",
    "    df_region = df_region.loc[crop_start : crop_end]\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # 7) Forward‐fill any remaining NaNs (limit=1)\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    df_region = df_region.ffill(limit=1)\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # 8) Construct time‐features\n",
    "    #    (Assumes you already have a function `construct_time_features(df)` defined elsewhere.)\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    construct_time_features(df_region)\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # 9) Create the target series: \"Imbalance_Minus_Spot\"\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    tar = df_region[imb_col] - df_region[spot_col]\n",
    "    tar.name = \"Imbalance_Minus_Spot\"\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # 10) Optionally generate a feature report\n",
    "    #    (Assumes you already have `generate_feature_report(...)` imported.)\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    if plot:\n",
    "        # name=\"Features\" is arbitrary; you can change if you like\n",
    "        generate_feature_report(\n",
    "            features=df_region,\n",
    "            target=tar,\n",
    "            document_name=output_path,\n",
    "            name=\"Features\"\n",
    "        )\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # 11) Return the final feature‐DataFrame and the target‐Series\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    return df_region, tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c19d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_and_encode_imbalance_cs(\n",
    "    imbalance_path: str,\n",
    "    region: str,\n",
    "    timestamp_col: str = \"timestamp\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the imbalance_cs_train Parquet, filter to the rows where `zone == region`,\n",
    "    and then create eight one-hot (dummy) columns indicating which other zones share\n",
    "    the same `wide_area_category` block code at each timestamp.\n",
    "\n",
    "    Args:\n",
    "        imbalance_path:   Path to the Parquet file containing imbalance_cs_train data.\n",
    "                          It is assumed to have columns:\n",
    "                            - timestamp_col  (DatetimeIndex)\n",
    "                            - \"zone\"         (string: one of the nine region names)\n",
    "                            - \"wide_area_category\" (int: block code)\n",
    "                            - …any number of other features…\n",
    "        region:           The name of the zone you want to keep (e.g. \"tokyo\", \"kansai\", etc.)\n",
    "        timestamp_col:    The name of the timestamp column in the file. After loading,\n",
    "                          this column will be converted to a DateTimeIndex. Default \"timestamp\".\n",
    "                          If you actually have separate \"date\" + \"period\" columns, see note below.\n",
    "\n",
    "    Returns:\n",
    "        df_region:  A DataFrame indexed by timestamp (tz-aware if the file was),\n",
    "                    containing:\n",
    "                      • all original columns from the imbalance file for rows where zone=region\n",
    "                        (EXCEPT \"zone\" and \"wide_area_category\", which we drop once we extract them),\n",
    "                      • plus eight new columns of the form \"is_same_block_<zone_name>\" (int),\n",
    "                        giving 1 if that other zone shares the same wide_area_category code at that time,\n",
    "                        else 0.\n",
    "\n",
    "        Example columns:\n",
    "            [ ... other tokyo features ..., \n",
    "              is_same_block_hokkaido,\n",
    "              is_same_block_tohoku,\n",
    "              is_same_block_chubu,\n",
    "              is_same_block_hokuriku,\n",
    "              is_same_block_kansai,\n",
    "              is_same_block_chugoku,\n",
    "              is_same_block_shikoku,\n",
    "              is_same_block_kyushu,\n",
    "              is_same_block_okinawa\n",
    "            ]\n",
    "    \"\"\"\n",
    "    # ----------------------------------------\n",
    "    # 1) Read the Parquet\n",
    "    # ----------------------------------------\n",
    "    df = pd.read_parquet(imbalance_path)\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # 2) Parse/normalize the timestamp index\n",
    "    #    (If your file truly has a single datetime column:)\n",
    "    # ----------------------------------------\n",
    "    if timestamp_col not in df.columns:\n",
    "        # If instead your file has 'date' + 'period' (30-minute slot):\n",
    "        # uncomment + adjust the following as needed:\n",
    "        #\n",
    "        # df[\"datetime_jst\"] = (\n",
    "        #     pd.to_datetime(df[\"date\"].astype(str))\n",
    "        #     + pd.to_timedelta((df[\"period\"] - 1) * 30, unit=\"m\")\n",
    "        # )\n",
    "        # df[\"datetime_jst\"] = df[\"datetime_jst\"].dt.tz_localize(\"Asia/Tokyo\")\n",
    "        # df = df.set_index(\"datetime_jst\")\n",
    "        # \n",
    "        # In that case, just reassign timestamp_col = \"datetime_jst\":\n",
    "        # timestamp_col = \"datetime_jst\"\n",
    "        #\n",
    "        # For now, I’ll raise an error so you can correct to your actual schema:\n",
    "        raise KeyError(\n",
    "            f\"Column '{timestamp_col}' not found in {imbalance_path}. \"\n",
    "            f\"Either rename your datetime column to '{timestamp_col}', or supply \"\n",
    "            f\"‘date’ + ‘period’ parsing logic above.\"\n",
    "        )\n",
    "    else:\n",
    "        # If tz information is missing, you may need to localize → Asia/Tokyo.\n",
    "        df[timestamp_col] = pd.to_datetime(df[timestamp_col])\n",
    "        if df[timestamp_col].dt.tz is None:\n",
    "            df[timestamp_col] = df[timestamp_col].dt.tz_localize(\"Asia/Tokyo\")\n",
    "        else:\n",
    "            df[timestamp_col] = df[timestamp_col].dt.tz_convert(\"Asia/Tokyo\")\n",
    "\n",
    "        df = df.set_index(timestamp_col)\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # 3) Pivot out the “wide_area_category” codes by zone\n",
    "    #    so we can quickly see “at time t, zone X had code Y.”\n",
    "    # ----------------------------------------\n",
    "    # We only need “zone” and “wide_area_category” for this step.\n",
    "    # If there are multiple rows for (timestamp, zone), you might want to\n",
    "    # take the latest or drop duplicates first. Here, I’ll assume it’s unique.\n",
    "    pivot_block = df[[\"zone\", \"wide_area_category\"]].copy()\n",
    "    # Make sure “zone” is a column, not the index:\n",
    "    pivot_block = pivot_block.reset_index()  \n",
    "\n",
    "    # Create a DataFrame whose index is timestamp, columns are the 9 zone names,\n",
    "    # and values are the wide_area_category for that zone at that timestamp:\n",
    "    block_df = pivot_block.pivot(\n",
    "        index=timestamp_col,\n",
    "        columns=\"zone\",\n",
    "        values=\"wide_area_category\"\n",
    "    )\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # 4) Filter to just the “region” rows\n",
    "    # ----------------------------------------\n",
    "    # This gives us one row per timestamp for our region. If the original file\n",
    "    # had multiple (timestamp, region) rows, you could .drop_duplicates(...) first.\n",
    "    df_region = df[df[\"zone\"] == region].copy()\n",
    "\n",
    "    # If region never appears, we must error:\n",
    "    if df_region.empty:\n",
    "        raise KeyError(f\"No rows found where zone == '{region}' in {imbalance_path}\")\n",
    "\n",
    "    # We’ll want to drop “zone” and “wide_area_category” from df_region once we extract them.\n",
    "    # First, record the region’s block code (so we can compare to others):\n",
    "    df_region[\"region_block_code\"] = df_region[\"wide_area_category\"]\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # 5) Build the dummy columns\n",
    "    # ----------------------------------------\n",
    "    # For each timestamp t, block_df.loc[t] is a row whose columns are the 9 zone names,\n",
    "    # and whose values are that zone’s wide_area_category code at time t.\n",
    "    #\n",
    "    # We want a boolean DataFrame: “is zone Z in the same block as our region at time t?”\n",
    "    # That is: block_df.eq(region_block_code, axis=0).\n",
    "    region_codes = df_region[\"region_block_code\"].rename(\"region_block_code\")\n",
    "\n",
    "    # Align the index of block_df with the index of df_region (some timestamps might not match exactly)\n",
    "    # We'll reindex block_df to only those timestamps where region appears.\n",
    "    block_df_at_region_times = block_df.reindex(df_region.index)\n",
    "\n",
    "    # Now compare: a True wherever block_df code == region_block_code\n",
    "    same_block_bool = block_df_at_region_times.eq(region_codes, axis=0)\n",
    "\n",
    "    # Convert True/False → 1/0\n",
    "    same_block_int = same_block_bool.astype(int)\n",
    "\n",
    "    # We do not need a dummy for the region itself (since it is obviously 1),\n",
    "    # so drop that column if you like, or keep it. I’ll drop it to get exactly 8 columns:\n",
    "    if region in same_block_int.columns:\n",
    "        same_block_int = same_block_int.drop(columns=[region])\n",
    "\n",
    "    # Rename the columns to “is_same_block_<zone>”\n",
    "    same_block_int.columns = [f\"is_same_block_{z}\" for z in same_block_int.columns]\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # 6) Merge these dummy columns back onto df_region\n",
    "    # ----------------------------------------\n",
    "    df_region = pd.concat([df_region, same_block_int], axis=1)\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # 7) Drop the helper columns “zone” + “wide_area_category” + “region_block_code”\n",
    "    #    (unless you want to keep them for reference)\n",
    "    # ----------------------------------------\n",
    "    df_region = df_region.drop(\n",
    "        columns=[\"zone\", \"wide_area_category\", \"region_block_code\"],\n",
    "        errors=\"ignore\"\n",
    "    )\n",
    "\n",
    "    # Now df_region has:\n",
    "    #   • its original features (all columns except we dropped zone/wide_area_category),\n",
    "    #   • plus exactly eight new columns “is_same_block_<other_zone>”.\n",
    "\n",
    "    return df_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e14a5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_and_encode_zone_data(path: str, region: str, timestamp_col: str = \"timestamp\") -> pd.DataFrame:\n",
    "    df = pd.read_parquet(path)\n",
    "\n",
    "    # Parse timestamp\n",
    "    if timestamp_col not in df.columns:\n",
    "        raise KeyError(f\"Column '{timestamp_col}' not found in {path}\")\n",
    "    df[timestamp_col] = pd.to_datetime(df[timestamp_col])\n",
    "    if df[timestamp_col].dt.tz is None:\n",
    "        df[timestamp_col] = df[timestamp_col].dt.tz_localize(\"Asia/Tokyo\")\n",
    "    else:\n",
    "        df[timestamp_col] = df[timestamp_col].dt.tz_convert(\"Asia/Tokyo\")\n",
    "    df = df.set_index(timestamp_col)\n",
    "\n",
    "    # Pivot block codes by zone\n",
    "    pivot = df[[\"zone\", \"wide_area_category\"]].reset_index()\n",
    "    block_df = pivot.pivot(index=timestamp_col, columns=\"zone\", values=\"wide_area_category\")\n",
    "\n",
    "    # Filter to region\n",
    "    df_region = df[df[\"zone\"] == region].copy()\n",
    "    if df_region.empty:\n",
    "        raise KeyError(f\"No rows found with zone == '{region}' in {path}\")\n",
    "    df_region[\"region_block_code\"] = df_region[\"wide_area_category\"]\n",
    "\n",
    "    # Create dummy columns\n",
    "    block_df_region = block_df.reindex(df_region.index)\n",
    "    same_block = block_df_region.eq(df_region[\"region_block_code\"], axis=0).astype(int)\n",
    "    if region in same_block.columns:\n",
    "        same_block = same_block.drop(columns=[region])\n",
    "    same_block.columns = [f\"is_same_block_{z}\" for z in same_block.columns]\n",
    "\n",
    "    # Merge and drop helpers\n",
    "    df_region = pd.concat([df_region, same_block], axis=1)\n",
    "    df_region = df_region.drop(columns=[\"zone\", \"wide_area_category\", \"region_block_code\"], errors=\"ignore\")\n",
    "\n",
    "    return df_region\n",
    "\n",
    "\n",
    "def build_feature_dataset(\n",
    "    input_paths: list[str],\n",
    "    imbalance_path: str,\n",
    "    daily_occto_path: str,\n",
    "    output_path: str,\n",
    "    region: str,\n",
    "    cols: list[str],\n",
    "    freq: str = \"30T\",\n",
    "    plot: bool = True\n",
    ") -> tuple[pd.DataFrame, pd.Series]:\n",
    "    # Load and process imbalance and daily_occto\n",
    "    df_imb = load_and_encode_zone_data(imbalance_path, region, timestamp_col=\"timestamp\")\n",
    "    df_imb = df_imb.resample(freq).mean()\n",
    "\n",
    "    df_daily = load_and_encode_zone_data(daily_occto_path, region, timestamp_col=\"timestamp\")\n",
    "    df_daily = df_daily.resample(freq).mean()\n",
    "\n",
    "    # Helper for other files\n",
    "    def _load_resample(path: str) -> pd.DataFrame:\n",
    "        df = pd.read_parquet(path).copy()\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        if df.index.tz is None:\n",
    "            df.index = df.index.tz_localize(\"Asia/Tokyo\")\n",
    "        else:\n",
    "            df.index = df.index.tz_convert(\"Asia/Tokyo\")\n",
    "        return df.resample(freq).mean()\n",
    "\n",
    "    loaded_dfs = [df_imb, df_daily]\n",
    "    start_times = [df_imb.index.min(), df_daily.index.min()]\n",
    "    end_times = [df_imb.index.max(), df_daily.index.max()]\n",
    "\n",
    "    for path in input_paths:\n",
    "        df_r = _load_resample(path)\n",
    "        loaded_dfs.append(df_r)\n",
    "        start_times.append(df_r.index.min())\n",
    "        end_times.append(df_r.index.max())\n",
    "\n",
    "    if not loaded_dfs:\n",
    "        raise ValueError(\"No data loaded.\")\n",
    "\n",
    "    # Find common range\n",
    "    latest_start = max(start_times)\n",
    "    earliest_end = min(end_times)\n",
    "    if latest_start >= earliest_end:\n",
    "        raise ValueError(f\"No overlapping range: {latest_start} >= {earliest_end}\")\n",
    "\n",
    "    aligned = [df.loc[latest_start:earliest_end] for df in loaded_dfs]\n",
    "\n",
    "    # Concatenate\n",
    "    concatenated = pd.concat(aligned, axis=1)\n",
    "\n",
    "    # Subset to region and cols\n",
    "    try:\n",
    "        df_region_multi = concatenated[region]\n",
    "    except KeyError:\n",
    "        raise KeyError(f\"Region '{region}' not found.\")\n",
    "\n",
    "    missing = [c for c in cols if c not in df_region_multi.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing columns for region {region}: {missing}\")\n",
    "    df_region_multi = df_region_multi[cols]\n",
    "\n",
    "    # Crop to valid target rows\n",
    "    imb_col = \"pri_imb_down_%_kwh_jst_min30_a\"\n",
    "    spot_col = \"pri_spot_jepx_%_kwh_jst_min30_a\"\n",
    "    if imb_col not in df_region_multi.columns or spot_col not in df_region_multi.columns:\n",
    "        raise KeyError(f\"Target columns missing: {imb_col}, {spot_col}\")\n",
    "    mask = df_region_multi[imb_col].notna() & df_region_multi[spot_col].notna()\n",
    "    if not mask.any():\n",
    "        raise ValueError(\"No valid target rows.\")\n",
    "    valid_idx = df_region_multi.index[mask]\n",
    "    df_region_multi = df_region_multi.loc[valid_idx.min():valid_idx.max()]\n",
    "\n",
    "    # Forward-fill\n",
    "    df_region_multi = df_region_multi.ffill(limit=1)\n",
    "\n",
    "    # Time features\n",
    "    construct_time_features(df_region_multi)\n",
    "\n",
    "    # Target series\n",
    "    tar = df_region_multi[imb_col] - df_region_multi[spot_col]\n",
    "    tar.name = \"Imbalance_Minus_Spot\"\n",
    "\n",
    "    # Feature report\n",
    "    if plot:\n",
    "        generate_feature_report(\n",
    "            features=df_region_multi,\n",
    "            target=tar,\n",
    "            document_name=output_path,\n",
    "            name=\"Features\"\n",
    "        )\n",
    "\n",
    "    return df_region_multi, tar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961b2221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1) Standardization / Un‐standardization Helpers\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def identity_standardize(df: pd.DataFrame, features: list, target: str):\n",
    "    \"\"\"\n",
    "    Shapes data and adds intercept without scaling.\n",
    "\n",
    "    Args:\n",
    "        df: pandas DataFrame (length T)\n",
    "        features: list of feature column names (to include in X)\n",
    "        target: name of target column\n",
    "\n",
    "    Returns:\n",
    "        x: np.ndarray of shape (K, T, 1),\n",
    "           where K = 1 + len(features);  first row is intercept,\n",
    "           remaining rows correspond to features in the given order.\n",
    "        y: np.ndarray of shape (T, 1)\n",
    "        means: np.ndarray of shape (q,) (all zeros if no numeric features)\n",
    "        stds: np.ndarray of shape (q,) (all ones if no numeric features)\n",
    "    \"\"\"\n",
    "    # 0) Check all requested features exist\n",
    "    missing = [c for c in features if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Requested features not found in DataFrame: {missing}\")\n",
    "\n",
    "    T = df.shape[0]\n",
    "    N = 1\n",
    "\n",
    "    # 1) Build feature array in THE ORDER specified by `features`\n",
    "    feat_array = df[features].to_numpy()        # shape = (T, K-1)\n",
    "    feat_TN    = feat_array.T[:, :, np.newaxis] # shape = (K-1, T, 1)\n",
    "\n",
    "    # 2) Add intercept\n",
    "    inter = np.ones((1, T, N))\n",
    "    x     = np.concatenate((inter, feat_TN), axis=0)  # shape = (K, T, 1)\n",
    "\n",
    "    # 3) Build target vector (unchanged)\n",
    "    y = df[target].to_numpy().reshape(T, N)  # shape = (T, 1)\n",
    "\n",
    "    # 4) Identify numeric features so we can build placeholder means/stds\n",
    "    numeric_cols = [c for c in features if pd.api.types.is_numeric_dtype(df[c])]\n",
    "    q = len(numeric_cols)\n",
    "\n",
    "    # Return dummy means/stds so that downstream code can always unpack 4 items\n",
    "    means = np.zeros(q)\n",
    "    stds  = np.ones(q)\n",
    "\n",
    "    return x, y, means, stds\n",
    "\n",
    "\n",
    "def z_standardize(df: pd.DataFrame, features: list, target: str):\n",
    "    \"\"\"\n",
    "    Takes in a DataFrame with specified features, adds an intercept, and returns:\n",
    "      - x: (K, T, 1) with intercept + standardized numeric + raw categorical\n",
    "      - y: (T, 1) the target vector (unchanged)\n",
    "      - means: (q,)  means of numeric columns\n",
    "      - stds:  (q,)  stds of numeric columns\n",
    "\n",
    "    Args:\n",
    "        df: pandas DataFrame (length T)\n",
    "        features: list of feature column names (some numeric, some categorical)\n",
    "        target: name of target column\n",
    "\n",
    "    Returns:\n",
    "        x: np.ndarray of shape (K, T, 1), where\n",
    "           K = 1 + len(features),\n",
    "           row 0 = intercept (all ones),\n",
    "           rows 1..q = (X_numeric - mean)/std,\n",
    "           rows q+1.. = categorical columns (unchanged).\n",
    "        y: np.ndarray of shape (T, 1)\n",
    "        means: np.ndarray of shape (q,), where q = number of numeric features\n",
    "        stds:  np.ndarray of shape (q,)\n",
    "    \"\"\"\n",
    "    # 0) Check all features exist\n",
    "    missing = [c for c in features if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Requested features not found in DataFrame: {missing}\")\n",
    "\n",
    "    T = df.shape[0]\n",
    "    N = 1\n",
    "\n",
    "    # 1) Partition features into numeric vs. categorical (preserving order)\n",
    "    numeric_cols = [c for c in features if pd.api.types.is_numeric_dtype(df[c])]\n",
    "    cat_cols     = [c for c in features if not pd.api.types.is_numeric_dtype(df[c])]\n",
    "    cols_order   = numeric_cols + cat_cols  # final order for columns\n",
    "\n",
    "    # 2) Compute means/stds on numeric columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        means = df[numeric_cols].mean().to_numpy()       # shape = (q,)\n",
    "        stds  = df[numeric_cols].std().to_numpy() + 1e-8 # shape = (q,)\n",
    "    else:\n",
    "        means = np.zeros(0)\n",
    "        stds  = np.ones(0)\n",
    "\n",
    "    # 3) Standardize numeric columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        df_scaled_numeric = (df[numeric_cols] - means) / stds  # DataFrame shape = (T, q)\n",
    "    else:\n",
    "        # Create an empty DataFrame with T rows and 0 columns\n",
    "        df_scaled_numeric = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # 4) Combine scaled numerics with categorical, in the order specified\n",
    "    df_combined = pd.concat([df_scaled_numeric, df[cat_cols]], axis=1)\n",
    "    df_new      = df_combined[cols_order]  # re‐order (though concat already did)\n",
    "\n",
    "    # 5) Build feature‐tensor x: shape = (K, T, 1)\n",
    "    feat_array = df_new.to_numpy()               # shape = (T, K-1)\n",
    "    feat_TN    = feat_array.T[:, :, np.newaxis]  # shape = (K-1, T, 1)\n",
    "    inter      = np.ones((1, T, N))              # shape = (1, T, 1)\n",
    "    x          = np.concatenate((inter, feat_TN), axis=0)  # shape = (K, T, 1)\n",
    "\n",
    "    # 6) Build target vector (unchanged)\n",
    "    y = df[target].to_numpy().reshape(T, N)  # shape = (T, 1)\n",
    "\n",
    "    return x, y, means, stds\n",
    "\n",
    "\n",
    "def z_unstandardize(betas: np.ndarray, means: np.ndarray, stds: np.ndarray):\n",
    "    \"\"\"\n",
    "    Un‐scale time‐varying betas from Z‐score standardization.\n",
    "\n",
    "    Args:\n",
    "        betas:   np.ndarray of shape (K, T, 1),\n",
    "                 where K = 1 + q + (#categorical features),\n",
    "                 row 0 = scaled intercept,\n",
    "                 rows 1..q = scaled numeric slopes,\n",
    "                 rows q+1.. = categorical slopes (unchanged),\n",
    "                 T = number of timepoints.\n",
    "        means:   np.ndarray of shape (q,), mean for each numeric predictor\n",
    "        stds:    np.ndarray of shape (q,), std for each numeric predictor\n",
    "\n",
    "    Returns:\n",
    "        betas_unscaled: np.ndarray of shape (K, T, 1),\n",
    "                        with numeric slopes and intercept un‐scaled back to original units.\n",
    "    \"\"\"\n",
    "    q = means.shape[0]  # number of numeric features\n",
    "    T = betas.shape[1]\n",
    "    N = 1\n",
    "\n",
    "    # 1) Un‐scale numeric slopes\n",
    "    #    betas[1:q+1, t, 0] is the scaled slope; divide by stds[i]\n",
    "    if q > 0:\n",
    "        slopes_scaled = betas[1 : q+1, :, :]                     # shape = (q, T, 1)\n",
    "        slopes_unscaled = slopes_scaled / stds.reshape(q, 1, 1)  # shape = (q, T, 1)\n",
    "    else:\n",
    "        slopes_unscaled = np.zeros((0, T, N))\n",
    "\n",
    "    # 2) Adjust intercept\n",
    "    #    intercept_unscaled[t] = scaled_intercept[t] - sum_i (scaled_slope_i[t] * (means[i]/stds[i]))\n",
    "    if q > 0:\n",
    "        temp = (means / stds).reshape(q, 1, 1)  # shape = (q, 1, 1)\n",
    "        adjustment = np.sum(betas[1 : q+1, :, :] * temp, axis=0)  # shape = (T, 1)\n",
    "    else:\n",
    "        adjustment = np.zeros((T, N))\n",
    "\n",
    "    intercept_unscaled = betas[0, :, :] - adjustment  # shape = (T, 1)\n",
    "\n",
    "    # 3) Keep categorical slopes unchanged\n",
    "    cat_betas = betas[q+1 :, :, :]  # shape = ((K-1-q), T, 1)\n",
    "    if cat_betas.ndim == 2:\n",
    "        cat_betas = cat_betas.reshape(1, T, N)\n",
    "\n",
    "    # 4) Re‐assemble\n",
    "    intercept_unscaled = intercept_unscaled.reshape(1, T, N)\n",
    "    betas_unscaled = np.concatenate(\n",
    "        [intercept_unscaled, slopes_unscaled, cat_betas], axis=0\n",
    "    )  # shape = (K, T, 1)\n",
    "\n",
    "    return betas_unscaled\n",
    "\n",
    "\n",
    "def ewm_standardize(df: pd.DataFrame, features: list, target: str,\n",
    "                    halflife: float, burnin_steps: int):\n",
    "    \"\"\"\n",
    "    EWMA scaling of numerical features, add intercepts, and reshape:\n",
    "      - Mask out the first `burnin_steps` of the EWM so they don’t bias.\n",
    "      - Backfill the masked rows from the first valid EWM.\n",
    "\n",
    "    Args:\n",
    "        df: pandas DataFrame (length T)\n",
    "        features: list of feature column names (some numeric, some categorical)\n",
    "        target: name of target column\n",
    "        halflife: float, half‐life parameter for pandas’ ewm\n",
    "        burnin_steps: int, number of initial rows to mask before backfill\n",
    "\n",
    "    Returns:\n",
    "        x: np.ndarray of shape (K, T, 1),\n",
    "           row 0 = intercept,\n",
    "           rows 1..q = (X_numeric - ewm_mean)/ewm_std (after burn‐in/backfill),\n",
    "           rows q+1.. = categorical columns (unchanged).\n",
    "        y: np.ndarray of shape (T, 1) (target, unchanged)\n",
    "        ewm_means: np.ndarray of shape (q, T)\n",
    "        ewm_stds:  np.ndarray of shape (q, T)\n",
    "    \"\"\"\n",
    "    # 0) Check all features exist\n",
    "    missing = [c for c in features if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Requested features not found in DataFrame: {missing}\")\n",
    "\n",
    "    T = df.shape[0]\n",
    "    N = 1\n",
    "\n",
    "    # 1) Partition features into numeric vs. categorical (preserve order)\n",
    "    numeric_cols = [c for c in features if pd.api.types.is_numeric_dtype(df[c])]\n",
    "    cat_cols     = [c for c in features if not pd.api.types.is_numeric_dtype(df[c])]\n",
    "    cols_order   = numeric_cols + cat_cols  # for final concatenation\n",
    "\n",
    "    q = len(numeric_cols)\n",
    "\n",
    "    # 2) Compute EWMA means/stds as DataFrames (shape: (T, q))\n",
    "    if q > 0:\n",
    "        ewm_means_df = df[numeric_cols].ewm(halflife=halflife, adjust=False).mean()\n",
    "        ewm_stds_df  = df[numeric_cols].ewm(halflife=halflife, adjust=False).std() + 1e-8\n",
    "    else:\n",
    "        # No numeric features: create empty DataFrames with shape (T, 0)\n",
    "        ewm_means_df = pd.DataFrame(index=df.index, columns=[])\n",
    "        ewm_stds_df  = pd.DataFrame(index=df.index, columns=[])\n",
    "\n",
    "    # 3) Mask the first `burnin_steps` rows for both means and stds\n",
    "    if q > 0:\n",
    "        ewm_means_df.iloc[:burnin_steps, :] = np.nan\n",
    "        ewm_stds_df.iloc[:burnin_steps, :]  = np.nan\n",
    "\n",
    "        # 4) Backfill masked rows\n",
    "        ewm_means_df = ewm_means_df.bfill()\n",
    "        ewm_stds_df  = ewm_stds_df.bfill()\n",
    "\n",
    "    # 5) Convert back to NumPy and transpose to (q, T)\n",
    "    if q > 0:\n",
    "        means = ewm_means_df.to_numpy().T  # shape = (q, T)\n",
    "        stds  = ewm_stds_df.to_numpy().T   # shape = (q, T)\n",
    "    else:\n",
    "        means = np.zeros((0, T))\n",
    "        stds  = np.ones((0, T))\n",
    "\n",
    "    # 6) Build scaled DataFrame for numeric columns (shape = (T, q))\n",
    "    if q > 0:\n",
    "        df_scaled_numeric = (df[numeric_cols] - ewm_means_df) / ewm_stds_df\n",
    "    else:\n",
    "        df_scaled_numeric = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # 7) Concatenate scaled numerics with categorical (unchanged)\n",
    "    df_combined = pd.concat([df_scaled_numeric, df[cat_cols]], axis=1)\n",
    "    df_new      = df_combined[cols_order]  # shape = (T, K-1)\n",
    "\n",
    "    # 8) Build feature‐tensor x: shape = (K, T, 1)\n",
    "    feat_array = df_new.to_numpy()               # shape = (T, K-1)\n",
    "    feat_TN    = feat_array.T[:, :, np.newaxis]  # shape = (K-1, T, 1)\n",
    "    inter      = np.ones((1, T, N))              # shape = (1, T, 1)\n",
    "    x          = np.concatenate((inter, feat_TN), axis=0)  # shape = (K, T, 1)\n",
    "\n",
    "    # 9) Build target vector (unchanged)\n",
    "    y = df[target].to_numpy().reshape(T, N)  # shape = (T, 1)\n",
    "\n",
    "    return x, y, means, stds\n",
    "\n",
    "\n",
    "def ewm_unstandardize(betas: np.ndarray, ewm_means: np.ndarray, ewm_stds: np.ndarray):\n",
    "    \"\"\"\n",
    "    Un‐scale time‐varying betas from EWM standardization.\n",
    "\n",
    "    Args:\n",
    "        betas:       np.ndarray of shape (K, T, 1),\n",
    "                     [0] = scaled intercept,\n",
    "                     [1:q+1] = scaled numeric slopes,\n",
    "                     [q+1:]  = categorical (unchanged).\n",
    "        ewm_means:   np.ndarray of shape (q, T)\n",
    "        ewm_stds:    np.ndarray of shape (q, T)\n",
    "\n",
    "    Returns:\n",
    "        betas_unscaled: np.ndarray of shape (K, T, 1),\n",
    "                        with numeric slopes and intercept un‐scaled to original units.\n",
    "    \"\"\"\n",
    "    q, T = ewm_means.shape\n",
    "    N = 1\n",
    "\n",
    "    # 1) Un‐scale numeric slopes\n",
    "    if q > 0:\n",
    "        slopes_scaled    = betas[1 : q+1, :, :]                 # shape = (q, T, 1)\n",
    "        slopes_unscaled  = slopes_scaled / ewm_stds.reshape(q, T, 1)  # shape = (q, T, 1)\n",
    "    else:\n",
    "        slopes_unscaled = np.zeros((0, T, N))\n",
    "\n",
    "    # 2) Compute intercept adjustment\n",
    "    if q > 0:\n",
    "        temp = (ewm_means / ewm_stds).reshape(q, T, 1)  # shape = (q, T, 1)\n",
    "        adjustment = np.sum(betas[1:q+1, :, :] * temp, axis=0)  # shape = (T, 1)\n",
    "    else:\n",
    "        adjustment = np.zeros((T, N))\n",
    "\n",
    "    intercept_unscaled = betas[0, :, :] - adjustment  # shape = (T, 1)\n",
    "\n",
    "    # 3) Keep categorical betas unchanged\n",
    "    cat_betas = betas[q+1 :, :, :]  # shape = ((K-1-q), T, 1)\n",
    "    if cat_betas.ndim == 2:\n",
    "        cat_betas = cat_betas.reshape(1, T, N)\n",
    "\n",
    "    # 4) Re‐assemble\n",
    "    intercept_unscaled = intercept_unscaled.reshape(1, T, N)\n",
    "    betas_unscaled = np.concatenate(\n",
    "        [intercept_unscaled, slopes_unscaled, cat_betas],\n",
    "        axis=0\n",
    "    )  # shape = (K, T, 1)\n",
    "\n",
    "    return betas_unscaled\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2) Grid‐Search Over Tau & Decay Scale\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def grid_search_param(x: np.ndarray, y: np.ndarray,\n",
    "                      scaling: str, means: np.ndarray, stds: np.ndarray,\n",
    "                      backshift: int):\n",
    "    \"\"\"\n",
    "    Runs a quick grid search over tau (prior covariance scale) and decay_scale,\n",
    "    using MSE on the original target to pick the best pair.  Always does predictions\n",
    "    in the “scaled‐space” so that no unit mismatch happens.\n",
    "\n",
    "    Args:\n",
    "        x:        np.ndarray, shape = (K, T, 1), scaled input features\n",
    "        y:        np.ndarray, shape = (T, 1), original target\n",
    "        scaling:  \"z\", \"ewm\", or \"identity\"\n",
    "        means:    np.ndarray (q,) if scaling==\"z\", or (q,T) if scaling==\"ewm\", else None\n",
    "        stds:     np.ndarray (q,) or (q,T) or None\n",
    "        backshift: int, number of time steps to lag coefficients before prediction\n",
    "\n",
    "    Returns:\n",
    "        best_tau:   float, best prior‐variance scale\n",
    "        best_decay: float, best decay_scale\n",
    "        mse_grid:   np.ndarray of shape (len(tau_vals), len(decay_vals)), MSE at each grid point\n",
    "    \"\"\"\n",
    "    # Number of coefficient‐rows (intercept + all features)\n",
    "    D = x.shape[0]\n",
    "\n",
    "    # Define grid\n",
    "    tau_vals   = np.logspace(-2, 2, num=5)   # e.g. [1e-2, 1e-1, 1, 10, 100]\n",
    "    decay_vals = np.logspace(0, 4, num=5)    # e.g. [1, 10, 100, 1000, 10000]\n",
    "\n",
    "    mse_grid = np.zeros((len(tau_vals), len(decay_vals)))\n",
    "    best_mse  = np.inf\n",
    "    best_tau  = None\n",
    "    best_decay= None\n",
    "\n",
    "    for i, tau in enumerate(tau_vals):\n",
    "        for j, decay in enumerate(decay_vals):\n",
    "            # 1) Fit Bayesian time‐series regression with decay\n",
    "            prior_mean  = np.zeros(D)\n",
    "            prior_covar = np.eye(D) * tau\n",
    "            betas_temp = decay_regress(\n",
    "                x=x, y=y,\n",
    "                prior_mean=prior_mean,\n",
    "                prior_covar=prior_covar,\n",
    "                decay_scale=decay\n",
    "            )  # shape = (D, T, 1)\n",
    "\n",
    "            # 2) Back‐shift coefficients (still in scaled‐space)\n",
    "            betas_lagged = np.roll(betas_temp, shift=backshift, axis=1)\n",
    "            betas_lagged[:, :backshift, :] = np.nan\n",
    "\n",
    "            # 3) Predict in scaled space:       (x is scaled, betas_lagged is scaled)\n",
    "            yhat_temp = np.nansum(x * betas_lagged, axis=0)  # shape = (T, 1)\n",
    "\n",
    "            # 4) Compute MSE vs original y\n",
    "            resid = yhat_temp.flatten() - y.flatten()\n",
    "            mse_temp = np.nanmean(resid**2)\n",
    "            mse_grid[i, j] = mse_temp\n",
    "\n",
    "            if mse_temp < best_mse:\n",
    "                best_mse   = mse_temp\n",
    "                best_tau   = tau\n",
    "                best_decay = decay\n",
    "\n",
    "    return best_tau, best_decay, mse_grid\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3) Plotting / Diagnostics\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def plot_model_diagnostics(y_flat: np.ndarray, yhat_flat: np.ndarray,\n",
    "                           resid: np.ndarray, betas_unscaled: np.ndarray,\n",
    "                           features: list, mse_grid: np.ndarray, grid_search: bool):\n",
    "    \"\"\"\n",
    "    Generates diagnostic plots:\n",
    "      1) Residuals over time\n",
    "      2) Residual histogram\n",
    "      3) Residuals vs. fitted\n",
    "      4) Q-Q plot of residuals\n",
    "      5) If grid_search=True, an MSE heatmap (tau × decay)\n",
    "      6) Time‐series + histogram for each beta (unscaled)\n",
    "\n",
    "    Args:\n",
    "        y_flat:         np.ndarray of shape (T,), actual target\n",
    "        yhat_flat:      np.ndarray of shape (T,), predicted target\n",
    "        resid:          np.ndarray of shape (T,), residuals = yhat−y\n",
    "        betas_unscaled: np.ndarray of shape (K, T, 1)\n",
    "        features:       list of length (K−1) of feature names (no intercept)\n",
    "        mse_grid:       np.ndarray of shape (len(tau_vals), len(decay_vals))\n",
    "        grid_search:    bool, whether to plot the MSE heatmap\n",
    "    \"\"\"\n",
    "    # 1) Residuals over time\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(resid, marker='o', linestyle='none')\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Residual\")\n",
    "    plt.title(\"Residuals Over Time\")\n",
    "    plt.show()\n",
    "\n",
    "    # 2) Residuals histogram\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.hist(resid, bins=50, edgecolor='k')\n",
    "    plt.xlabel(\"Residual\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Residuals Histogram\")\n",
    "    plt.show()\n",
    "\n",
    "    # 3) Residuals vs Fitted\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.scatter(yhat_flat, resid, alpha=0.7)\n",
    "    plt.axhline(0, color='gray', linewidth=1, linestyle='--')\n",
    "    plt.xlabel(\"Fitted Values\")\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    plt.title(\"Residuals vs Fitted\")\n",
    "    plt.show()\n",
    "\n",
    "    # 4) Q‐Q plot of residuals\n",
    "    fig = plt.figure(figsize=(6,4))\n",
    "    ax4 = fig.add_subplot(111)\n",
    "    sm.qqplot(resid, line='s', ax=ax4)\n",
    "    ax4.set_title(\"Q-Q Plot of Residuals\")\n",
    "    plt.show()\n",
    "\n",
    "    # 5) MSE Heatmap from grid‐search (if applicable)\n",
    "    if grid_search and mse_grid is not None:\n",
    "        # Assume tau_vals and decay_vals were defined inside grid_search_param;\n",
    "        # we can recreate them exactly here for axis ticks:\n",
    "        tau_vals   = np.logspace(-2, 2, num=mse_grid.shape[0])\n",
    "        decay_vals = np.logspace(0, 4, num=mse_grid.shape[1])\n",
    "\n",
    "        plt.figure(figsize=(8,6))\n",
    "        plt.imshow(mse_grid.T, origin='lower', aspect='auto',\n",
    "                   extent=[tau_vals[0], tau_vals[-1], decay_vals[0], decay_vals[-1]])\n",
    "        plt.colorbar(label='MSE')\n",
    "        plt.xlabel('Tau (prior‐variance)')\n",
    "        plt.ylabel('Decay Scale')\n",
    "        plt.title('MSE Heatmap')\n",
    "        plt.xscale('log')\n",
    "        plt.yscale('log')\n",
    "        plt.show()\n",
    "\n",
    "    # 6) Plot each beta‐time series + histogram (unscaled)\n",
    "    names = [\"Intercept\"] + features\n",
    "    K = betas_unscaled.shape[0]\n",
    "\n",
    "    for i in range(K):\n",
    "        b_i = betas_unscaled[i].flatten()  # shape = (T,)\n",
    "        fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(10,4))\n",
    "        ax1.plot(b_i, marker='o', linestyle='-')\n",
    "        ax1.set_title(f\"{names[i]} Over Time\")\n",
    "        ax1.set_xlabel(\"Time\")\n",
    "        ax1.set_ylabel(\"Beta Value\")\n",
    "\n",
    "        ax2.hist(b_i, bins=50, edgecolor='k')\n",
    "        ax2.set_title(f\"{names[i]} Histogram\")\n",
    "        ax2.set_xlabel(\"Beta Value\")\n",
    "        ax2.set_ylabel(\"Frequency\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4) Top‐Level Pipeline Function\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def decay_pipeline(df: pd.DataFrame, features: list, target: str,\n",
    "                   scaling: str = \"z\", scale_kwargs: dict = None,\n",
    "                   backshift: int = 48, tau: float = 10, decay_scale: float = 2880,\n",
    "                   grid_search: bool = True, plot: bool = False):\n",
    "    \"\"\"\n",
    "    Wrapper that:\n",
    "      0) Validates inputs,\n",
    "      1) Scales & shapes data (z, ewm, or identity),\n",
    "      2) Optionally grid‐searches over (tau, decay_scale),\n",
    "      3) Fits final decay_regress model,\n",
    "      4) Un‐scales betas for interpretability (if requested),\n",
    "      5) Produces predictions & metrics (MSE, R2),\n",
    "      6) (Optional) Plots diagnostics.\n",
    "\n",
    "    Args:\n",
    "        df:          pandas DataFrame (length T)\n",
    "        features:    list of feature column names\n",
    "        target:      string name of target column\n",
    "        scaling:     \"z\", \"ewm\", or \"identity\"\n",
    "        scale_kwargs: dict with keys \"halflife\" and \"burnin_steps\" if scaling==\"ewm\"\n",
    "        backshift:   int, time‐lag for coefficients before making predictions\n",
    "        tau:         float, default prior‐variance (if no grid_search)\n",
    "        decay_scale: float, default decay_scale (if no grid_search)\n",
    "        grid_search: bool, whether to run grid_search over (tau, decay_scale)\n",
    "        plot:        bool, whether to call plot_model_diagnostics at the end\n",
    "\n",
    "    Returns:\n",
    "        dict with keys:\n",
    "          \"prior_mean\":   np.ndarray of shape (K,)\n",
    "          \"prior_covar\":  np.ndarray of shape (K,K)  (covariance matrix)\n",
    "          \"decay_scale\":  float, the selected decay scale\n",
    "          \"betas\":        np.ndarray of shape (K, T, 1)  (lagged, unscaled if appropriate)\n",
    "          \"yhat\":         np.ndarray of shape (T, 1)       (predictions in original space)\n",
    "          \"resid\":        np.ndarray of shape (T,)         (yhat_flat - y_flat)\n",
    "          \"mse\":          float\n",
    "          \"r2\":           float\n",
    "    \"\"\"\n",
    "    # 0) Ensure features exist\n",
    "    missing = [c for c in features if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Requested features not found in DataFrame: {missing}\")\n",
    "\n",
    "    # 1) Scale & Shape\n",
    "    if scaling == \"z\":\n",
    "        x, y, means, stds = z_standardize(df, features, target)\n",
    "    elif scaling == \"ewm\":\n",
    "        if not isinstance(scale_kwargs, dict):\n",
    "            raise ValueError(\"When scaling='ewm', scale_kwargs must be a dict with keys 'halflife' and 'burnin_steps'.\")\n",
    "        hl = scale_kwargs.get(\"halflife\")\n",
    "        bs = scale_kwargs.get(\"burnin_steps\")\n",
    "        if hl is None or bs is None:\n",
    "            raise ValueError(\"scale_kwargs must contain 'halflife' and 'burnin_steps'.\")\n",
    "        x, y, means, stds = ewm_standardize(df, features, target, hl, bs)\n",
    "    else:\n",
    "        x, y, means, stds = identity_standardize(df, features, target)\n",
    "\n",
    "    D = x.shape[0]  # number of coefficient‐rows (intercept + features)\n",
    "\n",
    "    # 2) Set Priors and optionally run grid search\n",
    "    if grid_search:\n",
    "        best_tau, best_decay, mse_grid = grid_search_param(x, y, scaling, means, stds, backshift)\n",
    "        prior_mean  = np.zeros(D)\n",
    "        prior_covar = np.eye(D) * best_tau\n",
    "        decay_scale = best_decay\n",
    "    else:\n",
    "        best_tau    = tau\n",
    "        best_decay  = decay_scale\n",
    "        mse_grid    = None\n",
    "        prior_mean  = np.zeros(D)\n",
    "        prior_covar = np.eye(D) * tau\n",
    "\n",
    "    # 3) Fit final regression (in scaled‐space)\n",
    "    betas = decay_regress(\n",
    "        x=x, y=y,\n",
    "        prior_mean=prior_mean,\n",
    "        prior_covar=prior_covar,\n",
    "        decay_scale=decay_scale\n",
    "    )  # shape = (D, T, 1)\n",
    "\n",
    "    # 4) Un‐scale betas for interpretability (optional)\n",
    "    if scaling == \"z\":\n",
    "        betas_unscaled = z_unstandardize(betas, means, stds)\n",
    "    elif scaling == \"ewm\":\n",
    "        betas_unscaled = ewm_unstandardize(betas, means, stds)\n",
    "    else:\n",
    "        betas_unscaled = betas  # identity: no scaling applied\n",
    "\n",
    "    # 5) Backshift & Predict (still in scaled‐space, because y was never scaled)\n",
    "    betas_lagged = np.roll(betas, shift=backshift, axis=1)\n",
    "    betas_lagged[:, :backshift, :] = np.nan\n",
    "    yhat = np.nansum(x * betas_lagged, axis=0)  # shape = (T, 1)\n",
    "\n",
    "    # 6) Compute metrics (all in original y‐units, since y was never scaled):\n",
    "    y_flat    = y.flatten()\n",
    "    yhat_flat = yhat.flatten()\n",
    "    resid     = yhat_flat - y_flat\n",
    "    mse       = np.nanmean(resid**2)\n",
    "    ss_res    = np.nansum((y_flat - yhat_flat)**2)\n",
    "    ss_tot    = np.nansum((y_flat - np.nanmean(y_flat))**2)\n",
    "    r2        = 1 - (ss_res / ss_tot) if ss_tot > 0 else np.nan\n",
    "\n",
    "    # 7) Plot diagnostics if requested\n",
    "    if plot:\n",
    "        plot_model_diagnostics(y_flat, yhat_flat, resid, betas_unscaled, features, mse_grid, grid_search)\n",
    "\n",
    "    return {\n",
    "        \"prior_mean\":   prior_mean,\n",
    "        \"prior_covar\":  prior_covar,\n",
    "        \"decay_scale\":  decay_scale,\n",
    "        \"betas\":        betas_lagged,\n",
    "        \"yhat\":         yhat,\n",
    "        \"resid\":        resid,\n",
    "        \"mse\":          mse,\n",
    "        \"r2\":           r2\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cd6721",
   "metadata": {},
   "source": [
    "### Data Pre Processing Steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5090b2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "\n",
    "\n",
    "# 1) ---------------- LagFeaturesTransformer ----------------\n",
    "class LagFeaturesTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    - For every column in X:\n",
    "      • If column name contains \"_f\" → keep it untouched.\n",
    "      • If column is dtype 'category'     → keep it untouched.\n",
    "      • Otherwise, if column is numeric (including \"imbalance\"):\n",
    "          • create a lagged version \"<col>_lag1\" = df[col].shift(1)\n",
    "          • drop the original \"<col>\"\n",
    "      • Any non-numeric, non-forecast, non-categorical column is dropped.\n",
    "    \"\"\"\n",
    "    def __init__(self, lag: int = 1):\n",
    "        self.lag = lag\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = X.copy()\n",
    "        to_drop = []\n",
    "        lagged_data = {}\n",
    "\n",
    "        for col in df.columns:\n",
    "            if \"_f\" in col:\n",
    "                # forecast columns: keep as-is\n",
    "                continue\n",
    "\n",
    "            if pd.api.types.is_categorical_dtype(df[col]):\n",
    "                # already categorical: keep as-is\n",
    "                continue\n",
    "\n",
    "            if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                # numeric & not a forecast: create lag and drop original\n",
    "                lagged_name = f\"{col}_lag{self.lag}\"\n",
    "                lagged_data[lagged_name] = df[col].shift(self.lag)\n",
    "                to_drop.append(col)\n",
    "            else:\n",
    "                # non-numeric, non-forecast, non-categorical: drop\n",
    "                to_drop.append(col)\n",
    "\n",
    "        # Insert all lagged columns\n",
    "        for lagged_name, series in lagged_data.items():\n",
    "            df[lagged_name] = series\n",
    "\n",
    "        # Drop the originals\n",
    "        df = df.drop(columns=to_drop)\n",
    "        return df\n",
    "\n",
    "\n",
    "# 2) ---------------- CategoricalCaster ----------------\n",
    "class CategoricalCaster(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    - Given a list of column names, casts df[col] → pd.Categorical.\n",
    "      Leaves everything else untouched.\n",
    "    \"\"\"\n",
    "    def __init__(self, categorical_columns: list[str]):\n",
    "        self.categorical_columns = categorical_columns\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = X.copy()\n",
    "        for col in self.categorical_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].astype(\"category\")\n",
    "        return df\n",
    "\n",
    "\n",
    "# 3) ------------- PercentileCategorizer ----------------\n",
    "class PercentileCategorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    - cols_to_cat: list of numeric column names to bin by percentile.\n",
    "    - percentile: float in (0,1), e.g. 0.90 for 90th percentile.\n",
    "    - new_suffix: suffix for each new category column (e.g. \"_pctcat\").\n",
    "    \n",
    "    In fit(X):\n",
    "      • For each col in cols_to_cat, compute threshold = X[col].quantile(percentile).\n",
    "    In transform(X):\n",
    "      • For each col, create \"<col>_pctcat\" = \"high\" if X[col] >= threshold, else \"normal\".\n",
    "      • Cast each new \"<col>_pctcat\" to dtype 'category'.\n",
    "      • Leave original numeric columns intact.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        cols_to_cat: list[str],\n",
    "        percentile: float = 0.9,\n",
    "        new_suffix: str = \"_pctcat\"\n",
    "    ):\n",
    "        self.cols_to_cat = cols_to_cat\n",
    "        self.percentile = percentile\n",
    "        self.new_suffix = new_suffix\n",
    "        self.thresholds_: dict[str, float] = {}\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y=None):\n",
    "        for col in self.cols_to_cat:\n",
    "            if col not in X.columns:\n",
    "                raise ValueError(f\"Column '{col}' not found in X during PercentileCategorizer.fit().\")\n",
    "            if not pd.api.types.is_numeric_dtype(X[col]):\n",
    "                raise ValueError(f\"Column '{col}' must be numeric to create percentile categories.\")\n",
    "            self.thresholds_[col] = float(X[col].quantile(self.percentile))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = X.copy()\n",
    "        for col, thresh in self.thresholds_.items():\n",
    "            new_col = f\"{col}{self.new_suffix}\"\n",
    "            df[new_col] = np.where(df[col] >= thresh, \"high\", \"normal\")\n",
    "            df[new_col] = pd.Categorical(df[new_col], categories=[\"normal\", \"high\"])\n",
    "        return df\n",
    "\n",
    "\n",
    "# 4) ------------- The Full Feature-Engineering Pipeline -------------\n",
    "def build_feature_pipeline(\n",
    "    categorical_columns: list[str],\n",
    "    percentile_columns: list[str],\n",
    "    percentile: float = 0.9,\n",
    "    lag: int = 1\n",
    ") -> Pipeline:\n",
    "    \"\"\"\n",
    "    Returns an sklearn Pipeline that:\n",
    "      1) Lag all numeric columns not containing \"_f\" (and drop the originals).\n",
    "      2) Cast any pre-existing columns in `categorical_columns` to dtype 'category'.\n",
    "      3) For each column in `percentile_columns`, create a new \"<col>_pctcat\"\n",
    "         categorical column based on the given percentile.\n",
    "      4) Run a ColumnTransformer that:\n",
    "         - Imputes (median) and scales (StandardScaler) all remaining numeric columns.\n",
    "         - One-hot-encodes all categorical columns (including newly created \"<col>_pctcat\").\n",
    "    \"\"\"\n",
    "    # Step 1: lag & drop\n",
    "    lag_step = (\"lag_features\", LagFeaturesTransformer(lag=lag))\n",
    "\n",
    "    # Step 2: cast existing categories\n",
    "    cast_cat_step = (\"cast_categoricals\", CategoricalCaster(categorical_columns))\n",
    "\n",
    "    # Step 3: create percentile-based categories\n",
    "    pct_step = (\"percentile_categories\", PercentileCategorizer(\n",
    "        cols_to_cat=percentile_columns,\n",
    "        percentile=percentile,\n",
    "        new_suffix=\"_pctcat\"\n",
    "    ))\n",
    "\n",
    "    # Step 4: final ColumnTransformer\n",
    "    numeric_pipeline = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_pipeline = Pipeline([\n",
    "        (\"onehot\", OneHotEncoder(sparse=False, drop=\"if_binary\"))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            # All remaining numeric columns\n",
    "            (\"num\", numeric_pipeline, make_column_selector(dtype_include=\"number\")),\n",
    "            # All remaining categorical columns\n",
    "            (\"cat\", categorical_pipeline, make_column_selector(dtype_include=\"category\")),\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        lag_step,\n",
    "        cast_cat_step,\n",
    "        pct_step,\n",
    "        (\"preprocessor\", preprocessor),\n",
    "    ])\n",
    "\n",
    "    return pipeline\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
