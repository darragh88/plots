{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41f3c243",
   "metadata": {},
   "source": [
    "### DataLoading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a444585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Literal, Union\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Module-level docs & type aliases\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "Generic loaders & converters for regional-timeseries data.\n",
    "\n",
    "Canonical representation\n",
    "------------------------\n",
    "    CanonicalData = dict[str, pd.DataFrame]\n",
    "        key   – region name\n",
    "        value – DataFrame indexed by datetime; columns = features\n",
    "\n",
    "Disk layouts we support\n",
    "-----------------------\n",
    "    \"multi\" : MultiIndex columns (level-0 = region, level-1 = feature)\n",
    "    \"long\"  : Normal DataFrame with a *region* column\n",
    "    \"wide\"  : Wide DataFrame whose column names are \"<region><sep><feature>\"\n",
    "\"\"\"\n",
    "\n",
    "CanonicalData = Dict[str, pd.DataFrame]\n",
    "Layout        = Literal[\"multi\", \"long\", \"wide\"]\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# ─── Converters: DISK ➜ CANONICAL ────────────────────────────────────────────\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def _multi_to_dict(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    cols: List[str] | None = None\n",
    ") -> CanonicalData:\n",
    "    \"\"\"\n",
    "    Convert a Multi-Index column DataFrame to the canonical dict form.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df   : DataFrame with two column levels (region, feature)\n",
    "    cols : optional list of feature names to *keep* (others are dropped)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, DataFrame]  mapping region → feature-matrix\n",
    "    \"\"\"\n",
    "    out: CanonicalData = {}\n",
    "    for region in df.columns.get_level_values(0).unique():\n",
    "        sub = df.xs(region, axis=1, level=0)\n",
    "        if cols is not None:\n",
    "            sub = sub[[c for c in cols if c in sub.columns]]\n",
    "        out[region] = sub.copy()\n",
    "    return out\n",
    "\n",
    "\n",
    "def _long_to_dict(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    region_col: str = \"region\",\n",
    "    cols: List[str] | None = None,\n",
    ") -> CanonicalData:\n",
    "    \"\"\"\n",
    "    Convert a ‘long’ (tidy) DataFrame to canonical dict form.\n",
    "\n",
    "    A *long* frame must contain a `region_col` column; all other columns\n",
    "    are interpreted as features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df         : tidy DataFrame with duplicate timestamps per region\n",
    "    region_col : column that identifies the region\n",
    "    cols       : optional feature filter\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Canonical dict keyed by region\n",
    "    \"\"\"\n",
    "    if region_col not in df.columns:\n",
    "        raise KeyError(f\"Expected column '{region_col}' in long-layout dataframe.\")\n",
    "\n",
    "    features = [c for c in df.columns if c != region_col]\n",
    "    if cols is not None:\n",
    "        features = [c for c in features if c in cols]\n",
    "\n",
    "    out: CanonicalData = {}\n",
    "    for region, grp in df.groupby(region_col):\n",
    "        frame = grp.drop(columns=region_col)\n",
    "        out[region] = frame[features].copy()\n",
    "    return out\n",
    "\n",
    "\n",
    "def _wide_to_dict(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    sep: str = \"-\",\n",
    "    cols: List[str] | None = None,\n",
    ") -> CanonicalData:\n",
    "    \"\"\"\n",
    "    Convert a wide DataFrame with ‘region-feature’ column names into canon dict.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df   : DataFrame whose columns look like \"tokyo-demand\"\n",
    "    sep  : separator between region and feature\n",
    "    cols : optional feature filter\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Canonical dict keyed by region\n",
    "    \"\"\"\n",
    "    parts = df.columns.to_series().str.split(sep, n=1, expand=True)\n",
    "    if parts.isna().any().any():\n",
    "        raise ValueError(\n",
    "            f\"Column names do not all match <region>{sep}<feature> pattern.\"\n",
    "        )\n",
    "\n",
    "    out: CanonicalData = {}\n",
    "    for region in parts[0].unique():\n",
    "        mask      = parts[0] == region\n",
    "        sub_cols  = df.columns[mask]\n",
    "        features  = parts[1][mask]\n",
    "        sub_frame = df[sub_cols].copy()\n",
    "        sub_frame.columns = features\n",
    "        if cols is not None:\n",
    "            sub_frame = sub_frame[[c for c in cols if c in sub_frame.columns]]\n",
    "        out[region] = sub_frame\n",
    "    return out\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# ─── Converters: CANONICAL ➜ DISK ────────────────────────────────────────────\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def _dict_to_multi(data: CanonicalData) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combine a canonical dict into a Multi-Index column DataFrame.\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    for region, df in data.items():\n",
    "        tmp = df.copy()\n",
    "        tmp.columns = pd.MultiIndex.from_product([[region], tmp.columns])\n",
    "        frames.append(tmp)\n",
    "    return pd.concat(frames, axis=1).sort_index(axis=1)\n",
    "\n",
    "\n",
    "def _dict_to_long(\n",
    "    data: CanonicalData,\n",
    "    *,\n",
    "    region_col: str = \"region\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combine a canonical dict into a long (tidy) DataFrame.\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    for region, df in data.items():\n",
    "        tmp = df.copy()\n",
    "        tmp[region_col] = region\n",
    "        frames.append(tmp)\n",
    "    combined = pd.concat(frames)\n",
    "    order = [c for c in combined.columns if c != region_col] + [region_col]\n",
    "    return combined[order]\n",
    "\n",
    "\n",
    "def _dict_to_wide(\n",
    "    data: CanonicalData,\n",
    "    *,\n",
    "    sep: str = \"-\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combine a canonical dict into wide ‘region-feature’ columns.\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    for region, df in data.items():\n",
    "        tmp = df.copy()\n",
    "        tmp.columns = [f\"{region}{sep}{c}\" for c in tmp.columns]\n",
    "        frames.append(tmp)\n",
    "    return pd.concat(frames, axis=1)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# ─── Public helpers: one-file load / convert ────────────────────────────────\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def load_parquet_as_canonical(\n",
    "    path: str | Path,\n",
    "    *,\n",
    "    layout: Layout,\n",
    "    region_col: str = \"region\",\n",
    "    sep: str = \"-\",\n",
    "    cols: List[str] | None = None,\n",
    ") -> CanonicalData:\n",
    "    \"\"\"\n",
    "    Read a parquet file of known *layout* and return canonical dict form.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path       : file path\n",
    "    layout     : \"multi\", \"long\", or \"wide\" (must match the file)\n",
    "    region_col : name of the region column for *long* layout\n",
    "    sep        : region-feature separator for *wide* layout\n",
    "    cols       : optional feature subset\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    CanonicalData\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(path)\n",
    "    if layout == \"multi\":\n",
    "        return _multi_to_dict(df, cols=cols)\n",
    "    if layout == \"long\":\n",
    "        return _long_to_dict(df, region_col=region_col, cols=cols)\n",
    "    if layout == \"wide\":\n",
    "        return _wide_to_dict(df, sep=sep, cols=cols)\n",
    "    raise ValueError(f\"Unsupported layout '{layout}'.\")\n",
    "\n",
    "\n",
    "def canonical_to_layout(\n",
    "    data: CanonicalData,\n",
    "    layout: Layout,\n",
    "    *,\n",
    "    region_col: str = \"region\",\n",
    "    sep: str = \"-\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert *canonical* dict back to the requested layout format.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data       : canonical dict\n",
    "    layout     : target layout (\"multi\", \"long\", \"wide\")\n",
    "    region_col : name of region column for long layout\n",
    "    sep        : separator for wide layout\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame in the specified layout\n",
    "    \"\"\"\n",
    "    if layout == \"multi\":\n",
    "        return _dict_to_multi(data)\n",
    "    if layout == \"long\":\n",
    "        return _dict_to_long(data, region_col=region_col)\n",
    "    if layout == \"wide\":\n",
    "        return _dict_to_wide(data, sep=sep)\n",
    "    raise ValueError(f\"Unsupported target layout '{layout}'.\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# ─── Helpers for multi-file ingestion ───────────────────────────────────────\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def _collect_by_layout(\n",
    "    paths: List[str] | None,\n",
    "    layout: Layout,\n",
    "    *,\n",
    "    cols: List[str] | None,\n",
    "    region_col: str,\n",
    "    sep: str,\n",
    ") -> Dict[str, List[pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Internal helper: load every file in `paths` (given its layout)\n",
    "    and collate DataFrames by region.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[region, list[DataFrame]]\n",
    "    \"\"\"\n",
    "    bucket: Dict[str, List[pd.DataFrame]] = {}\n",
    "    for p in paths or []:\n",
    "        data = load_parquet_as_canonical(\n",
    "            p, layout=layout, region_col=region_col, sep=sep, cols=cols\n",
    "        )\n",
    "        for region, df in data.items():\n",
    "            bucket.setdefault(region, []).append(df)\n",
    "    return bucket\n",
    "\n",
    "\n",
    "def load_region_dataset(\n",
    "    *,\n",
    "    region: str,\n",
    "    multi_paths: List[str] | None = None,\n",
    "    long_paths: List[str] | None = None,\n",
    "    wide_paths: List[str] | None = None,\n",
    "    cols: List[str] | None = None,\n",
    "    region_col: str = \"region\",\n",
    "    sep: str = \"-\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate **one** region’s data from any number of parquet files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    region      : name of the region to extract\n",
    "    multi_paths : list of files in *multi* layout\n",
    "    long_paths  : list of files in *long* layout\n",
    "    wide_paths  : list of files in *wide* layout\n",
    "    cols        : optional feature subset\n",
    "    region_col  : region column (long layout)\n",
    "    sep         : separator (wide layout)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame with union of timestamps & features for that region\n",
    "    (duplicate columns are de-duplicated, keeping last-read version).\n",
    "    \"\"\"\n",
    "    frames: List[pd.DataFrame] = []\n",
    "    for layout, paths in (\n",
    "        (\"multi\", multi_paths),\n",
    "        (\"long\", long_paths),\n",
    "        (\"wide\", wide_paths),\n",
    "    ):\n",
    "        for p in paths or []:\n",
    "            data = load_parquet_as_canonical(\n",
    "                p, layout=layout, region_col=region_col, sep=sep, cols=cols\n",
    "            )\n",
    "            if region in data:\n",
    "                frames.append(data[region])\n",
    "\n",
    "    if not frames:\n",
    "        raise KeyError(f\"Region '{region}' not found in supplied paths.\")\n",
    "\n",
    "    combined = pd.concat(frames, axis=1)\n",
    "    combined = combined.loc[:, ~combined.columns.duplicated()]  # keep last duplicate\n",
    "    combined.sort_index(inplace=True)\n",
    "    return combined\n",
    "\n",
    "\n",
    "def load_all_regions_dataset(\n",
    "    *,\n",
    "    multi_paths: List[str] | None = None,\n",
    "    long_paths: List[str] | None = None,\n",
    "    wide_paths: List[str] | None = None,\n",
    "    regions: List[str] | None = None,\n",
    "    cols: List[str] | None = None,\n",
    "    region_col: str = \"region\",\n",
    "    sep: str = \"-\",\n",
    ") -> CanonicalData:\n",
    "    \"\"\"\n",
    "    Load **every** region (or a specified subset) from the supplied file lists.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    multi_paths : parquet files in *multi* layout\n",
    "    long_paths  : parquet files in *long* layout\n",
    "    wide_paths  : parquet files in *wide* layout\n",
    "    regions     : optional subset of region names to return\n",
    "    cols        : optional feature subset\n",
    "    region_col  : region column (long layout)\n",
    "    sep         : separator (wide layout)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    CanonicalData containing one DataFrame per region.\n",
    "    \"\"\"\n",
    "    # Read & collate per-layout\n",
    "    bucket: Dict[str, List[pd.DataFrame]] = {}\n",
    "\n",
    "    for paths, layout in (\n",
    "        (multi_paths, \"multi\"),\n",
    "        (long_paths, \"long\"),\n",
    "        (wide_paths, \"wide\"),\n",
    "    ):\n",
    "        part = _collect_by_layout(\n",
    "            paths,\n",
    "            layout,\n",
    "            cols=cols,\n",
    "            region_col=region_col,\n",
    "            sep=sep,\n",
    "        )\n",
    "        for region, frames in part.items():\n",
    "            bucket.setdefault(region, []).extend(frames)\n",
    "\n",
    "    # Filter to requested subset (if any)\n",
    "    if regions is not None:\n",
    "        bucket = {r: bucket[r] for r in regions if r in bucket}\n",
    "\n",
    "    # Merge frames per region\n",
    "    out: CanonicalData = {}\n",
    "    for region, frames in bucket.items():\n",
    "        df = pd.concat(frames, axis=1)\n",
    "        df = df.loc[:, ~df.columns.duplicated()]\n",
    "        df.sort_index(inplace=True)\n",
    "        out[region] = df\n",
    "\n",
    "    # Handle missing regions gracefully\n",
    "    if regions is not None:\n",
    "        missing = set(regions) - set(out)\n",
    "        if missing:\n",
    "            raise KeyError(\n",
    "                f\"The following requested regions were not found in any file: \"\n",
    "                f\"{', '.join(sorted(missing))}\"\n",
    "            )\n",
    "\n",
    "    return out\n",
    "#88888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Literal\n",
    "import pandas as pd\n",
    "\n",
    "# -------------  Canonical type & layout tags  --------------------------------\n",
    "CanonicalData = Dict[str, pd.DataFrame]\n",
    "Layout        = Literal[\"multi\", \"long\", \"wide\"]\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# ═════════════════════════════ I/O CONVERSION ════════════════════════════════\n",
    "# (unchanged from earlier — trimmed for brevity; keep the same helper fns:\n",
    "#     _multi_to_dict, _long_to_dict, _wide_to_dict,\n",
    "#     _dict_to_multi,  _dict_to_long,  _dict_to_wide,\n",
    "#     load_parquet_as_canonical, canonical_to_layout)\n",
    "# ----------------------------------------------------------------------------- \n",
    "\n",
    "# ...  <keep earlier converter code here exactly as before> ...\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# ══════════════════════════ PRE-MERGE FEATURE PASS ═══════════════════════════\n",
    "# ----------------------------------------------------------------------------- \n",
    "\n",
    "from jp_da_imb.utils.time import construct_time_features   # ← your existing util\n",
    "\n",
    "\n",
    "def preprocess_region_df(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    start_date: str | pd.Timestamp | None = None,\n",
    "    end_date:   str | pd.Timestamp | None = None,\n",
    "    freq: str | None = \"30T\",\n",
    "    na_removal: bool = True,\n",
    "    add_time_feats: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply *per-file* hygiene & feature engineering **before** merging with other\n",
    "    DataFrames for the same region.\n",
    "\n",
    "    Steps (all optional, controlled by kwargs)\n",
    "    -----------------------------------------\n",
    "    1. Clip to [start_date, end_date] (if provided)\n",
    "    2. Resample to `freq` (mean aggregation)       — skip if `freq=None`\n",
    "    3. Drop rows with any NA                       — only if `na_removal=True`\n",
    "    4. Add calendar/time features (and cast them\n",
    "       to `category`) using your `construct_time_features`\n",
    "       — only if `add_time_feats=True`\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    # 1. Clip date range -------------------------------------------------------\n",
    "    if start_date is not None:\n",
    "        out = out.loc[out.index >= pd.to_datetime(start_date)]\n",
    "    if end_date is not None:\n",
    "        out = out.loc[out.index <= pd.to_datetime(end_date)]\n",
    "\n",
    "    # 2. Resample --------------------------------------------------------------\n",
    "    if freq is not None:\n",
    "        out = out.resample(freq).mean()\n",
    "\n",
    "    # 3. Remove NAs ------------------------------------------------------------\n",
    "    if na_removal:\n",
    "        out = out.dropna()\n",
    "\n",
    "    # 4. Calendar / categorical time features ---------------------------------\n",
    "    if add_time_feats:\n",
    "        construct_time_features(out)     # mutates in place, adds columns\n",
    "        time_cols = [\n",
    "            \"weekday\", \"hour\", \"month\", \"quarter\",\n",
    "            \"koma\", \"koma_week\", \"is_holiday\",\n",
    "            \"is_peak\", \"is_weekend\",\n",
    "        ]\n",
    "        for c in time_cols:\n",
    "            if c in out.columns and not pd.api.types.is_categorical_dtype(out[c]):\n",
    "                out[c] = pd.Categorical(out[c])\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# ═══════════════  MULTI-FILE INGEST, NOW WITH PRE-PASS  ══════════════════════\n",
    "# ----------------------------------------------------------------------------- \n",
    "\n",
    "def _collect_by_layout(\n",
    "    paths: List[str] | None,\n",
    "    layout: Layout,\n",
    "    *,\n",
    "    cols: List[str] | None,\n",
    "    region_col: str,\n",
    "    sep: str,\n",
    "    preprocess_kwargs: dict,\n",
    ") -> Dict[str, List[pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Load every parquet file in `paths` -> canonical dict,\n",
    "    run `preprocess_region_df` on each region DataFrame,\n",
    "    and bucket them by region.\n",
    "    \"\"\"\n",
    "    bucket: Dict[str, List[pd.DataFrame]] = {}\n",
    "    for p in paths or []:\n",
    "        data = load_parquet_as_canonical(\n",
    "            p, layout=layout, region_col=region_col, sep=sep, cols=cols\n",
    "        )\n",
    "        for region, df in data.items():\n",
    "            df_proc = preprocess_region_df(df, **preprocess_kwargs)\n",
    "            bucket.setdefault(region, []).append(df_proc)\n",
    "    return bucket\n",
    "\n",
    "\n",
    "def load_region_dataset(\n",
    "    *,\n",
    "    region: str,\n",
    "    multi_paths: List[str] | None = None,\n",
    "    long_paths:  List[str] | None = None,\n",
    "    wide_paths:  List[str] | None = None,\n",
    "    cols:       List[str] | None = None,\n",
    "    region_col: str = \"region\",\n",
    "    sep: str = \"-\",\n",
    "    # --- pre-processing options ----------------------------------------------\n",
    "    start_date: str | pd.Timestamp | None = None,\n",
    "    end_date:   str | pd.Timestamp | None = None,\n",
    "    freq: str | None = \"30T\",\n",
    "    na_removal: bool = True,\n",
    "    add_time_feats: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a **single-region** DataFrame by reading any combination of\n",
    "    *multi*, *long*, and *wide* parquet files, applying the pre-processing\n",
    "    steps to each file **before** they are merged together.\n",
    "\n",
    "    Other parameters are identical to the earlier version, plus the\n",
    "    pre-processing kwargs shown above.\n",
    "    \"\"\"\n",
    "    preprocess_kwargs = dict(\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        freq=freq,\n",
    "        na_removal=na_removal,\n",
    "        add_time_feats=add_time_feats,\n",
    "    )\n",
    "\n",
    "    frames: List[pd.DataFrame] = []\n",
    "    for layout, paths in (\n",
    "        (\"multi\", multi_paths),\n",
    "        (\"long\",  long_paths),\n",
    "        (\"wide\",  wide_paths),\n",
    "    ):\n",
    "        for p in paths or []:\n",
    "            data = load_parquet_as_canonical(\n",
    "                p, layout=layout, region_col=region_col, sep=sep, cols=cols\n",
    "            )\n",
    "            if region in data:\n",
    "                frames.append(preprocess_region_df(data[region], **preprocess_kwargs))\n",
    "\n",
    "    if not frames:\n",
    "        raise KeyError(f\"Region '{region}' not found in supplied paths.\")\n",
    "\n",
    "    combined = pd.concat(frames, axis=1)\n",
    "    combined = combined.loc[:, ~combined.columns.duplicated()]  # keep last dup\n",
    "    combined.sort_index(inplace=True)\n",
    "    return combined\n",
    "\n",
    "\n",
    "def load_all_regions_dataset(\n",
    "    *,\n",
    "    multi_paths: List[str] | None = None,\n",
    "    long_paths:  List[str] | None = None,\n",
    "    wide_paths:  List[str] | None = None,\n",
    "    regions:     List[str] | None = None,\n",
    "    cols:       List[str] | None = None,\n",
    "    region_col: str = \"region\",\n",
    "    sep: str = \"-\",\n",
    "    # --- pre-processing options ----------------------------------------------\n",
    "    start_date: str | pd.Timestamp | None = None,\n",
    "    end_date:   str | pd.Timestamp | None = None,\n",
    "    freq: str | None = \"30T\",\n",
    "    na_removal: bool = True,\n",
    "    add_time_feats: bool = True,\n",
    ") -> CanonicalData:\n",
    "    \"\"\"\n",
    "    Load **all** regions (or a given subset) from the provided file lists,\n",
    "    applying the pre-merge feature-creation steps to every individual file.\n",
    "    \"\"\"\n",
    "    preprocess_kwargs = dict(\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        freq=freq,\n",
    "        na_removal=na_removal,\n",
    "        add_time_feats=add_time_feats,\n",
    "    )\n",
    "\n",
    "    bucket: Dict[str, List[pd.DataFrame]] = {}\n",
    "\n",
    "    for paths, layout in (\n",
    "        (multi_paths, \"multi\"),\n",
    "        (long_paths,  \"long\"),\n",
    "        (wide_paths,  \"wide\"),\n",
    "    ):\n",
    "        part = _collect_by_layout(\n",
    "            paths,\n",
    "            layout,\n",
    "            cols=cols,\n",
    "            region_col=region_col,\n",
    "            sep=sep,\n",
    "            preprocess_kwargs=preprocess_kwargs,\n",
    "        )\n",
    "        for region, frames in part.items():\n",
    "            bucket.setdefault(region, []).extend(frames)\n",
    "\n",
    "    # subset filter\n",
    "    if regions is not None:\n",
    "        bucket = {r: bucket[r] for r in regions if r in bucket}\n",
    "\n",
    "    # merge frames per region\n",
    "    out: CanonicalData = {}\n",
    "    for region, frames in bucket.items():\n",
    "        df = pd.concat(frames, axis=1)\n",
    "        df = df.loc[:, ~df.columns.duplicated()]\n",
    "        df.sort_index(inplace=True)\n",
    "        out[region] = df\n",
    "\n",
    "    # raise on missing explicit requests\n",
    "    if regions is not None:\n",
    "        missing = set(regions) - set(out)\n",
    "        if missing:\n",
    "            raise KeyError(\n",
    "                f\"The following requested regions were not found in any file: \"\n",
    "                f\\\"{', '.join(sorted(missing))}\\\"\n",
    "            )\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1b635a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Literal\n",
    "import pandas as pd\n",
    "from jp_da_imb.utils.time import construct_time_features   # ← your util\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Canonical type & layout tags\n",
    "# -----------------------------------------------------------\n",
    "CanonicalData = Dict[str, pd.DataFrame]\n",
    "Layout        = Literal[\"multi\", \"long\", \"wide\"]\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "#  DISK ➜ CANONICAL converters (unchanged from earlier)\n",
    "#  _multi_to_dict, _long_to_dict, _wide_to_dict\n",
    "#  CANONICAL ➜ DISK converters\n",
    "#  _dict_to_multi, _dict_to_long, _dict_to_wide\n",
    "#  load_parquet_as_canonical, canonical_to_layout\n",
    "#  (omitted here for brevity but keep exactly as before)\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "#  Per-file pre-merge feature pass (unchanged)\n",
    "# -----------------------------------------------------------\n",
    "def preprocess_region_df(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    start_date: str | pd.Timestamp | None = None,\n",
    "    end_date:   str | pd.Timestamp | None = None,\n",
    "    freq: str | None = \"30T\",\n",
    "    na_removal: bool = True,\n",
    "    add_time_feats: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "\n",
    "    if start_date is not None:\n",
    "        out = out.loc[out.index >= pd.to_datetime(start_date)]\n",
    "    if end_date is not None:\n",
    "        out = out.loc[out.index <= pd.to_datetime(end_date)]\n",
    "\n",
    "    if freq is not None:\n",
    "        out = out.resample(freq).mean()\n",
    "\n",
    "    if na_removal:\n",
    "        out = out.dropna()\n",
    "\n",
    "    if add_time_feats:\n",
    "        construct_time_features(out)\n",
    "        for c in [\n",
    "            \"weekday\", \"hour\", \"month\", \"quarter\",\n",
    "            \"koma\", \"koma_week\", \"is_holiday\",\n",
    "            \"is_peak\", \"is_weekend\",\n",
    "        ]:\n",
    "            if c in out.columns and not pd.api.types.is_categorical_dtype(out[c]):\n",
    "                out[c] = pd.Categorical(out[c])\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "#  SINGLE-REGION LOADER  (long files can differ in region_col)\n",
    "# -----------------------------------------------------------\n",
    "def load_region_dataset(\n",
    "    *,\n",
    "    region: str,\n",
    "    multi_paths: List[str] | None = None,\n",
    "    long_paths:  List[str] | None = None,\n",
    "    wide_paths:  List[str] | None = None,\n",
    "    long_region_cols: List[str] | None = None,\n",
    "    cols:       List[str] | None = None,\n",
    "    sep: str = \"-\",\n",
    "    # --- pre-processing options\n",
    "    start_date: str | pd.Timestamp | None = None,\n",
    "    end_date:   str | pd.Timestamp | None = None,\n",
    "    freq: str | None = \"30T\",\n",
    "    na_removal: bool = True,\n",
    "    add_time_feats: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate *one* region’s data from any mix of multi/long/wide parquet files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    region            : which region to return\n",
    "    long_region_cols  : list parallel to `long_paths` giving the column\n",
    "                        name that holds the region label *for each* file.\n",
    "                        Omit to default every long file to \\\"region\\\".\n",
    "    All other args are as before.\n",
    "    \"\"\"\n",
    "    preprocess_kwargs = dict(\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        freq=freq,\n",
    "        na_removal=na_removal,\n",
    "        add_time_feats=add_time_feats,\n",
    "    )\n",
    "\n",
    "    frames: List[pd.DataFrame] = []\n",
    "\n",
    "    # ---- multi layout (single region column level 0) ------------------------\n",
    "    for p in multi_paths or []:\n",
    "        data = load_parquet_as_canonical(p, layout=\"multi\", cols=cols)\n",
    "        if region in data:\n",
    "            frames.append(preprocess_region_df(data[region], **preprocess_kwargs))\n",
    "\n",
    "    # ---- wide layout --------------------------------------------------------\n",
    "    for p in wide_paths or []:\n",
    "        data = load_parquet_as_canonical(p, layout=\"wide\", sep=sep, cols=cols)\n",
    "        if region in data:\n",
    "            frames.append(preprocess_region_df(data[region], **preprocess_kwargs))\n",
    "\n",
    "    # ---- long layout  (per-file region_col!) -------------------------------\n",
    "    if long_paths:\n",
    "        # fill default list if none supplied\n",
    "        if long_region_cols is None:\n",
    "            long_region_cols = [\"region\"] * len(long_paths)\n",
    "        if len(long_region_cols) != len(long_paths):\n",
    "            raise ValueError(\n",
    "                \"long_region_cols must be the same length as long_paths\"\n",
    "            )\n",
    "\n",
    "        for p, reg_col in zip(long_paths, long_region_cols):\n",
    "            data = load_parquet_as_canonical(\n",
    "                p, layout=\"long\", region_col=reg_col, cols=cols\n",
    "            )\n",
    "            if region in data:\n",
    "                frames.append(preprocess_region_df(data[region], **preprocess_kwargs))\n",
    "\n",
    "    if not frames:\n",
    "        raise KeyError(f\"Region '{region}' not found in supplied paths.\")\n",
    "\n",
    "    combined = pd.concat(frames, axis=1)\n",
    "    combined = combined.loc[:, ~combined.columns.duplicated()]\n",
    "    combined.sort_index(inplace=True)\n",
    "    return combined\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "#  ALL-REGIONS LOADER  (long paths paired with region_cols)\n",
    "# -----------------------------------------------------------\n",
    "def load_all_regions_dataset(\n",
    "    *,\n",
    "    multi_paths: List[str] | None = None,\n",
    "    long_paths:  List[str] | None = None,\n",
    "    wide_paths:  List[str] | None = None,\n",
    "    long_region_cols: List[str] | None = None,\n",
    "    regions:     List[str] | None = None,\n",
    "    cols:        List[str] | None = None,\n",
    "    sep: str = \"-\",\n",
    "    # --- pre-processing options\n",
    "    start_date: str | pd.Timestamp | None = None,\n",
    "    end_date:   str | pd.Timestamp | None = None,\n",
    "    freq: str | None = \"30T\",\n",
    "    na_removal: bool = True,\n",
    "    add_time_feats: bool = True,\n",
    ") -> CanonicalData:\n",
    "    \"\"\"\n",
    "    Load every region (or the ones in `regions`) applying the pre-merge\n",
    "    feature pass. Long-format files can each declare their own region column\n",
    "    via `long_region_cols`.\n",
    "    \"\"\"\n",
    "    preprocess_kwargs = dict(\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        freq=freq,\n",
    "        na_removal=na_removal,\n",
    "        add_time_feats=add_time_feats,\n",
    "    )\n",
    "\n",
    "    bucket: Dict[str, List[pd.DataFrame]] = {}\n",
    "\n",
    "    # -------- helper to merge into bucket -----------------\n",
    "    def _collect(data_dict: CanonicalData):\n",
    "        for r, df in data_dict.items():\n",
    "            bucket.setdefault(r, []).append(\n",
    "                preprocess_region_df(df, **preprocess_kwargs)\n",
    "            )\n",
    "\n",
    "    # ---- multi files -------------------------------------\n",
    "    for p in multi_paths or []:\n",
    "        _collect(load_parquet_as_canonical(p, layout=\"multi\", cols=cols))\n",
    "\n",
    "    # ---- wide files --------------------------------------\n",
    "    for p in wide_paths or []:\n",
    "        _collect(load_parquet_as_canonical(p, layout=\"wide\", sep=sep, cols=cols))\n",
    "\n",
    "    # ---- long files (per-file region_col) ----------------\n",
    "    if long_paths:\n",
    "        if long_region_cols is None:\n",
    "            long_region_cols = [\"region\"] * len(long_paths)\n",
    "        if len(long_region_cols) != len(long_paths):\n",
    "            raise ValueError(\n",
    "                \"long_region_cols must be the same length as long_paths\"\n",
    "            )\n",
    "        for p, reg_col in zip(long_paths, long_region_cols):\n",
    "            _collect(load_parquet_as_canonical(\n",
    "                p, layout=\"long\", region_col=reg_col, cols=cols\n",
    "            ))\n",
    "\n",
    "    # ---- subset / merge ----------------------------------\n",
    "    if regions is not None:\n",
    "        bucket = {r: bucket[r] for r in regions if r in bucket}\n",
    "\n",
    "    out: CanonicalData = {}\n",
    "    for r, frames in bucket.items():\n",
    "        df = pd.concat(frames, axis=1)\n",
    "        df = df.loc[:, ~df.columns.duplicated()]\n",
    "        df.sort_index(inplace=True)\n",
    "        out[r] = df\n",
    "\n",
    "    if regions is not None:\n",
    "        missing = set(regions) - set(out)\n",
    "        if missing:\n",
    "            raise KeyError(\n",
    "                \"The following requested regions were not found in any file: \"\n",
    "                + \", \".join(sorted(missing))\n",
    "            )\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167144bb",
   "metadata": {},
   "source": [
    "### Target Creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deb62a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Dict\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Helper 1 ── single DataFrame\n",
    "# ------------------------------------------------------------------\n",
    "def add_target_column(\n",
    "    df: pd.DataFrame,\n",
    "    minuend: str,\n",
    "    subtrahend: str,\n",
    "    *,\n",
    "    target_name: str = \"target\",\n",
    "    trim_to_valid: bool = False,\n",
    "    inplace: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add `target_name = df[minuend] - df[subtrahend]` to **one** DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    minuend, subtrahend : column names to subtract\n",
    "    target_name         : name of new column (default \\\"target\\\")\n",
    "    trim_to_valid       : if True, cut the DataFrame to the slice\n",
    "                          [first_non_NA, last_non_NA] of the new column\n",
    "    inplace             : True → mutate the original frame, False → return a copy\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame  (original or copy, according to `inplace`)\n",
    "    \"\"\"\n",
    "    if not inplace:\n",
    "        df = df.copy()\n",
    "\n",
    "    if minuend not in df.columns or subtrahend not in df.columns:\n",
    "        raise KeyError(f\\\"Missing '{minuend}' or '{subtrahend}' in columns.\\\")\n",
    "\n",
    "    df[target_name] = df[minuend] - df[subtrahend]\n",
    "\n",
    "    if trim_to_valid:\n",
    "        first = df[target_name].first_valid_index()\n",
    "        last  = df[target_name].last_valid_index()\n",
    "        if first is None or last is None:\n",
    "            raise ValueError(f\\\"`{target_name}` has no non-NA values to trim to.\\\")\n",
    "        df = df.loc[first:last]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Helper 2 ── canonical dict (ℹ️ deep-copies unless inplace=True)\n",
    "# ------------------------------------------------------------------\n",
    "CanonicalData = Dict[str, pd.DataFrame]\n",
    "\n",
    "def add_target_to_canonical(\n",
    "    data: CanonicalData,\n",
    "    minuend: str,\n",
    "    subtrahend: str,\n",
    "    *,\n",
    "    target_name: str = \"target\",\n",
    "    trim_to_valid: bool = False,\n",
    "    inplace: bool = False,\n",
    ") -> CanonicalData:\n",
    "    \"\"\"\n",
    "    Add the `target_name` column to **each** DataFrame in a canonical dict.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data              : dict[str, DataFrame]\n",
    "    minuend, subtrahend : columns to subtract\n",
    "    target_name       : new column name\n",
    "    trim_to_valid     : if True, slice every DataFrame to the first/last\n",
    "                        non-NA of the new column\n",
    "    inplace           : True → mutate the dict & frames in place\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    CanonicalData  (original or a deep copy, per `inplace`)\n",
    "    \"\"\"\n",
    "    out = data if inplace else {r: df.copy() for r, df in data.items()}\n",
    "\n",
    "    for region, df in out.items():\n",
    "        if minuend not in df.columns or subtrahend not in df.columns:\n",
    "            raise KeyError(\n",
    "                f\\\"Region '{region}' is missing '{minuend}' or '{subtrahend}'.\\\"\n",
    "            )\n",
    "\n",
    "        df[target_name] = df[minuend] - df[subtrahend]\n",
    "\n",
    "        if trim_to_valid:\n",
    "            first = df[target_name].first_valid_index()\n",
    "            last  = df[target_name].last_valid_index()\n",
    "            if first is None or last is None:\n",
    "                raise ValueError(\n",
    "                    f\\\"Region '{region}': `{target_name}` has no non-NA values to trim to.\\\"\n",
    "                )\n",
    "            out[region] = df.loc[first:last]\n",
    "\n",
    "    return out\n",
    "How to use them\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Single DataFrame example\n",
    "tokyo_df = add_target_column(\n",
    "    tokyo_df,\n",
    "    minuend=\"pri_imb_down_¥_kwh_jst_min30_a\",\n",
    "    subtrahend=\"pri_spot_jepx_¥_kwh_jst_min30_a\",\n",
    "    target_name=\"ImbalanceMinusSpot\",\n",
    "    trim_to_valid=True,     # ← crops to first/last non-NA\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# Canonical dict example\n",
    "data = add_target_to_canonical(\n",
    "    data,\n",
    "    minuend=\"demand\",\n",
    "    subtrahend=\"supply\",\n",
    "    target_name=\"net_demand\",\n",
    "    trim_to_valid=True,     # each region trimmed independently\n",
    "    inplace=False,          # get a new dict back\n",
    ")\n",
    "Both functions now guarantee that your returned frames start at the first timestamp\n",
    "where target is defined and end at the last one—handy for modelling pipelines\n",
    "that can’t handle leading/trailing NaNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea814bb",
   "metadata": {},
   "source": [
    "### calculating the number of subgraphs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9acca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from functools import lru_cache\n",
    "\n",
    "def connected_subsets_containing(G, start, available):\n",
    "    \"\"\"\n",
    "    Yield every connected subset S⊆available that includes 'start'.\n",
    "    \"\"\"\n",
    "    stack = [({start}, set(G.neighbors(start)) & available)]\n",
    "    while stack:\n",
    "        current, frontier = stack.pop()\n",
    "        yield frozenset(current)          # every prefix is connected\n",
    "        for v in list(frontier):          # expand by one frontier node\n",
    "            new_current  = current | {v}\n",
    "            new_frontier = (frontier | (set(G.neighbors(v)) & available)) - new_current\n",
    "            stack.append((new_current, new_frontier))\n",
    "\n",
    "def all_connected_partitions(G):\n",
    "    nodes = frozenset(G.nodes)\n",
    "\n",
    "    @lru_cache(None)\n",
    "    def helper(remaining):\n",
    "        if not remaining:\n",
    "            return {()}                   # empty partition\n",
    "        start = min(remaining)            # canonical choice\n",
    "        result = set()\n",
    "        for S in connected_subsets_containing(G, start, remaining):\n",
    "            rest = remaining - S\n",
    "            for tail in helper(rest):\n",
    "                result.add(tuple(sorted((S,)+tail, key=sorted)))\n",
    "        return result\n",
    "\n",
    "    return helper(nodes)\n",
    "\n",
    "# -----------------  example  -----------------\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(range(9))\n",
    "G.add_edges_from([(0,1),(1,2),(2,3),(3,4),    # a 5-node path\n",
    "                  (5,6),(6,7),(7,5),          # a triangle\n",
    "                  (4,5)])                     # a bridge 4-5\n",
    "# node 8 is isolated\n",
    "\n",
    "parts = all_connected_partitions(G)\n",
    "print(\"number of possible connectivity states:\", len(parts))\n",
    "print(\"some examples:\")\n",
    "for p in sorted(list(parts)[:10]):            # show first 10\n",
    "    print(\"  \", [sorted(block) for block in p])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf622f0f",
   "metadata": {},
   "source": [
    "### Combining regions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437f6c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Sequence\n",
    "import pandas as pd\n",
    "\n",
    "CanonicalData = Dict[str, pd.DataFrame]\n",
    "\n",
    "def combine_regions(\n",
    "    data: CanonicalData,\n",
    "    regions: Sequence[str],\n",
    "    *,\n",
    "    new_region: str | None = None,          # ← now optional\n",
    "    add: Sequence[str] | None = None,\n",
    "    average: Sequence[str] | None = None,\n",
    "    drop: Sequence[str] | None = None,\n",
    "    keep_first: Sequence[str] | None = None,\n",
    "    remove_originals: bool = False,\n",
    "    inplace: bool = False,\n",
    ") -> CanonicalData:\n",
    "    \"\"\"\n",
    "    Collapse multiple regional DataFrames into a single aggregate.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data, regions            : see earlier version\n",
    "    new_region               : dict key for the combined frame; if omitted,\n",
    "                               defaults to \\\"region1_region2_...\\\" (order-preserving)\n",
    "    add, average, drop,\n",
    "    keep_first, remove_originals, inplace\n",
    "                              : same semantics as before\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    CanonicalData\n",
    "    \"\"\"\n",
    "    if not regions:\n",
    "        raise ValueError(\"`regions` must contain at least one region name.\")\n",
    "\n",
    "    missing = [r for r in regions if r not in data]\n",
    "    if missing:\n",
    "        raise KeyError(\"Regions not found: \" + \", \".join(missing))\n",
    "\n",
    "    # auto-generate name if not supplied\n",
    "    if new_region is None:\n",
    "        new_region = \"_\".join(regions)\n",
    "\n",
    "    add      = set(add or [])\n",
    "    average  = set(average or [])\n",
    "    drop_set = set(drop or [])\n",
    "    keep1    = set(keep_first or [])\n",
    "\n",
    "    # guard against overlaps\n",
    "    overlap = (add & average) | (add & keep1) | (average & keep1)\n",
    "    overlap |= (add | average | keep1) & drop_set\n",
    "    if overlap:\n",
    "        raise ValueError(\n",
    "            \"Columns appear in multiple operation lists: \"\n",
    "            + \", \".join(sorted(overlap))\n",
    "        )\n",
    "\n",
    "    out: CanonicalData = data if inplace else {r: df.copy() for r, df in data.items()}\n",
    "\n",
    "    # build wide MultiIndex frame: level-0 = region\n",
    "    wide = pd.concat([out[r] for r in regions], axis=1, keys=regions)\n",
    "    features = wide.columns.get_level_values(1).unique()\n",
    "\n",
    "    combined_cols = {}\n",
    "    for col in features:\n",
    "        if col in drop_set:\n",
    "            continue\n",
    "        if col in add:\n",
    "            combined_cols[col] = wide.xs(col, level=1, axis=1).sum(axis=1)\n",
    "        elif col in average:\n",
    "            combined_cols[col] = wide.xs(col, level=1, axis=1).mean(axis=1)\n",
    "        elif col in keep1:\n",
    "            combined_cols[col] = out[regions[0]][col]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"No rule provided for column '{col}'. \"\n",
    "                \"Put it in add/average/keep_first/drop.\"\n",
    "            )\n",
    "\n",
    "    combined_df = pd.concat(combined_cols, axis=1).sort_index()\n",
    "\n",
    "    out[new_region] = combined_df\n",
    "\n",
    "    if remove_originals:\n",
    "        for r in regions:\n",
    "            out.pop(r, None)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46b14c5",
   "metadata": {},
   "source": [
    "### Weighted Sum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39d1076",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Sequence, Iterable\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "CanonicalData = Dict[str, pd.DataFrame]\n",
    "\n",
    "def combine_regions(\n",
    "    data: CanonicalData,\n",
    "    regions: Sequence[str],\n",
    "    *,\n",
    "    new_region: str | None = None,\n",
    "    add: Sequence[str] | None = None,\n",
    "    average: Sequence[str] | None = None,\n",
    "    drop: Sequence[str] | None = None,\n",
    "    keep_first: Sequence[str] | None = None,\n",
    "    weights: Iterable[float] | None = None,          # ← NEW\n",
    "    remove_originals: bool = False,\n",
    "    inplace: bool = False,\n",
    ") -> CanonicalData:\n",
    "    \"\"\"\n",
    "    Collapse several regions into a single aggregate DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data               : canonical dict (region → DataFrame)\n",
    "    regions            : list / tuple of region keys to combine\n",
    "    new_region         : key for combined frame; defaults to \\\"r1_r2_...\\\" if None\n",
    "    add                : columns to *sum* across regions\n",
    "    average            : columns to (weighted) *average* across regions\n",
    "    drop               : columns to discard\n",
    "    keep_first         : columns copied from the first region in `regions`\n",
    "    weights            : iterable of weights (same length as `regions`) used\n",
    "                         for columns in `average`. If None → simple mean\n",
    "    remove_originals   : delete source regions after combining\n",
    "    inplace            : modify `data` in place\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    CanonicalData (mutated or copied, depending on `inplace`)\n",
    "    \"\"\"\n",
    "    if not regions:\n",
    "        raise ValueError(\"`regions` must contain at least one region name.\")\n",
    "\n",
    "    # Validate weights --------------------------------------------------------\n",
    "    if weights is not None:\n",
    "        weights = list(weights)\n",
    "        if len(weights) != len(regions):\n",
    "            raise ValueError(\"`weights` length must match `regions` length.\")\n",
    "        w = np.array(weights, dtype=float)\n",
    "        if (w < 0).any():\n",
    "            raise ValueError(\"`weights` must be non-negative.\")\n",
    "        if w.sum() == 0:\n",
    "            raise ValueError(\"Sum of `weights` must be > 0.\")\n",
    "        w = w / w.sum()                          # normalise once\n",
    "    else:\n",
    "        w = None\n",
    "\n",
    "    # Missing-region check\n",
    "    missing = [r for r in regions if r not in data]\n",
    "    if missing:\n",
    "        raise KeyError(\"Regions not found: \" + \", \".join(missing))\n",
    "\n",
    "    # Auto name if needed\n",
    "    if new_region is None:\n",
    "        new_region = \"_\".join(regions)\n",
    "\n",
    "    # Sets for quick lookup\n",
    "    add      = set(add or [])\n",
    "    average  = set(average or [])\n",
    "    drop_set = set(drop or [])\n",
    "    keep1    = set(keep_first or [])\n",
    "\n",
    "    # Overlap guard\n",
    "    overlap = (add & average) | (add & keep1) | (average & keep1)\n",
    "    overlap |= (add | average | keep1) & drop_set\n",
    "    if overlap:\n",
    "        raise ValueError(\n",
    "            \"Columns appear in multiple operation lists: \"\n",
    "            + \", \".join(sorted(overlap))\n",
    "        )\n",
    "\n",
    "    # Work on copy unless requested in place\n",
    "    out: CanonicalData = data if inplace else {r: df.copy() for r, df in data.items()}\n",
    "\n",
    "    # Concatenate wide frame  (lvl-0 = region, lvl-1 = feature)\n",
    "    wide = pd.concat([out[r] for r in regions], axis=1, keys=regions)\n",
    "    features = wide.columns.get_level_values(1).unique()\n",
    "\n",
    "    combined_cols = {}\n",
    "    for col in features:\n",
    "        if col in drop_set:\n",
    "            continue\n",
    "\n",
    "        if col in add:\n",
    "            combined_cols[col] = wide.xs(col, level=1, axis=1).sum(axis=1)\n",
    "\n",
    "        elif col in average:\n",
    "            block = wide.xs(col, level=1, axis=1)\n",
    "            if w is None:\n",
    "                combined_cols[col] = block.mean(axis=1)\n",
    "            else:\n",
    "                combined_cols[col] = (block.values * w).sum(axis=1)\n",
    "\n",
    "        elif col in keep1:\n",
    "            combined_cols[col] = out[regions[0]][col]\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"No rule specified for column '{col}'. \"\n",
    "                \"Put it in add, average, keep_first, or drop.\"\n",
    "            )\n",
    "\n",
    "    combined_df = pd.concat(combined_cols, axis=1).sort_index()\n",
    "\n",
    "    out[new_region] = combined_df\n",
    "\n",
    "    if remove_originals:\n",
    "        for r in regions:\n",
    "            out.pop(r, None)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be0eaa6",
   "metadata": {},
   "source": [
    "### Dummy Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffa6433",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import pandas as pd\n",
    "\n",
    "CanonicalData = Dict[str, pd.DataFrame]\n",
    "\n",
    "def add_wide_area_coupling(\n",
    "    data: CanonicalData,\n",
    "    *,\n",
    "    category_col: str = \"wide_area_category\",\n",
    "    prefix: str = \"is_same_wide_area_\",\n",
    "    inplace: bool = False,\n",
    ") -> CanonicalData:\n",
    "    \"\"\"\n",
    "    For every DataFrame in *canonical* `data`, append indicator columns that\n",
    "    flag whether its `category_col` value equals the corresponding value of\n",
    "    each other region **row-by-row**.\n",
    "\n",
    "    New columns are named  f\\\"{prefix}{other_region}\\\".\n",
    "    The self column (e.g. \\\"is_same_wide_area_tokyo\\\" in Tokyo’s frame) is\n",
    "    constant 1, giving you the requested “base region” flag.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data         : dict[region, DataFrame]\n",
    "    category_col : column containing the wide-area ID in *each* frame\n",
    "    prefix       : column-name prefix for the indicators\n",
    "    inplace      : if True mutate `data`; otherwise return a deep copy\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    CanonicalData  (same object or a deep copy, according to `inplace`)\n",
    "    \"\"\"\n",
    "    # ----------------------------------------------------------------- checks\n",
    "    missing = [r for r, df in data.items() if category_col not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(\n",
    "            f\"Column '{category_col}' not found in: \" + \", \".join(missing)\n",
    "        )\n",
    "\n",
    "    out = data if inplace else {r: df.copy() for r, df in data.items()}\n",
    "\n",
    "    # ----------------------------------------------------------------- gather\n",
    "    # Build a wide frame: columns = region, index = union of all timestamps\n",
    "    cat_df = pd.concat(\n",
    "        {r: df[category_col] for r, df in out.items()}, axis=1\n",
    "    )  # outer join → NaNs where region missing\n",
    "\n",
    "    # ----------------------------------------------------------------- expand indicators\n",
    "    for region in out:\n",
    "        # Equality test row-wise (NaNs never match)\n",
    "        same = cat_df.eq(cat_df[region], axis=0).astype(int)\n",
    "\n",
    "        # Rename columns, keep *all* regions incl. self\n",
    "        same = same.rename(columns=lambda r: f\"{prefix}{r}\")\n",
    "\n",
    "        # Attach to the region’s DataFrame (align on index)\n",
    "        out[region] = pd.concat([out[region], same], axis=1)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a16041",
   "metadata": {},
   "source": [
    "### EMWA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420d7b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict\n",
    "\n",
    "CanonicalData = Dict[str, pd.DataFrame]\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1)  Single-DataFrame EWMA scaler\n",
    "# ---------------------------------------------------------------------------\n",
    "def scale_df_ewm(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    halflife: int = 1_000,\n",
    "    burnin_steps: int = 1_000,\n",
    "    remove_na: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    EWMA-standardise the numeric columns of *one* DataFrame.\n",
    "\n",
    "    Steps\n",
    "    -----\n",
    "    1. Identify numeric vs. non-numeric cols.\n",
    "    2. Compute EWMA mean & std for numeric cols.\n",
    "    3. Mask the first `burnin_steps` rows, then bfill.\n",
    "    4. Scale: (value - mean) / std.\n",
    "    5. Re-attach categorical cols, cast everything to float.\n",
    "    6. Optionally drop any remaining NaNs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    halflife     : EWMA halflife in rows\n",
    "    burnin_steps : number of initial rows to mask (so they aren’t scaled)\n",
    "    remove_na    : if True, drop rows containing NaNs after scaling\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Scaled DataFrame (all float dtype)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Split cols\n",
    "    num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
    "    cat_cols = [c for c in df.columns if c not in num_cols]\n",
    "\n",
    "    if num_cols:\n",
    "        # EWMA mean / std\n",
    "        means = (\n",
    "            df[num_cols]\n",
    "            .ewm(halflife=halflife, adjust=False)\n",
    "            .mean()\n",
    "        )\n",
    "        stds = (\n",
    "            df[num_cols]\n",
    "            .ewm(halflife=halflife, adjust=False)\n",
    "            .std()\n",
    "            + 1e-8\n",
    "        )\n",
    "\n",
    "        # Burn-in mask\n",
    "        means.iloc[:burnin_steps, :] = np.nan\n",
    "        stds.iloc[:burnin_steps, :] = np.nan\n",
    "\n",
    "        # Backfill masked rows\n",
    "        means = means.bfill()\n",
    "        stds = stds.bfill()\n",
    "\n",
    "        scaled_num = (df[num_cols] - means) / stds\n",
    "    else:\n",
    "        scaled_num = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # Reassemble, preserve original column order\n",
    "    scaled = pd.concat([scaled_num, df[cat_cols]], axis=1)[df.columns]\n",
    "\n",
    "    scaled = scaled.astype(float)\n",
    "\n",
    "    if remove_na:\n",
    "        scaled = scaled.dropna()\n",
    "\n",
    "    return scaled\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2)  Canonical-dict EWMA scaler\n",
    "# ---------------------------------------------------------------------------\n",
    "def scale_canonical_ewm(\n",
    "    data: CanonicalData,\n",
    "    *,\n",
    "    halflife: int = 1_000,\n",
    "    burnin_steps: int = 1_000,\n",
    "    remove_na: bool = True,\n",
    "    inplace: bool = False,\n",
    ") -> CanonicalData:\n",
    "    \"\"\"\n",
    "    Apply `scale_df_ewm` to **every** DataFrame in a canonical dict.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    halflife, burnin_steps, remove_na : forwarded to `scale_df_ewm`\n",
    "    inplace                           : if False, deep-copies dict & frames\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    CanonicalData  (mutated or copied, per `inplace`)\n",
    "    \"\"\"\n",
    "    out = data if inplace else {r: df.copy() for r, df in data.items()}\n",
    "\n",
    "    for region, df in out.items():\n",
    "        out[region] = scale_df_ewm(\n",
    "            df,\n",
    "            halflife=halflife,\n",
    "            burnin_steps=burnin_steps,\n",
    "            remove_na=remove_na,\n",
    "        )\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f1d3b1",
   "metadata": {},
   "source": [
    "### Expanding Mean and std deviation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab911f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict\n",
    "\n",
    "CanonicalData = Dict[str, pd.DataFrame]\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1)  Single-DataFrame EXPANDING scaler\n",
    "# ---------------------------------------------------------------------------\n",
    "def scale_df_expanding(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    burnin_steps: int = 1_000,\n",
    "    remove_na: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Standardise numeric columns using their *expanding* mean/std.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    burnin_steps : rows to mask (then back-fill) before scaling\n",
    "    remove_na    : drop remaining NaNs after scaling\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame  (all columns float)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
    "    cat_cols = [c for c in df.columns if c not in num_cols]\n",
    "\n",
    "    if num_cols:\n",
    "        means = (\n",
    "            df[num_cols]\n",
    "            .expanding(min_periods=1)\n",
    "            .mean()\n",
    "        )\n",
    "        stds = (\n",
    "            df[num_cols]\n",
    "            .expanding(min_periods=2)        # std needs ≥2 points\n",
    "            .std()\n",
    "            + 1e-8\n",
    "        )\n",
    "\n",
    "        # mask early window\n",
    "        means.iloc[:burnin_steps, :] = np.nan\n",
    "        stds.iloc[:burnin_steps, :] = np.nan\n",
    "        means = means.bfill()\n",
    "        stds = stds.bfill()\n",
    "\n",
    "        scaled_num = (df[num_cols] - means) / stds\n",
    "    else:\n",
    "        scaled_num = pd.DataFrame(index=df.index)\n",
    "\n",
    "    scaled = pd.concat([scaled_num, df[cat_cols]], axis=1)[df.columns]\n",
    "    scaled = scaled.astype(float)\n",
    "\n",
    "    if remove_na:\n",
    "        scaled = scaled.dropna()\n",
    "\n",
    "    return scaled\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2)  Canonical-dict EXPANDING scaler\n",
    "# ---------------------------------------------------------------------------\n",
    "def scale_canonical_expanding(\n",
    "    data: CanonicalData,\n",
    "    *,\n",
    "    burnin_steps: int = 1_000,\n",
    "    remove_na: bool = True,\n",
    "    inplace: bool = False,\n",
    ") -> CanonicalData:\n",
    "    \"\"\"\n",
    "    Apply `scale_df_expanding` to every DataFrame in a canonical dict.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    burnin_steps, remove_na : forwarded to single-frame helper\n",
    "    inplace                 : mutate original dict & frames\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    CanonicalData\n",
    "    \"\"\"\n",
    "    out = data if inplace else {r: df.copy() for r, df in data.items()}\n",
    "\n",
    "    for region, df in out.items():\n",
    "        out[region] = scale_df_expanding(\n",
    "            df,\n",
    "            burnin_steps=burnin_steps,\n",
    "            remove_na=remove_na,\n",
    "        )\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4490309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Sequence\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Common alias\n",
    "# ---------------------------------------------------------------------------\n",
    "CanonicalData = Dict[str, pd.DataFrame]\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Utility used by both scalers\n",
    "# ---------------------------------------------------------------------------\n",
    "def _prep_cols(df: pd.DataFrame, target: Sequence[str] | str | None):\n",
    "    \"\"\"\n",
    "    Split columns into:\n",
    "        numeric_not_target, categorical, target_cols\n",
    "    Returns each list plus the original column order.\n",
    "    \"\"\"\n",
    "    if target is None:\n",
    "        target = []\n",
    "    elif isinstance(target, str):\n",
    "        target = [target]\n",
    "\n",
    "    missing = [c for c in target if c not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(\"Target column(s) not found: \" + \", \".join(missing))\n",
    "\n",
    "    target_set = set(target)\n",
    "\n",
    "    numeric_cols  = [\n",
    "        c for c in df.columns\n",
    "        if pd.api.types.is_numeric_dtype(df[c]) and c not in target_set\n",
    "    ]\n",
    "    cat_cols      = [c for c in df.columns if c not in numeric_cols and c not in target_set]\n",
    "    target_cols   = list(target)\n",
    "\n",
    "    return numeric_cols, cat_cols, target_cols, list(df.columns)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1)  SINGLE-DATAFRAME  ▸  EWMA\n",
    "# ---------------------------------------------------------------------------\n",
    "def scale_df_ewm(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    halflife: int = 1_000,\n",
    "    burnin_steps: int = 1_000,\n",
    "    remove_na: bool = True,\n",
    "    target: Sequence[str] | str | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    EWMA-standardise numeric columns, keep `target` column(s) unchanged.\n",
    "\n",
    "    All columns are cast to float on return.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    num_cols, cat_cols, tar_cols, orig_order = _prep_cols(df, target)\n",
    "\n",
    "    # ---------- scale numeric columns\n",
    "    if num_cols:\n",
    "        means = df[num_cols].ewm(halflife=halflife, adjust=False).mean()\n",
    "        stds  = df[num_cols].ewm(halflife=halflife, adjust=False).std() + 1e-8\n",
    "\n",
    "        means.iloc[:burnin_steps, :] = np.nan\n",
    "        stds.iloc[:burnin_steps, :]  = np.nan\n",
    "        means, stds = means.bfill(), stds.bfill()\n",
    "\n",
    "        scaled_num = (df[num_cols] - means) / stds\n",
    "    else:\n",
    "        scaled_num = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # ---------- reassemble & cast\n",
    "    parts = [scaled_num, df[cat_cols], df[tar_cols]]\n",
    "    out   = pd.concat(parts, axis=1)[orig_order].astype(float)\n",
    "\n",
    "    if remove_na:\n",
    "        out = out.dropna()\n",
    "\n",
    "    return out\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2)  CANONICAL DICT  ▸  EWMA\n",
    "# ---------------------------------------------------------------------------\n",
    "def scale_canonical_ewm(\n",
    "    data: CanonicalData,\n",
    "    *,\n",
    "    halflife: int = 1_000,\n",
    "    burnin_steps: int = 1_000,\n",
    "    remove_na: bool = True,\n",
    "    target: Sequence[str] | str | None = None,\n",
    "    inplace: bool = False,\n",
    ") -> CanonicalData:\n",
    "    \"\"\"\n",
    "    Apply `scale_df_ewm` to every DataFrame in a canonical dict.\n",
    "    \"\"\"\n",
    "    out = data if inplace else {r: df.copy() for r, df in data.items()}\n",
    "\n",
    "    for r in out:\n",
    "        out[r] = scale_df_ewm(\n",
    "            out[r],\n",
    "            halflife=halflife,\n",
    "            burnin_steps=burnin_steps,\n",
    "            remove_na=remove_na,\n",
    "            target=target,\n",
    "        )\n",
    "    return out\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3)  SINGLE-DATAFRAME  ▸  EXPANDING\n",
    "# ---------------------------------------------------------------------------\n",
    "def scale_df_expanding(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    burnin_steps: int = 1_000,\n",
    "    remove_na: bool = True,\n",
    "    target: Sequence[str] | str | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Expanding-window standardisation (cumulative mean/std) with `target`\n",
    "    column(s) left untouched.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    num_cols, cat_cols, tar_cols, orig_order = _prep_cols(df, target)\n",
    "\n",
    "    if num_cols:\n",
    "        means = df[num_cols].expanding(min_periods=1).mean()\n",
    "        stds  = df[num_cols].expanding(min_periods=2).std() + 1e-8\n",
    "\n",
    "        means.iloc[:burnin_steps, :] = np.nan\n",
    "        stds.iloc[:burnin_steps, :]  = np.nan\n",
    "        means, stds = means.bfill(), stds.bfill()\n",
    "\n",
    "        scaled_num = (df[num_cols] - means) / stds\n",
    "    else:\n",
    "        scaled_num = pd.DataFrame(index=df.index)\n",
    "\n",
    "    out = pd.concat([scaled_num, df[cat_cols], df[tar_cols]], axis=1)[orig_order].astype(float)\n",
    "\n",
    "    if remove_na:\n",
    "        out = out.dropna()\n",
    "\n",
    "    return out\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 4)  CANONICAL DICT  ▸  EXPANDING\n",
    "# ---------------------------------------------------------------------------\n",
    "def scale_canonical_expanding(\n",
    "    data: CanonicalData,\n",
    "    *,\n",
    "    burnin_steps: int = 1_000,\n",
    "    remove_na: bool = True,\n",
    "    target: Sequence[str] | str | None = None,\n",
    "    inplace: bool = False,\n",
    ") -> CanonicalData:\n",
    "    \"\"\"\n",
    "    Apply `scale_df_expanding` to every DataFrame in a canonical dict.\n",
    "    \"\"\"\n",
    "    out = data if inplace else {r: df.copy() for r, df in data.items()}\n",
    "\n",
    "    for r in out:\n",
    "        out[r] = scale_df_expanding(\n",
    "            out[r],\n",
    "            burnin_steps=burnin_steps,\n",
    "            remove_na=remove_na,\n",
    "            target=target,\n",
    "        )\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be077570",
   "metadata": {},
   "source": [
    "### Plotting function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73332155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "\n",
    "\n",
    "def animated_lowess_trend(\n",
    "    df,\n",
    "    x_col: str,\n",
    "    y_col: str,\n",
    "    period: str = \"W\",          # \"D\", \"W\", \"M\", \"A\" (year) … any Pandas offset alias\n",
    "    lowess_frac: float = 0.3,   # smoothing span for LOWESS (0-1)\n",
    "    nbins_hist: int = 40,       # #bins for the marginal histogram\n",
    "    show_scatter: bool = True,  # plot raw points?\n",
    "    scatter_opacity: float = 0.2,\n",
    "    hist_height: float = 0.22,  # fraction of fig height reserved for histogram\n",
    "):\n",
    "    \"\"\"\n",
    "    Animated LOWESS trend of ``y_col`` vs ``x_col`` over successive time periods.\n",
    "\n",
    "    * df must have a DatetimeIndex\n",
    "    * x_col and y_col must be numeric\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # Prep: label rows by time period\n",
    "    # ------------------------------------------------------------------ #\n",
    "    df2 = df.copy()\n",
    "    df2[\"period\"] = df2.index.to_series().dt.to_period(period).astype(str)\n",
    "    periods = sorted(df2[\"period\"].unique())\n",
    "\n",
    "    # Axis ranges (so they stay fixed across frames)\n",
    "    x_range = (df2[x_col].min(), df2[x_col].max())\n",
    "    y_range = (df2[y_col].min(), df2[y_col].max())\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # Build frames\n",
    "    # ------------------------------------------------------------------ #\n",
    "    frames = []\n",
    "    for p in periods:\n",
    "        frame_df = df2[df2[\"period\"] == p]\n",
    "\n",
    "        # LOWESS smoothing – returns sorted (x, ŷ)\n",
    "        smooth_xy = lowess(\n",
    "            frame_df[y_col].values,\n",
    "            frame_df[x_col].values,\n",
    "            frac=lowess_frac,\n",
    "            return_sorted=True,\n",
    "        )\n",
    "        x_smooth, y_smooth = smooth_xy[:, 0], smooth_xy[:, 1]\n",
    "\n",
    "        # -- trend line\n",
    "        trend_trace = go.Scatter(\n",
    "            x=x_smooth,\n",
    "            y=y_smooth,\n",
    "            mode=\"lines\",\n",
    "            line=dict(width=2),\n",
    "            name=\"LOWESS trend\",\n",
    "            showlegend=(p == periods[0]),\n",
    "        )\n",
    "\n",
    "        # -- optional raw points (low opacity)\n",
    "        scatter_trace = go.Scatter(\n",
    "            x=frame_df[x_col],\n",
    "            y=frame_df[y_col],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=4, opacity=scatter_opacity),\n",
    "            name=\"points\",\n",
    "            showlegend=False,\n",
    "            visible=show_scatter,\n",
    "        )\n",
    "\n",
    "        # -- slim histogram of x\n",
    "        hist_trace = go.Histogram(\n",
    "            x=frame_df[x_col],\n",
    "            nbinsx=nbins_hist,\n",
    "            marker=dict(opacity=0.6),\n",
    "            showlegend=False,\n",
    "        )\n",
    "\n",
    "        frames.append(go.Frame(data=[trend_trace, scatter_trace, hist_trace], name=p))\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # Initial traces (first period)\n",
    "    # ------------------------------------------------------------------ #\n",
    "    init_trend, init_scatter, init_hist = frames[0].data\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # Slider\n",
    "    # ------------------------------------------------------------------ #\n",
    "    slider_steps = [\n",
    "        dict(\n",
    "            method=\"animate\",\n",
    "            args=[[p], {\"frame\": {\"duration\": 450, \"redraw\": False}, \"mode\": \"immediate\"}],\n",
    "            label=p,\n",
    "        )\n",
    "        for p in periods\n",
    "    ]\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # Figure & subplots (2 rows: main + histogram)\n",
    "    # ------------------------------------------------------------------ #\n",
    "    fig = make_subplots(\n",
    "        rows=2,\n",
    "        cols=1,\n",
    "        shared_xaxes=True,\n",
    "        vertical_spacing=0.03,\n",
    "        row_heights=[1 - hist_height, hist_height],\n",
    "    )\n",
    "\n",
    "    # Add initial traces\n",
    "    fig.add_trace(init_trend, row=1, col=1)\n",
    "    fig.add_trace(init_scatter, row=1, col=1)\n",
    "    fig.add_trace(init_hist, row=2, col=1)\n",
    "\n",
    "    # Axes labels / ranges\n",
    "    fig.update_yaxes(title_text=y_col, range=y_range, row=1, col=1)\n",
    "    fig.update_yaxes(showticklabels=False, row=2, col=1)\n",
    "    fig.update_xaxes(title_text=x_col, range=x_range, row=2, col=1)\n",
    "\n",
    "    # Layout with slider & play/pause buttons\n",
    "    fig.update_layout(\n",
    "        height=520,\n",
    "        title=f\"{y_col} vs {x_col} — LOWESS Trend, animated by {period}\",\n",
    "        sliders=[\n",
    "            dict(\n",
    "                active=0,\n",
    "                pad={\"t\": 55},\n",
    "                steps=slider_steps,\n",
    "                currentvalue={\"prefix\": \"Period: \"},\n",
    "            )\n",
    "        ],\n",
    "        updatemenus=[\n",
    "            {\n",
    "                \"type\": \"buttons\",\n",
    "                \"buttons\": [\n",
    "                    {\n",
    "                        \"label\": \"Play\",\n",
    "                        \"method\": \"animate\",\n",
    "                        \"args\": [\n",
    "                            None,\n",
    "                            {\"frame\": {\"duration\": 450, \"redraw\": False}, \"fromcurrent\": True},\n",
    "                        ],\n",
    "                    },\n",
    "                    {\n",
    "                        \"label\": \"Pause\",\n",
    "                        \"method\": \"animate\",\n",
    "                        \"args\": [[None], {\"frame\": {\"duration\": 0, \"redraw\": False}, \"mode\": \"immediate\"}],\n",
    "                    },\n",
    "                ],\n",
    "                \"direction\": \"left\",\n",
    "                \"pad\": {\"r\": 10, \"t\": 10},\n",
    "                \"showactive\": False,\n",
    "                \"x\": 0.1,\n",
    "                \"y\": 1.17,\n",
    "                \"xanchor\": \"right\",\n",
    "            }\n",
    "        ],\n",
    "        bargap=0.07,\n",
    "        frames=frames,\n",
    "    )\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d7c7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = your DataFrame; assume its index is a DatetimeIndex\n",
    "animated_lowess_trend(\n",
    "    df,\n",
    "    x_col=\"feature_x\",\n",
    "    y_col=\"target_y\",\n",
    "    period=\"M\",          # switch to \"D\", \"W\", \"A\", etc. as you like\n",
    "    lowess_frac=0.25,    # narrower span → wigglier curve\n",
    "    show_scatter=True,   # set False if you only want the smooth line\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
