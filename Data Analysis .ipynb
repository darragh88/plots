{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c08c070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def build_feature_dataset(\n",
    "    input_paths: list[str],\n",
    "    output_path: str,\n",
    "    region: str,\n",
    "    cols: list[str],\n",
    "    freq: str = \"30T\",\n",
    "    plot: bool = True\n",
    ") -> tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    Loading Features \n",
    "    Args:\n",
    "        input_paths:    List of Parquet file paths. Each file must load into a DataFrame\n",
    "                        whose columns are a MultiIndex with levels [region, variable_name].\n",
    "        output_path:    File‐path (including filename) where the feature report should be written.\n",
    "        region:         The first‐level column key (region) to subset by after concatenation.\n",
    "        cols:           A list of variable names (second‐level columns) to keep, once we subset to `region`.\n",
    "        freq:           A Pandas offset alias (e.g. \"30T\", \"15T\", \"1H\") used to resample each DataFrame.\n",
    "                        Default is \"30T\".\n",
    "        plot:           If True, calls `generate_feature_report(...)` on the final feature set.\n",
    "\n",
    "    Returns:\n",
    "        feat:   A DataFrame of shape [n_samples × n_features], containing:\n",
    "                • the time‐features (weekday, hour, month, etc.),\n",
    "                • the chosen columns in `cols`,\n",
    "                • and any newly added columns (forward‐filled) for modeling.\n",
    "        tar:    A pd.Series named \"Imbalance_Minus_Spot\", aligned with `feat.index`, \n",
    "                containing the (imbalance_price − spot_price) at each timestamp.\n",
    "    \"\"\"\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # Helpers for timezone‐normalization + resampling\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    def _load_and_resample_one(path: str, freq_rule: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Loads one Parquet file into a DataFrame with a DateTimeIndex, normalizes\n",
    "        its index to Asia/Tokyo, and resamples to `freq_rule` using .mean().\n",
    "        \"\"\"\n",
    "        df = pd.read_parquet(path)\n",
    "\n",
    "        # Ensure index is datetime:\n",
    "        df = df.copy()\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "\n",
    "        # If tz‐naive → assume it's already JST, so localize to Asia/Tokyo.\n",
    "        # If tz‐aware (e.g. UTC or anything), convert to Asia/Tokyo.\n",
    "        if df.index.tz is None:\n",
    "            df.index = df.index.tz_localize(\"Asia/Tokyo\")\n",
    "        else:\n",
    "            df.index = df.index.tz_convert(\"Asia/Tokyo\")\n",
    "\n",
    "        # Resample to the requested frequency, taking the mean of each\n",
    "        # (e.g. if `freq_rule=\"30T\"`, each 30‐minute block is averaged).\n",
    "        df_resampled = df.resample(freq_rule).mean()\n",
    "        return df_resampled\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # 1) Load + resample each input DataFrame; collect start/end times\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    loaded_dfs = []\n",
    "    start_times = []\n",
    "    end_times = []\n",
    "\n",
    "    for path in input_paths:\n",
    "        df_resampled = _load_and_resample_one(path, freq)\n",
    "        loaded_dfs.append(df_resampled)\n",
    "\n",
    "        # Record the new index range\n",
    "        start_times.append(df_resampled.index.min())\n",
    "        end_times.append(df_resampled.index.max())\n",
    "\n",
    "    if not loaded_dfs:\n",
    "        raise ValueError(\"`input_paths` must contain at least one parquet file.\")\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # 2) Find the common date‐range: [latest_start, earliest_end]\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    latest_start = max(start_times)\n",
    "    earliest_end = min(end_times)\n",
    "\n",
    "    if latest_start >= earliest_end:\n",
    "        raise ValueError(\n",
    "            f\"No overlapping time‐range found among the loaded files. \"\n",
    "            f\"latest_start={latest_start}, earliest_end={earliest_end}\"\n",
    "        )\n",
    "\n",
    "    # 3) Truncate each DataFrame to [latest_start : earliest_end]\n",
    "    aligned_dfs = [\n",
    "        df.loc[latest_start : earliest_end] for df in loaded_dfs\n",
    "    ]\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # 4) Concatenate side‐by‐side (axis=1)\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # Since each df had columns = MultiIndex [region, variable], \n",
    "    # the concatenation keeps the same MultiIndex column structure.\n",
    "    concatenated = pd.concat(aligned_dfs, axis=1)\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # 5) Subset by region (first level) and then by `cols` (second level)\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # This picks out one “slice” of the MultiIndex at level=0 == region.\n",
    "    try:\n",
    "        df_region = concatenated[region]\n",
    "    except KeyError:\n",
    "        raise KeyError(f\"Region '{region}' not found in the concatenated columns.\")\n",
    "\n",
    "    # Now df_region’s columns are the second level only. We keep exactly `cols`.\n",
    "    missing = [c for c in cols if c not in df_region.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"The following requested columns are not present for region {region}: {missing}\")\n",
    "\n",
    "    df_region = df_region[cols]\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # 6) Find the first+last index where BOTH spot & imbalance are non‐NaN\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    imb_col = \"pri_imb_down_%_kwh_jst_min30_a\"\n",
    "    spot_col = \"pri_spot_jepx_%_kwh_jst_min30_a\"\n",
    "\n",
    "    # Ensure those two are in `cols` (or else we can’t form the target)\n",
    "    if imb_col not in df_region.columns or spot_col not in df_region.columns:\n",
    "        raise KeyError(\n",
    "            f\"Cannot find both target columns ('{imb_col}' and '{spot_col}') in df_region. \"\n",
    "            f\"Got columns={list(df_region.columns)}\"\n",
    "        )\n",
    "\n",
    "    # Build a mask where both are non‐NaN:\n",
    "    both_valid = (\n",
    "        df_region[imb_col].notna() &\n",
    "        df_region[spot_col].notna()\n",
    "    )\n",
    "    # If there is no timestamp where both are valid, it's an error:\n",
    "    if not both_valid.any():\n",
    "        raise ValueError(\n",
    "            f\"No timestamp exists where both '{imb_col}' and '{spot_col}' are non‐NaN.\"\n",
    "        )\n",
    "\n",
    "    valid_times = df_region.index[both_valid]\n",
    "    crop_start = valid_times.min()\n",
    "    crop_end = valid_times.max()\n",
    "\n",
    "    # Crop the DataFrame so that the first row has both non‐NaN, and the last row has both non‐NaN\n",
    "    df_region = df_region.loc[crop_start : crop_end]\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # 7) Forward‐fill any remaining NaNs (limit=1)\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    df_region = df_region.ffill(limit=1)\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # 8) Construct time‐features\n",
    "    #    (Assumes you already have a function `construct_time_features(df)` defined elsewhere.)\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    construct_time_features(df_region)\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # 9) Create the target series: \"Imbalance_Minus_Spot\"\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    tar = df_region[imb_col] - df_region[spot_col]\n",
    "    tar.name = \"Imbalance_Minus_Spot\"\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # 10) Optionally generate a feature report\n",
    "    #    (Assumes you already have `generate_feature_report(...)` imported.)\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    if plot:\n",
    "        # name=\"Features\" is arbitrary; you can change if you like\n",
    "        generate_feature_report(\n",
    "            features=df_region,\n",
    "            target=tar,\n",
    "            document_name=output_path,\n",
    "            name=\"Features\"\n",
    "        )\n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # 11) Return the final feature‐DataFrame and the target‐Series\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    return df_region, tar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
