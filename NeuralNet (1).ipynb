{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c85a0693",
   "metadata": {},
   "source": [
    "### Sequential Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739494a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightning >=2.2\n",
    "import torch, pytorch_lightning as pl\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---------- 1.  Build an iterable Dataset that streams in time order ----------\n",
    "class RollingWindowDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    Yields (X_window, y_target) pairs in chronological order.\n",
    "    Assumes df already sorted by timestamp ascending.\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame, feature_cols, target_col,\n",
    "                 window: int = 24, horizon: int = 1):\n",
    "        self.X = df[feature_cols].values.astype(np.float32)\n",
    "        self.y = df[target_col].values.astype(np.float32)\n",
    "        self.window, self.horizon = window, horizon\n",
    "\n",
    "    def __iter__(self):\n",
    "        # leave the last `horizon` rows unused as inputs\n",
    "        for t in range(self.window, len(self.X) - self.horizon):\n",
    "            X_win = self.X[t - self.window:t]          # shape (window, n_feat)\n",
    "            y_tar = self.y[t + self.horizon - 1]       # scalar regression target\n",
    "            yield torch.from_numpy(X_win), torch.tensor(y_tar)\n",
    "\n",
    "# ---------- 2.  LightningDataModule ----------\n",
    "class TSDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, df, feature_cols, target_col, window=24, horizon=1,\n",
    "                 val_size=0.05, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        df = self.hparams.df\n",
    "        split_idx = int(len(df) * (1 - self.hparams.val_size))\n",
    "        self.train_ds = RollingWindowDataset(df.iloc[:split_idx],\n",
    "                                             self.hparams.feature_cols,\n",
    "                                             self.hparams.target_col,\n",
    "                                             self.hparams.window,\n",
    "                                             self.hparams.horizon)\n",
    "        self.val_ds   = RollingWindowDataset(df.iloc[split_idx:],\n",
    "                                             self.hparams.feature_cols,\n",
    "                                             self.hparams.target_col,\n",
    "                                             self.hparams.window,\n",
    "                                             self.hparams.horizon)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_ds,\n",
    "                          batch_size=self.hparams.batch_size,\n",
    "                          shuffle=False,  # critical!\n",
    "                          drop_last=False)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_ds,\n",
    "                          batch_size=self.hparams.batch_size,\n",
    "                          shuffle=False)\n",
    "\n",
    "# ---------- 3.  LightningModule ----------\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleTSRegressor(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Example network: flatten window → 2-layer MLP → scalar.\n",
    "    Replace with LSTM/Transformer for better TS handling.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, window, lr=1e-3, hidden=128):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        in_dim = n_features * window\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),                       # (B, window*n_feat)\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden),\n",
    "            nn.Linear(hidden, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)           # (B,)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
    "        sched = torch.optim.lr_scheduler.StepLR(opt, step_size=10, gamma=0.5)\n",
    "        return [opt], [sched]\n",
    "\n",
    "# ---------- 4.  Putting it together ----------\n",
    "def run_training(df, feature_cols, target_col):\n",
    "    window = 24          # last 24 timesteps → predict t+1\n",
    "    dm = TSDataModule(df, feature_cols, target_col, window=window,\n",
    "                      horizon=1, val_size=0.05, batch_size=64)\n",
    "    n_feat = len(feature_cols)\n",
    "    model = SimpleTSRegressor(n_features=n_feat, window=window)\n",
    "    trainer = pl.Trainer(max_epochs=50,\n",
    "                         gradient_clip_val=1.0,\n",
    "                         callbacks=[\n",
    "                             pl.callbacks.ModelCheckpoint(\n",
    "                                 monitor=\"val_loss\", save_top_k=3, mode=\"min\"\n",
    "                             )\n",
    "                         ])\n",
    "    trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f74ed6",
   "metadata": {},
   "source": [
    "Loss Logger "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f1fe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"seq_ts\")\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=50,\n",
    "                     logger=logger,              # ⬅️ add logger\n",
    "                     callbacks=[\n",
    "                         pl.callbacks.ModelCheckpoint(\n",
    "                             monitor=\"val_loss\", mode=\"min\", save_top_k=3),\n",
    "                         pl.callbacks.RichProgressBar()  # nicer bar (optional)\n",
    "                     ])\n",
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530e5871",
   "metadata": {},
   "source": [
    "### Choosing different mechanisms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7211854e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "def build_backbone(arch: str,\n",
    "                   n_features: int,\n",
    "                   window: int,\n",
    "                   hidden: int = 128,\n",
    "                   n_layers: int = 2,\n",
    "                   n_heads: int = 4):\n",
    "    \"\"\"\n",
    "    Returns a nn.Module that maps (B, window, n_features) ➜ (B, 1)\n",
    "    Supported arch: \"mlp\", \"lstm\", \"gru\", \"cnn\", \"transformer\"\n",
    "    \"\"\"\n",
    "    arch = arch.lower()\n",
    "\n",
    "    # 1) Plain MLP (baseline we used before)\n",
    "    if arch == \"mlp\":\n",
    "        in_dim = n_features * window\n",
    "        return nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden),\n",
    "            nn.Linear(hidden, 1)\n",
    "        )\n",
    "\n",
    "    # 2) LSTM (take last hidden state)\n",
    "    if arch == \"lstm\":\n",
    "        class LSTMHead(nn.Module):\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "                self.lstm = nn.LSTM(\n",
    "                    input_size=n_features,\n",
    "                    hidden_size=hidden,\n",
    "                    num_layers=n_layers,\n",
    "                    batch_first=True)\n",
    "                self.fc = nn.Linear(hidden, 1)\n",
    "\n",
    "            def forward(self, x):             # x: (B, w, n_feat)\n",
    "                out, _ = self.lstm(x)\n",
    "                return self.fc(out[:, -1])     # last time step\n",
    "        return LSTMHead()\n",
    "\n",
    "    # 3) GRU\n",
    "    if arch == \"gru\":\n",
    "        class GRUHead(nn.Module):\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "                self.gru = nn.GRU(\n",
    "                    input_size=n_features,\n",
    "                    hidden_size=hidden,\n",
    "                    num_layers=n_layers,\n",
    "                    batch_first=True)\n",
    "                self.fc = nn.Linear(hidden, 1)\n",
    "\n",
    "            def forward(self, x):\n",
    "                out, _ = self.gru(x)\n",
    "                return self.fc(out[:, -1])\n",
    "        return GRUHead()\n",
    "\n",
    "    # 4) 1-D Temporal Convolution (Causal)\n",
    "    if arch == \"cnn\":\n",
    "        class CNNHead(nn.Module):\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "                self.conv = nn.Sequential(\n",
    "                    nn.Conv1d(n_features, hidden,\n",
    "                              kernel_size=3, padding=2, dilation=2),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv1d(hidden, hidden,\n",
    "                              kernel_size=3, padding=4, dilation=4),\n",
    "                    nn.ReLU(),\n",
    "                    nn.AdaptiveAvgPool1d(1)\n",
    "                )\n",
    "                self.fc = nn.Linear(hidden, 1)\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = x.permute(0, 2, 1)        # (B, C=n_feat, L=window)\n",
    "                x = self.conv(x).squeeze(-1)   # (B, hidden)\n",
    "                return self.fc(x)\n",
    "        return CNNHead()\n",
    "\n",
    "    # 5) Transformer Encoder (positional embedding + last token)\n",
    "    if arch == \"transformer\":\n",
    "        class TransEncHead(nn.Module):\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "                self.input_proj = nn.Linear(n_features, hidden)\n",
    "                layer = nn.TransformerEncoderLayer(\n",
    "                    d_model=hidden, nhead=n_heads,\n",
    "                    batch_first=True, norm_first=True)\n",
    "                self.encoder = nn.TransformerEncoder(layer, num_layers=n_layers)\n",
    "                # learned positional embedding\n",
    "                self.pos = nn.Parameter(torch.zeros(window, hidden))\n",
    "                nn.init.uniform_(self.pos, -0.02, 0.02)\n",
    "                self.fc = nn.Linear(hidden, 1)\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = self.input_proj(x) + self.pos  # add position\n",
    "                x = self.encoder(x)\n",
    "                return self.fc(x[:, -1])\n",
    "        return TransEncHead()\n",
    "\n",
    "    raise ValueError(f\"Unknown architecture: {arch}\")\n",
    "\n",
    "\n",
    "    ### Calling Function ###\n",
    "    \n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FlexibleTSRegressor(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Same loss-switcher we used before; only backbone replaced by build_backbone.\n",
    "    \"\"\"\n",
    "    def __init__(self, *,\n",
    "                 n_features: int,\n",
    "                 window: int,\n",
    "                 backbone: str = \"mlp\",\n",
    "                 hidden: int = 128,\n",
    "                 loss_name: str = \"mse\",\n",
    "                 lr: float = 1e-3,\n",
    "                 **loss_kw):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # build chosen architecture\n",
    "        self.backbone = build_backbone(backbone,\n",
    "                                       n_features, window,\n",
    "                                       hidden=hidden)\n",
    "\n",
    "        # ----- loss selection (identical to earlier snippet) ------------\n",
    "        ln = loss_name.lower()\n",
    "        if ln == \"mse\":\n",
    "            self.criterion = nn.MSELoss()\n",
    "        elif ln == \"mae\":\n",
    "            self.criterion = nn.L1Loss()\n",
    "        elif ln == \"huber\":\n",
    "            beta = loss_kw.get(\"huber_beta\", 1.0)\n",
    "            self.criterion = nn.SmoothL1Loss(beta=beta)\n",
    "        else:\n",
    "            self.criterion = None             # rmse / quantile handled below\n",
    "        self.loss_name = ln\n",
    "        self.quantile = loss_kw.get(\"quantile\", 0.9)\n",
    "\n",
    "    # ----- forward & loss ------------------------------------------------\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x).squeeze(-1)\n",
    "\n",
    "    def _compute_loss(self, y_hat, y):\n",
    "        if self.loss_name in (\"mse\", \"mae\", \"huber\"):\n",
    "            return self.criterion(y_hat, y)\n",
    "        if self.loss_name == \"rmse\":\n",
    "            return F.mse_loss(y_hat, y).sqrt()\n",
    "        if self.loss_name == \"quantile\":\n",
    "            q = self.quantile\n",
    "            diff = y_hat - y\n",
    "            return torch.where(diff >= 0, q * diff, (q - 1) * diff).mean()\n",
    "        raise RuntimeError\n",
    "\n",
    "    # ----- training & validation ----------------------------------------\n",
    "    def training_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        loss = self._compute_loss(self(x), y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        loss = self._compute_loss(self(x), y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.AdamW(self.parameters(),\n",
    "                                lr=self.hparams.lr)\n",
    "        sch = torch.optim.lr_scheduler.StepLR(opt, 10, gamma=0.5)\n",
    "        return [opt], [sch]\n",
    "\n",
    "# keep your existing RollingWindowDataset and TSDataModule\n",
    "\n",
    "n_feat  = len(feature_cols)\n",
    "window  = 24\n",
    "\n",
    "# ➊ MLP (baseline)\n",
    "model = FlexibleTSRegressor(n_features=n_feat,\n",
    "                            window=window,\n",
    "                            backbone=\"mlp\")\n",
    "\n",
    "# ➋ LSTM\n",
    "model = FlexibleTSRegressor(n_features=n_feat,\n",
    "                            window=window,\n",
    "                            backbone=\"lstm\",\n",
    "                            hidden=256, n_layers=2)\n",
    "\n",
    "# ➌ GRU\n",
    "model = FlexibleTSRegressor(n_features=n_feat,\n",
    "                            window=window,\n",
    "                            backbone=\"gru\",\n",
    "                            hidden=256, n_layers=3)\n",
    "\n",
    "#  Temporal-CNN\n",
    "model = FlexibleTSRegressor(n_features=n_feat,\n",
    "                            window=window,\n",
    "                            backbone=\"cnn\",\n",
    "                            hidden=128)\n",
    "\n",
    "# Transformer Encoder\n",
    "model = FlexibleTSRegressor(n_features=n_feat,\n",
    "                            window=window,\n",
    "                            backbone=\"transformer\",\n",
    "                            hidden=128,\n",
    "                            n_layers=2,\n",
    "                            n_heads=4,\n",
    "                            loss_name=\"quantile\",\n",
    "                            quantile=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c709a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 1. Imports\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "import os, torch, pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 2. Rolling-window Dataset  (indexable so Lightning knows its length)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "class RollingWindowDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame,\n",
    "                 feature_cols, target_col,\n",
    "                 window: int = 24, horizon: int = 1):\n",
    "        self.X  = df[feature_cols].values.astype(\"float32\")\n",
    "        self.y  = df[target_col].values.astype(\"float32\")\n",
    "        self.w  = window\n",
    "        self.h  = horizon\n",
    "        self.indices = range(window, len(df) - horizon)\n",
    "\n",
    "    def __len__(self):               # Lightning can now draw 0/??? bars\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        t = self.indices[idx]\n",
    "        X_win = self.X[t-self.w : t]              # shape (window, n_feat)\n",
    "        y_tar = self.y[t + self.h - 1]            # scalar\n",
    "        return torch.from_numpy(X_win), torch.tensor(y_tar)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 3. DataModule (keeps order, no shuffle!)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "class TSDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, df, feature_cols, target_col,\n",
    "                 window=24, horizon=1,\n",
    "                 val_size=0.05, batch_size=64,\n",
    "                 num_workers=os.cpu_count()//2):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=[\"df\"])\n",
    "        self.df = df.copy()\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        split = int(len(self.df) * (1 - self.hparams.val_size))\n",
    "        train_df = self.df.iloc[:split]\n",
    "        val_df   = self.df.iloc[split:]\n",
    "\n",
    "        self.train_ds = RollingWindowDataset(train_df,\n",
    "                                             self.hparams.feature_cols,\n",
    "                                             self.hparams.target_col,\n",
    "                                             self.hparams.window,\n",
    "                                             self.hparams.horizon)\n",
    "        self.val_ds   = RollingWindowDataset(val_df,\n",
    "                                             self.hparams.feature_cols,\n",
    "                                             self.hparams.target_col,\n",
    "                                             self.hparams.window,\n",
    "                                             self.hparams.horizon)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_ds,\n",
    "                          batch_size=self.hparams.batch_size,\n",
    "                          shuffle=False,\n",
    "                          drop_last=False,\n",
    "                          num_workers=self.hparams.num_workers,\n",
    "                          persistent_workers=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_ds,\n",
    "                          batch_size=self.hparams.batch_size,\n",
    "                          shuffle=False,\n",
    "                          num_workers=self.hparams.num_workers,\n",
    "                          persistent_workers=True)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 4. Simple MLP regressor (swap with LSTM/GRU later if you like)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "class SimpleTSRegressor(pl.LightningModule):\n",
    "    def __init__(self, n_features, window, hidden=128, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(window * n_features, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden),\n",
    "            nn.Linear(hidden, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "    def training_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        loss = F.mse_loss(self(x), y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        loss = F.mse_loss(self(x), y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt  = torch.optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
    "        sched = torch.optim.lr_scheduler.StepLR(opt, 10, gamma=0.5)\n",
    "        return [opt], [sched]\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 5. One-shot training + return best checkpoint path\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def train_and_get_best_ckpt(df, feature_cols, target_col,\n",
    "                            window=24, horizon=1):\n",
    "    dm = TSDataModule(df, feature_cols, target_col,\n",
    "                      window=window, horizon=horizon)\n",
    "\n",
    "    model = SimpleTSRegressor(n_features=len(feature_cols), window=window)\n",
    "\n",
    "    ckpt_cb = pl.callbacks.ModelCheckpoint(\n",
    "        monitor=\"val_loss\", mode=\"min\", save_top_k=1)\n",
    "\n",
    "    logger = TensorBoardLogger(\"lightning_logs\", name=\"seq_ts\")\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=50,\n",
    "        accelerator=\"auto\",\n",
    "        devices=\"auto\",\n",
    "        logger=logger,\n",
    "        callbacks=[ckpt_cb, pl.callbacks.RichProgressBar()])\n",
    "    trainer.fit(model, dm)\n",
    "\n",
    "    return ckpt_cb.best_model_path\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 6. ==== RUN TRAINING =========================================================\n",
    "# df, feature_cols, target_col must already exist in your interpreter\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "ckpt_path = train_and_get_best_ckpt(df, feature_cols, target_col)\n",
    "print(\"Best model checkpoint ➜\", ckpt_path)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 7. Load best model & make predictions on (for example) the validation slice\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "best_model = SimpleTSRegressor.load_from_checkpoint(\n",
    "                 ckpt_path,\n",
    "                 n_features=len(feature_cols),\n",
    "                 window=24).eval()\n",
    "\n",
    "val_ds = RollingWindowDataset(df.iloc[int(len(df)*0.95):],\n",
    "                              feature_cols, target_col,\n",
    "                              window=24, horizon=1)\n",
    "val_dl = DataLoader(val_ds, batch_size=64, shuffle=False)\n",
    "\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for x, _ in val_dl:\n",
    "        preds.append(best_model(x).cpu())\n",
    "preds = torch.cat(preds).numpy()\n",
    "print(\"Inference done. 1st five predictions:\", preds[:5])\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 8. Launch TensorBoard (run this **in a terminal**, not inside Python)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "#   tensorboard --logdir lightning_logs\n",
    "# Then open the printed URL in your browser to see train/val loss curves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5147a2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard --logdir lightning_logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
