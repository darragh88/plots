{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30d8a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "gnn_experiment_refactored.py\n",
    "—————————————————————————————————\n",
    "End-to-end GNN experimentation wrapper for **binary rare-event\n",
    "classification** on temporal node snapshots.\n",
    "\n",
    "Key additions vs. original version\n",
    "• Per-node positive-class weights built into BCE loss\n",
    "• Edge-dropout (train-time only)\n",
    "• Configurable LR scheduler with LR print at each epoch\n",
    "\"\"\"\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 0. Imports\n",
    "# ----------------------------------------------------------------------\n",
    "import json\n",
    "import pathlib\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import Dict, Any, Optional, Union, List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import (\n",
    "    GCNConv,\n",
    "    GATv2Conv,\n",
    "    GCN,\n",
    "    GraphSAGE,\n",
    "    GAT,\n",
    ")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1. Configuration object\n",
    "# ----------------------------------------------------------------------\n",
    "@dataclass\n",
    "class GNNConfig:\n",
    "    # --- model ---------------------------------------------------------\n",
    "    task: str = \"node_clf\"            # {\"node_clf\", \"edge_clf\"}\n",
    "    model_name: str = \"gcn\"           # {\"gcn\",\"graphsage\",\"gat\",\"gatv2\"}\n",
    "    num_layers: int = 2\n",
    "    hidden_dim: int = 128\n",
    "    heads: int = 8                    # for attention models\n",
    "    norm: str = \"batch\"               # {\"batch\",\"layer\",None}\n",
    "\n",
    "    # --- data & label --------------------------------------------------\n",
    "    target_col: str = \"target\"\n",
    "\n",
    "    # --- split ---------------------------------------------------------\n",
    "    split_mode: str = \"date\"          # {\"date\",\"ratio\"}\n",
    "    cutoff_date: Optional[str] = None\n",
    "    val_ratio: float = 0.10\n",
    "    test_ratio: float = 0.10\n",
    "    shuffle_in_split: bool = False\n",
    "\n",
    "    # --- optimisation --------------------------------------------------\n",
    "    batch_size: int = 32\n",
    "    lr: float = 1e-3\n",
    "    epochs: int = 100\n",
    "    patience: int = 10\n",
    "    grad_clip: float = 1.0\n",
    "\n",
    "    optimiser: str = \"adam\"           # {\"adam\",\"adamw\",\"sgd\"}\n",
    "    optimiser_kwargs: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "    # --- LR scheduler --------------------------------------------------\n",
    "    lr_scheduler: Optional[str] = None         # {\"step\",\"plateau\",\"cosine\",\"onecycle\"}\n",
    "    lr_scheduler_kwargs: Dict[str, Any] = field(default_factory=dict)\n",
    "    print_lr_each_epoch: bool = True\n",
    "\n",
    "    # --- loss / weights -----------------------------------------------\n",
    "    loss_fn: str = \"bce\"              # {\"bce\",\"cross_entropy\"}\n",
    "    class_weights: Optional[List[float]] = None   # global BCE/CE weights\n",
    "    node_pos_weights: Optional[List[float]] = None  # per-node list (len == #nodes)\n",
    "\n",
    "    # --- regularisation -----------------------------------------------\n",
    "    in_dropout: float = 0.0           # feature-level dropout\n",
    "    layer_dropout: float = 0.5        # model internal dropout\n",
    "    edge_dropout: float = 0.0         # probability of dropping each edge at train-time\n",
    "\n",
    "    # --- misc ----------------------------------------------------------\n",
    "    device: str = \"cuda\"\n",
    "    run_name: str = \"default_run\"\n",
    "    seed: int = 42\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    @staticmethod\n",
    "    def load(cfg: Union[\"GNNConfig\", str, Dict[str, Any]]) -> \"GNNConfig\":\n",
    "        if isinstance(cfg, GNNConfig):\n",
    "            return cfg\n",
    "        if isinstance(cfg, dict):\n",
    "            return GNNConfig(**cfg)\n",
    "        if isinstance(cfg, (str, pathlib.Path)):\n",
    "            path = pathlib.Path(cfg)\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                if path.suffix == \".json\":\n",
    "                    data = json.load(f)\n",
    "                else:\n",
    "                    raise ValueError(\"Unsupported file type for config path\")\n",
    "            return GNNConfig(**data)\n",
    "        raise TypeError(f\"Unsupported cfg type: {type(cfg)}\")\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return asdict(self)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2. Utilities\n",
    "# ----------------------------------------------------------------------\n",
    "def has_nan(t: torch.Tensor) -> bool:\n",
    "    return torch.isnan(t).any() or torch.isinf(t).any()\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3. Mini model zoo\n",
    "# ----------------------------------------------------------------------\n",
    "class GNNNodeSimple(torch.nn.Module):\n",
    "    def __init__(self, d_in: int, hidden: int, d_out: int, p_drop: float):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(d_in, hidden)\n",
    "        self.conv2 = GCNConv(hidden, hidden)\n",
    "        self.dropout = torch.nn.Dropout(p_drop)\n",
    "        self.fc = torch.nn.Linear(hidden, d_out)\n",
    "\n",
    "    def forward(self, data: Data):\n",
    "        x, ei = data.x, data.edge_index\n",
    "        x = torch.relu(self.conv1(x, ei))\n",
    "        x = torch.relu(self.conv2(x, ei))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class GNNGCN(torch.nn.Module):\n",
    "    def __init__(self, d_in: int, hidden: int, d_out: int, layers: int, p_drop: float):\n",
    "        super().__init__()\n",
    "        self.body = GCN(d_in, hidden, layers, norm=\"batch\", act=\"relu\")\n",
    "        self.dropout = torch.nn.Dropout(p_drop)\n",
    "        self.fc = torch.nn.Linear(hidden, d_out)\n",
    "\n",
    "    def forward(self, data: Data):\n",
    "        x = self.body(data.x, data.edge_index)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class GNNSage(torch.nn.Module):\n",
    "    def __init__(self, d_in: int, hidden: int, d_out: int, layers: int, p_drop: float):\n",
    "        super().__init__()\n",
    "        self.body = GraphSAGE(d_in, hidden, layers, norm=\"batch\", act=\"relu\")\n",
    "        self.dropout = torch.nn.Dropout(p_drop)\n",
    "        self.fc = torch.nn.Linear(hidden, d_out)\n",
    "\n",
    "    def forward(self, data: Data):\n",
    "        x = self.body(data.x, data.edge_index)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class GNNGAT(torch.nn.Module):\n",
    "    def __init__(self, d_in: int, hidden: int, d_out: int, layers: int, heads: int, p_drop: float):\n",
    "        super().__init__()\n",
    "        self.body = GAT(d_in, hidden, layers, heads=heads, norm=\"batch\", act=\"relu\")\n",
    "        self.dropout = torch.nn.Dropout(p_drop)\n",
    "        self.fc = torch.nn.Linear(hidden, d_out)\n",
    "\n",
    "    def forward(self, data: Data):\n",
    "        x = self.body(data.x, data.edge_index)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class GNNGAT2(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_in: int,\n",
    "        hidden: int,\n",
    "        d_out: int,\n",
    "        layers: int,\n",
    "        heads: int,\n",
    "        edge_dim: int,\n",
    "        p_drop: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATv2Conv(d_in, hidden, heads=heads, concat=True, edge_dim=edge_dim)\n",
    "        self.conv2 = GATv2Conv(hidden * heads, hidden, heads=1, concat=False, edge_dim=edge_dim)\n",
    "        self.dropout = torch.nn.Dropout(p_drop)\n",
    "        self.fc = torch.nn.Linear(hidden, d_out)\n",
    "\n",
    "    def forward(self, data: Data):\n",
    "        x = torch.relu(self.conv1(data.x, data.edge_index, data.edge_attr))\n",
    "        x = torch.relu(self.conv2(x, data.edge_index, data.edge_attr))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 4. Experiment wrapper\n",
    "# ----------------------------------------------------------------------\n",
    "class GNNExperiment:\n",
    "    \"\"\"\n",
    "    Raw node/edge time-series → PyG snapshots → DataLoaders → Model.\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------------------ constructor\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_frames: Dict[str, pd.DataFrame],\n",
    "        edge_frames: Dict[str, pd.DataFrame],\n",
    "        graph: nx.DiGraph,\n",
    "        cfg: Union[GNNConfig, str, Dict[str, Any]] = GNNConfig(),\n",
    "    ):\n",
    "        # raw inputs\n",
    "        self.node_frames = node_frames\n",
    "        self.edge_frames = edge_frames\n",
    "        self.graph = graph\n",
    "\n",
    "        # configuration\n",
    "        self.cfg = GNNConfig.load(cfg)\n",
    "\n",
    "        # runtime placeholders\n",
    "        self.reg_order: List[str] = []\n",
    "        self.edge_order: List[Tuple[int, int]] = []\n",
    "        self.snapshots: Optional[List[Data]] = None\n",
    "\n",
    "        self.train_dl = self.val_dl = self.test_dl = None\n",
    "\n",
    "        self.model = None\n",
    "        self.optimizer = None\n",
    "        self.scheduler = None\n",
    "        self.loss_fn = None\n",
    "        self.metric_fn = None\n",
    "\n",
    "        self.history: Dict[str, list] = {\"train_loss\": [], \"val_loss\": [], \"val_metric\": []}\n",
    "        self.best_val_loss = float(\"inf\")\n",
    "        self.best_ckpt = None\n",
    "        self.patience_counter = 0\n",
    "\n",
    "        # per-node weight lookup\n",
    "        self._node_weight_lookup: Optional[torch.Tensor] = None\n",
    "\n",
    "        # device\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if (self.cfg.device == \"cuda\" and torch.cuda.is_available()) else \"cpu\"\n",
    "        )\n",
    "\n",
    "    # ---------------------------------------------------------------- helpers\n",
    "    @staticmethod\n",
    "    def _edge_key(src: str, dst: str) -> str:\n",
    "        return f\"{src}-{dst}\".lower()\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_same_index(idxs: List[pd.DatetimeIndex]) -> List[pd.Timestamp]:\n",
    "        common = sorted(set.intersection(*(set(ix) for ix in idxs)))\n",
    "        if not common:\n",
    "            raise ValueError(\"No common timestamps across supplied DataFrames\")\n",
    "        return common\n",
    "\n",
    "    # ------------------------------------------------------- 1. snapshots\n",
    "    def prepare_snapshots(self) -> \"GNNExperiment\":\n",
    "        # node & edge order\n",
    "        self.reg_order = sorted(self.graph.nodes)\n",
    "        node_pos = {n: i for i, n in enumerate(self.reg_order)}\n",
    "        self.edge_order = [(node_pos[s], node_pos[d]) for (s, d) in self.graph.edges]\n",
    "\n",
    "        # node-weight lookup\n",
    "        if self.cfg.node_pos_weights is not None:\n",
    "            if len(self.cfg.node_pos_weights) != len(self.reg_order):\n",
    "                raise ValueError(\"len(node_pos_weights) must equal number of nodes\")\n",
    "            self._node_weight_lookup = torch.tensor(self.cfg.node_pos_weights, dtype=torch.float32)\n",
    "        else:\n",
    "            self._node_weight_lookup = None\n",
    "\n",
    "        # timestamps intersection\n",
    "        ts_common = self._ensure_same_index(\n",
    "            [df.index for df in self.node_frames.values()]\n",
    "            + [df.index for df in self.edge_frames.values()]\n",
    "        )\n",
    "\n",
    "        snapshots: List[Data] = []\n",
    "        for ts in ts_common:\n",
    "            # node features + label\n",
    "            feats, labels = [], []\n",
    "            for region in self.reg_order:\n",
    "                row = self.node_frames[region].loc[ts]\n",
    "                labels.append(row[self.cfg.target_col])\n",
    "                feats.append(row.drop(self.cfg.target_col).to_numpy(dtype=np.float32))\n",
    "            x = torch.tensor(np.vstack(feats), dtype=torch.float32)\n",
    "            y = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "            # edge features\n",
    "            edge_rows = []\n",
    "            for s_idx, d_idx in self.edge_order:\n",
    "                s, d = self.reg_order[s_idx], self.reg_order[d_idx]\n",
    "                edge_rows.append(\n",
    "                    self.edge_frames[self._edge_key(s, d)].loc[ts].to_numpy(dtype=np.float32)\n",
    "                )\n",
    "            edge_attr = torch.tensor(np.vstack(edge_rows), dtype=torch.float32)\n",
    "            edge_index = torch.tensor(np.array(self.edge_order).T, dtype=torch.long)\n",
    "\n",
    "            node_weight = (\n",
    "                self._node_weight_lookup.clone()\n",
    "                if self._node_weight_lookup is not None\n",
    "                else torch.ones(len(self.reg_order))\n",
    "            )\n",
    "\n",
    "            snapshots.append(\n",
    "                Data(\n",
    "                    x=x,\n",
    "                    edge_index=edge_index,\n",
    "                    edge_attr=edge_attr,\n",
    "                    y=y,\n",
    "                    node_weight=node_weight,\n",
    "                    snap_time=torch.tensor([pd.Timestamp(ts).value]),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.snapshots = snapshots\n",
    "        return self\n",
    "\n",
    "    # ------------------------------------------------------- 2. loaders\n",
    "    def build_loaders(self) -> \"GNNExperiment\":\n",
    "        if self.snapshots is None:\n",
    "            raise RuntimeError(\"Call prepare_snapshots() before build_loaders()\")\n",
    "\n",
    "        snaps_sorted = sorted(self.snapshots, key=lambda g: g.snap_time.item())\n",
    "\n",
    "        if self.cfg.split_mode == \"date\":\n",
    "            if self.cfg.cutoff_date is None:\n",
    "                raise ValueError(\"cutoff_date must be set for date split\")\n",
    "            cutoff_int = pd.Timestamp(self.cfg.cutoff_date).value\n",
    "            train_set = [g for g in snaps_sorted if g.snap_time.item() <= cutoff_int]\n",
    "            holdout = [g for g in snaps_sorted if g.snap_time.item() > cutoff_int]\n",
    "            if not holdout:\n",
    "                raise ValueError(\"No snapshots after cutoff_date for val/test split\")\n",
    "            mid = len(holdout) // 2\n",
    "            val_set, test_set = holdout[:mid], holdout[mid:]\n",
    "        else:\n",
    "            n_total = len(snaps_sorted)\n",
    "            n_test = int(n_total * self.cfg.test_ratio)\n",
    "            n_val = int(n_total * self.cfg.val_ratio)\n",
    "            n_train = n_total - n_val - n_test\n",
    "            train_set = snaps_sorted[:n_train]\n",
    "            val_set = snaps_sorted[n_train : n_train + n_val]\n",
    "            test_set = snaps_sorted[n_train + n_val :]\n",
    "\n",
    "        dl_kw = dict(batch_size=self.cfg.batch_size, shuffle=self.cfg.shuffle_in_split)\n",
    "        self.train_dl = DataLoader(train_set, **dl_kw)\n",
    "        self.val_dl = DataLoader(val_set, batch_size=self.cfg.batch_size, shuffle=False)\n",
    "        self.test_dl = DataLoader(test_set, batch_size=self.cfg.batch_size, shuffle=False)\n",
    "        return self\n",
    "\n",
    "    # ------------------------------------------------------- 3. model\n",
    "    def init_model(self) -> \"GNNExperiment\":\n",
    "        if self.snapshots is None:\n",
    "            raise RuntimeError(\"Call prepare_snapshots() first.\")\n",
    "\n",
    "        d_in = self.snapshots[0].x.size(1)\n",
    "        d_out = 1  # binary logit\n",
    "        edge_dim = self.snapshots[0].edge_attr.size(1)\n",
    "        p_drop = self.cfg.layer_dropout\n",
    "\n",
    "        name = self.cfg.model_name.lower()\n",
    "        if name in {\"simple\", \"baseline\"}:\n",
    "            model = GNNNodeSimple(d_in, self.cfg.hidden_dim, d_out, p_drop)\n",
    "        elif name == \"gcn\":\n",
    "            model = GNNGCN(d_in, self.cfg.hidden_dim, d_out, self.cfg.num_layers, p_drop)\n",
    "        elif name in {\"graphsage\", \"sage\"}:\n",
    "            model = GNNSage(d_in, self.cfg.hidden_dim, d_out, self.cfg.num_layers, p_drop)\n",
    "        elif name == \"gat\":\n",
    "            model = GNNGAT(d_in, self.cfg.hidden_dim, d_out, self.cfg.num_layers, self.cfg.heads, p_drop)\n",
    "        elif name in {\"gatv2\", \"gat2\"}:\n",
    "            model = GNNGAT2(\n",
    "                d_in,\n",
    "                self.cfg.hidden_dim,\n",
    "                d_out,\n",
    "                self.cfg.num_layers,\n",
    "                self.cfg.heads,\n",
    "                edge_dim,\n",
    "                p_drop,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model_name '{self.cfg.model_name}'\")\n",
    "\n",
    "        self.model = model.to(self.device)\n",
    "        return self\n",
    "\n",
    "    # ------------------------------------------------------- 4. compile\n",
    "    def compile(self) -> \"GNNExperiment\":\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"Call init_model() before compile()\")\n",
    "\n",
    "        # ---- loss ------------------------------------------------------\n",
    "        if self.cfg.loss_fn.lower() in {\"bce\", \"binary\"}:\n",
    "            self.loss_fn = torch.nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "            self.metric_fn = torch.nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "        elif self.cfg.loss_fn.lower() == \"cross_entropy\":\n",
    "            self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "            self.metric_fn = self.loss_fn\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported loss_fn '{self.cfg.loss_fn}'\")\n",
    "\n",
    "        # ---- optimiser -------------------------------------------------\n",
    "        opt_kw = dict(lr=self.cfg.lr, **self.cfg.optimiser_kwargs)\n",
    "        opt_name = self.cfg.optimiser.lower()\n",
    "        if opt_name == \"adam\":\n",
    "            self.optimizer = torch.optim.Adam(self.model.parameters(), **opt_kw)\n",
    "        elif opt_name == \"adamw\":\n",
    "            self.optimizer = torch.optim.AdamW(self.model.parameters(), **opt_kw)\n",
    "        elif opt_name == \"sgd\":\n",
    "            self.optimizer = torch.optim.SGD(self.model.parameters(), **opt_kw)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown optimiser '{self.cfg.optimiser}'\")\n",
    "\n",
    "        # ---- scheduler -------------------------------------------------\n",
    "        self.scheduler = None\n",
    "        if self.cfg.lr_scheduler is not None:\n",
    "            sname = self.cfg.lr_scheduler.lower()\n",
    "            kw = self.cfg.lr_scheduler_kwargs\n",
    "            if sname == \"step\":\n",
    "                self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, **kw)\n",
    "            elif sname == \"cosine\":\n",
    "                self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, **kw)\n",
    "            elif sname == \"plateau\":\n",
    "                self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, **kw)\n",
    "            elif sname == \"onecycle\":\n",
    "                self.scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optimizer, **kw)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown lr_scheduler '{self.cfg.lr_scheduler}'\")\n",
    "\n",
    "        # track last LR for pretty print\n",
    "        self._last_lr = self.optimizer.param_groups[0][\"lr\"]\n",
    "        return self\n",
    "\n",
    "    # ------------------------------------------------ edge-dropout helper\n",
    "    def _apply_edge_dropout(self, data: Data) -> Data:\n",
    "        p = getattr(self.cfg, \"edge_dropout\", 0.0)\n",
    "        if p <= 0.0 or not self.model.training:\n",
    "            return data\n",
    "        E = data.edge_index.size(1)\n",
    "        keep_mask = torch.rand(E, device=data.edge_index.device) >= p\n",
    "        if keep_mask.all():\n",
    "            return data\n",
    "        data.edge_index = data.edge_index[:, keep_mask]\n",
    "        if data.edge_attr is not None:\n",
    "            data.edge_attr = data.edge_attr[keep_mask]\n",
    "        return data\n",
    "\n",
    "    # ------------------------------------------------ train-epoch helper\n",
    "    def _train_epoch(self, debug: bool = False) -> float:\n",
    "        self.model.train()\n",
    "        total = 0.0\n",
    "        for step, data in enumerate(self.train_dl):\n",
    "            data = data.to(self.device)\n",
    "            data = self._apply_edge_dropout(data)\n",
    "\n",
    "            data.x = data.x.float()\n",
    "            if data.edge_attr is not None:\n",
    "                data.edge_attr = data.edge_attr.float()\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            logits = self.model(data).squeeze(-1)\n",
    "            loss_vec = self.loss_fn(logits, data.y)\n",
    "            loss_vec = loss_vec * data.node_weight.to(loss_vec.device)\n",
    "            loss = loss_vec.mean()\n",
    "\n",
    "            if debug and (has_nan(loss) or has_nan(logits)):\n",
    "                raise RuntimeError(f\"NaN detected at step {step}\")\n",
    "\n",
    "            loss.backward()\n",
    "            if self.cfg.grad_clip is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.cfg.grad_clip)\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total += loss.item()\n",
    "        return total / len(self.train_dl)\n",
    "\n",
    "    # ------------------------------------------------ eval helper\n",
    "    @torch.no_grad()\n",
    "    def _eval_loader(self, loader: DataLoader) -> Tuple[float, float]:\n",
    "        self.model.eval()\n",
    "        total_loss = total_metric = 0.0\n",
    "        for data in loader:\n",
    "            data = data.to(self.device)\n",
    "            data.x = data.x.float()\n",
    "            if data.edge_attr is not None:\n",
    "                data.edge_attr = data.edge_attr.float()\n",
    "\n",
    "            logits = self.model(data).squeeze(-1)\n",
    "            loss_vec = self.loss_fn(logits, data.y)\n",
    "            metric_vec = self.metric_fn(logits, data.y)\n",
    "\n",
    "            w = data.node_weight.to(loss_vec.device)\n",
    "            total_loss += (loss_vec * w).mean().item()\n",
    "            total_metric += (metric_vec * w).mean().item()\n",
    "        n = len(loader)\n",
    "        return total_loss / n, total_metric / n\n",
    "\n",
    "    # ------------------------------------------------ main training loop\n",
    "    def train(self, debug: bool = False):\n",
    "        if any(v is None for v in (self.model, self.optimizer, self.loss_fn)):\n",
    "            raise RuntimeError(\"Call init_model() and compile() before train()\")\n",
    "\n",
    "        for epoch in range(1, self.cfg.epochs + 1):\n",
    "            tr_loss = self._train_epoch(debug)\n",
    "            val_loss, val_metric = self._eval_loader(self.val_dl)\n",
    "\n",
    "            # scheduler\n",
    "            if self.scheduler is not None:\n",
    "                if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                    self.scheduler.step(val_loss)\n",
    "                else:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "            # LR print\n",
    "            if self.cfg.print_lr_each_epoch:\n",
    "                curr_lr = self.optimizer.param_groups[0][\"lr\"]\n",
    "                lr_msg = f\"LR changed → {curr_lr:.2e}\" if curr_lr != self._last_lr else f\"LR={curr_lr:.2e}\"\n",
    "                self._last_lr = curr_lr\n",
    "            else:\n",
    "                lr_msg = \"\"\n",
    "\n",
    "            # store history\n",
    "            self.history[\"train_loss\"].append(tr_loss)\n",
    "            self.history[\"val_loss\"].append(val_loss)\n",
    "            self.history[\"val_metric\"].append(val_metric)\n",
    "\n",
    "            print(\n",
    "                f\"[Epoch {epoch:03d}/{self.cfg.epochs}] \"\n",
    "                f\"train={tr_loss:.4f}  val={val_loss:.4f}  \"\n",
    "                f\"metric={val_metric:.4f}  {lr_msg}\"\n",
    "            )\n",
    "\n",
    "            # early stopping\n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "                self.patience_counter = 0\n",
    "                self.best_ckpt = {k: v.cpu() for k, v in self.model.state_dict().items()}\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "                if self.patience_counter >= self.cfg.patience:\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "        return self\n",
    "\n",
    "    # ------------------------------------------------ evaluate\n",
    "    def evaluate(self, split: str = \"test\") -> Dict[str, float]:\n",
    "        loader = {\"train\": self.train_dl, \"val\": self.val_dl, \"test\": self.test_dl}.get(split)\n",
    "        if loader is None:\n",
    "            raise ValueError(\"split must be 'train', 'val' or 'test'\")\n",
    "        loss, metric = self._eval_loader(loader)\n",
    "        return {\"loss\": loss, \"metric\": metric}\n",
    "\n",
    "    # ------------------------------------------------ predict\n",
    "    @torch.no_grad()\n",
    "    def predict(\n",
    "        self,\n",
    "        node_frames_new: Dict[str, pd.DataFrame],\n",
    "        edge_frames_new: Dict[str, pd.DataFrame],\n",
    "        timestamps: Optional[List[pd.Timestamp]] = None,\n",
    "        return_df: bool = True,\n",
    "    ):\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"Train or load a model before calling predict()\")\n",
    "\n",
    "        # decide timestamps\n",
    "        if timestamps is None:\n",
    "            idxs = (\n",
    "                [df.index for df in node_frames_new.values()]\n",
    "                + [df.index for df in edge_frames_new.values()]\n",
    "            )\n",
    "            timestamps = sorted(set.intersection(*(set(ix) for ix in idxs)))\n",
    "        else:\n",
    "            timestamps = [pd.Timestamp(ts) for ts in timestamps]\n",
    "\n",
    "        snaps = []\n",
    "        for ts in timestamps:\n",
    "            feats = []\n",
    "            for region in self.reg_order:\n",
    "                feats.append(\n",
    "                    node_frames_new[region]\n",
    "                    .loc[ts]\n",
    "                    .drop(self.cfg.target_col)\n",
    "                    .to_numpy(dtype=np.float32)\n",
    "                )\n",
    "            x = torch.tensor(np.vstack(feats), dtype=torch.float32)\n",
    "\n",
    "            edge_rows = []\n",
    "            for s_idx, d_idx in self.edge_order:\n",
    "                s, d = self.reg_order[s_idx], self.reg_order[d_idx]\n",
    "                edge_rows.append(\n",
    "                    edge_frames_new[f\"{s}-{d}\".lower()].loc[ts].to_numpy(dtype=np.float32)\n",
    "                )\n",
    "            edge_attr = torch.tensor(np.vstack(edge_rows), dtype=torch.float32)\n",
    "            edge_index = torch.tensor(np.array(self.edge_order).T, dtype=torch.long)\n",
    "\n",
    "            snaps.append(Data(x=x, edge_index=edge_index, edge_attr=edge_attr))\n",
    "\n",
    "        loader = DataLoader(snaps, batch_size=self.cfg.batch_size, shuffle=False)\n",
    "        self.model.eval()\n",
    "        preds = []\n",
    "        for data in loader:\n",
    "            data = data.to(self.device)\n",
    "            data.x = data.x.float()\n",
    "            if data.edge_attr is not None:\n",
    "                data.edge_attr = data.edge_attr.float()\n",
    "            logits = self.model(data).squeeze(-1).cpu()\n",
    "            preds.append(logits)\n",
    "\n",
    "        y_hat = torch.cat(preds, dim=0)           # (len(ts)*N)\n",
    "        y_hat = y_hat.reshape(len(timestamps), len(self.reg_order))\n",
    "\n",
    "        if return_df:\n",
    "            return pd.DataFrame(\n",
    "                y_hat.numpy(),\n",
    "                index=pd.to_datetime(timestamps),\n",
    "                columns=self.reg_order,\n",
    "            )\n",
    "        return y_hat\n",
    "\n",
    "    # ------------------------------------------------ plot history\n",
    "    def plot_history(self, metric: str = \"loss\"):\n",
    "        if not self.history[\"train_loss\"]:\n",
    "            raise RuntimeError(\"Nothing in history – did you call train()?\")\n",
    "\n",
    "        if metric == \"loss\":\n",
    "            plt.figure(figsize=(6, 4))\n",
    "            plt.plot(self.history[\"train_loss\"], label=\"Train\")\n",
    "            plt.plot(self.history[\"val_loss\"], label=\"Val\")\n",
    "            plt.xlabel(\"Epoch\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.title(\"Training / Validation Loss\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "\n",
    "        elif metric in {\"metric\", \"mae\", \"accuracy\"}:\n",
    "            if not self.history[\"val_metric\"]:\n",
    "                raise ValueError(\"Metric history empty; choose 'loss' instead.\")\n",
    "            plt.figure(figsize=(6, 4))\n",
    "            plt.plot(self.history[\"val_metric\"], label=f\"Val {metric}\")\n",
    "            plt.xlabel(\"Epoch\")\n",
    "            plt.ylabel(metric.upper())\n",
    "            plt.title(f\"Validation {metric.upper()} Curve\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "        else:\n",
    "            raise ValueError(\"metric must be 'loss' or 'metric'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3333357",
   "metadata": {},
   "source": [
    "### Rare event detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7469eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess  # pip install statsmodels\n",
    "\n",
    "def prob_vs_continuous(\n",
    "    df_proba: pd.DataFrame,\n",
    "    df_true_cont: pd.DataFrame,\n",
    "    node: str,\n",
    "    cutoff: float,\n",
    "    max_points: int = 5000,\n",
    "    lowess_frac: float = 0.15,\n",
    "):\n",
    "    \"\"\"\n",
    "    Scatter + LOWESS smoothed curve of predicted probability vs. true continuous value.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_proba      : DataFrame of predicted probabilities (shape [t, nodes])\n",
    "    df_true_cont  : DataFrame of true continuous target (same shape / index / columns)\n",
    "    node          : Which node/column to plot\n",
    "    cutoff        : Threshold that defines the binary label\n",
    "    max_points    : Sub-sample size for scatter to keep the plot light\n",
    "    lowess_frac   : Span for LOWESS smoothing (between 0 and 1)\n",
    "    \"\"\"\n",
    "    p = df_proba[node].copy()\n",
    "    x = df_true_cont[node].copy()\n",
    "\n",
    "    # Sub-sample for scatter if very large\n",
    "    if len(p) > max_points:\n",
    "        idx = np.random.choice(len(p), size=max_points, replace=False)\n",
    "        p = p.iloc[idx]\n",
    "        x = x.iloc[idx]\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.scatterplot(x=x, y=p, alpha=0.3, s=15, edgecolor=\"none\")\n",
    "\n",
    "    # LOWESS smooth\n",
    "    sm = lowess(p, x, frac=lowess_frac, return_sorted=True)\n",
    "    plt.plot(sm[:, 0], sm[:, 1], color=\"red\", lw=2, label=\"LOWESS avg\")\n",
    "\n",
    "    # cutoff line\n",
    "    plt.axvline(cutoff, color=\"gray\", ls=\"--\", label=f\"cutoff = {cutoff}\")\n",
    "\n",
    "    plt.xlabel(\"True continuous value\")\n",
    "    plt.ylabel(\"Predicted probability\")\n",
    "    plt.title(f\"{node}: p̂ vs. true value\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Suppose df_logits and df_true_continuous share index & columns\n",
    "df_proba = 1 / (1 + np.exp(-df_logits))       # convert to probability\n",
    "prob_vs_continuous(\n",
    "    df_proba,\n",
    "    df_true_continuous,\n",
    "    node=\"Tokyo\",\n",
    "    cutoff=0.10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca4b7ac",
   "metadata": {},
   "source": [
    "### Comparing to baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69c1487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic_threshold_noscale.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ────────────────────────  helpers ────────────────────────\n",
    "def build_lagged_features(df: pd.DataFrame,\n",
    "                          target_col: str,\n",
    "                          lags: List[int]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds lag columns target_col_lag{k} for each k in lags.\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    for k in lags:\n",
    "        out[f\"{target_col}_lag{k}\"] = out[target_col].shift(k)\n",
    "    return out\n",
    "\n",
    "\n",
    "def plot_pr_curve(y_true: np.ndarray,\n",
    "                  y_proba: np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    Precision-Recall plot with average-precision (AP) in the title.\n",
    "    \"\"\"\n",
    "    prec, rec, _ = precision_recall_curve(y_true, y_proba)\n",
    "    ap = average_precision_score(y_true, y_proba)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.step(rec, prec, where=\"post\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(f\"Precision–Recall curve  (AP = {ap:.3f})\")\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.xlim(0, 1.0)\n",
    "    plt.grid(alpha=.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ────────────────────────  main training routine ────────────────────────\n",
    "def train_logistic_model(df: pd.DataFrame,\n",
    "                         target_col: str = \"value\",\n",
    "                         threshold: float = 10.0,\n",
    "                         lags: List[int] = (1, 2, 3),\n",
    "                         test_size: float = 0.2,\n",
    "                         class_weight: Dict[int, float] | str | None = None,\n",
    "                         solver: str = \"lbfgs\",\n",
    "                         max_iter: int = 1000,\n",
    "                         random_state: int | None = 42\n",
    "                         ) -> Tuple[LogisticRegression, dict]:\n",
    "    \"\"\"\n",
    "    Fits a LogisticRegression without scaling.\n",
    "    • df must have a DateTime-like index already sorted ascending.\n",
    "    • class_weight can be a dict such as {0: 1, 1: 200} or \"balanced\".\n",
    "    Returns (fitted_model, metrics_dict).\n",
    "    \"\"\"\n",
    "\n",
    "    # 1 . Feature engineering – add lags\n",
    "    data = build_lagged_features(df, target_col, lags)\n",
    "\n",
    "    # 2 . Binary label: 1 if below threshold\n",
    "    data[\"y\"] = (data[target_col] < threshold).astype(int)\n",
    "\n",
    "    # 3 . Drop rows made NaN by shifting\n",
    "    data = data.dropna()\n",
    "\n",
    "    # 4 . Time-based split (no leakage!)\n",
    "    split_idx = int(len(data) * (1 - test_size))\n",
    "    train, test = data.iloc[:split_idx], data.iloc[split_idx:]\n",
    "\n",
    "    X_train = train.drop(columns=[target_col, \"y\"])\n",
    "    y_train = train[\"y\"]\n",
    "    X_test = test.drop(columns=[target_col, \"y\"])\n",
    "    y_test = test[\"y\"]\n",
    "\n",
    "    # 5 . Fit logistic model\n",
    "    clf = LogisticRegression(\n",
    "        class_weight=class_weight,\n",
    "        solver=solver,\n",
    "        max_iter=max_iter,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # 6 . Evaluation focused on PR\n",
    "    y_proba = clf.predict_proba(X_test)[:, 1]\n",
    "    ap = average_precision_score(y_test, y_proba)\n",
    "    prec, rec, thr = precision_recall_curve(y_test, y_proba)\n",
    "\n",
    "    metrics = {\n",
    "        \"average_precision\": ap,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"thresholds\": thr,\n",
    "        \"y_test\": y_test.to_numpy(),\n",
    "        \"y_proba\": y_proba,\n",
    "    }\n",
    "\n",
    "    return clf, metrics\n",
    "How you might call it\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Example imbalance: positives ≈ 0.5 %, weight them 200× higher\n",
    "cw = {0: 1, 1: 200}\n",
    "\n",
    "model, metrics = train_logistic_model(\n",
    "    df,                          # your DateTime-indexed DataFrame\n",
    "    target_col=\"value\",\n",
    "    threshold=10,\n",
    "    lags=[1, 2, 3, 6, 12],       # whatever lags make sense\n",
    "    test_size=0.25,\n",
    "    class_weight=cw\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d4cb77",
   "metadata": {},
   "source": [
    "### Quantile regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3bdc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantile_regression_gbr.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_pinball_loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ────────────────────────  helpers ────────────────────────\n",
    "def build_lagged_features(df: pd.DataFrame,\n",
    "                          target_col: str,\n",
    "                          lags: List[int]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds lag columns target_col_lag{k} for each k in lags.\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    for k in lags:\n",
    "        out[f\"{target_col}_lag{k}\"] = out[target_col].shift(k)\n",
    "    return out\n",
    "\n",
    "\n",
    "def plot_actual_vs_pred(y_true: np.ndarray,\n",
    "                        y_pred: np.ndarray,\n",
    "                        alpha: float) -> None:\n",
    "    \"\"\"\n",
    "    Quick scatter plot to eyeball calibration:\n",
    "    points should lie mostly *above* the diagonal for lower-tail quantiles.\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.scatter(y_true, y_pred, s=10, alpha=0.3)\n",
    "    diag_min, diag_max = y_true.min(), y_true.max()\n",
    "    plt.plot([diag_min, diag_max], [diag_min, diag_max],\n",
    "             linestyle='--', linewidth=1)\n",
    "    plt.xlabel(\"Actual\")\n",
    "    plt.ylabel(f\"Predicted Q{int(alpha*100)}\")\n",
    "    plt.title(f\"Actual vs Predicted α-Quantile (α={alpha})\")\n",
    "    plt.grid(alpha=.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ────────────────────────  main training routine ────────────────────────\n",
    "def train_quantile_model(df: pd.DataFrame,\n",
    "                         target_col: str = \"value\",\n",
    "                         alpha: float = 0.05,\n",
    "                         lags: List[int] = (1, 2, 3),\n",
    "                         test_size: float = 0.2,\n",
    "                         n_estimators: int = 500,\n",
    "                         learning_rate: float = 0.03,\n",
    "                         max_depth: int = 3,\n",
    "                         random_state: int | None = 42\n",
    "                         ) -> Tuple[GradientBoostingRegressor, dict]:\n",
    "    \"\"\"\n",
    "    Fits a GradientBoostingRegressor with quantile loss.\n",
    "    • df must have a DateTime-like index already sorted ascending.\n",
    "    Returns (fitted_model, metrics_dict).\n",
    "    \"\"\"\n",
    "\n",
    "    # 1 . Feature engineering – add lags\n",
    "    data = build_lagged_features(df, target_col, lags)\n",
    "\n",
    "    # 2 . Drop rows made NaN by shifting\n",
    "    data = data.dropna()\n",
    "\n",
    "    # 3 . Time-based split (no leakage!)\n",
    "    split_idx = int(len(data) * (1 - test_size))\n",
    "    train, test = data.iloc[:split_idx], data.iloc[split_idx:]\n",
    "\n",
    "    X_train = train.drop(columns=[target_col])\n",
    "    y_train = train[target_col]\n",
    "    X_test = test.drop(columns=[target_col])\n",
    "    y_test = test[target_col]\n",
    "\n",
    "    # 4 . Fit quantile GBRT\n",
    "    gbr = GradientBoostingRegressor(\n",
    "        loss=\"quantile\",\n",
    "        alpha=alpha,\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        max_depth=max_depth,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    gbr.fit(X_train, y_train)\n",
    "\n",
    "    # 5 . Evaluation\n",
    "    y_pred = gbr.predict(X_test)\n",
    "    pinball = mean_pinball_loss(y_test, y_pred, alpha=alpha)\n",
    "    coverage = (y_test < y_pred).mean()     # empirical P(actual < predicted)\n",
    "\n",
    "    metrics = {\n",
    "        \"pinball_loss\": pinball,\n",
    "        \"coverage\": coverage,\n",
    "        \"y_test\": y_test.to_numpy(),\n",
    "        \"y_pred\": y_pred,\n",
    "    }\n",
    "\n",
    "    return gbr, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4dda2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantile_regression_gbr import train_quantile_model, plot_actual_vs_pred\n",
    "\n",
    "# Suppose 'df' is your DateTime-indexed DataFrame with column 'value'\n",
    "model, metrics = train_quantile_model(\n",
    "    df,\n",
    "    target_col=\"value\",\n",
    "    alpha=0.01,           # 1 % lower-tail quantile\n",
    "    lags=[1, 2, 3, 6, 12],\n",
    "    test_size=0.25\n",
    ")\n",
    "\n",
    "print(\"Pinball Loss:\", round(metrics[\"pinball_loss\"], 6))\n",
    "print(\"Empirical coverage:\", round(metrics[\"coverage\"], 4))  # should be ≈ 0.01\n",
    "\n",
    "plot_actual_vs_pred(metrics[\"y_test\"], metrics[\"y_pred\"], alpha=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febcd3a1",
   "metadata": {},
   "source": [
    "### Quantile regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d81445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantile_linear_regression.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_pinball_loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ────────────────────────  helpers ────────────────────────\n",
    "def build_lagged_features(df: pd.DataFrame,\n",
    "                          target_col: str,\n",
    "                          lags: List[int]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds lag columns target_col_lag{k} for each k in lags.\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    for k in lags:\n",
    "        out[f\"{target_col}_lag{k}\"] = out[target_col].shift(k)\n",
    "    return out\n",
    "\n",
    "\n",
    "def plot_actual_vs_pred(y_true: np.ndarray,\n",
    "                        y_pred: np.ndarray,\n",
    "                        alpha: float) -> None:\n",
    "    \"\"\"\n",
    "    Scatter plot to eyeball calibration of the predicted α-quantile.\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.scatter(y_true, y_pred, s=14, alpha=0.3)\n",
    "    lims = [y_true.min(), y_true.max()]\n",
    "    plt.plot(lims, lims, \"--\", linewidth=1)\n",
    "    plt.xlabel(\"Actual\")\n",
    "    plt.ylabel(f\"Predicted Q{int(alpha*100)}\")\n",
    "    plt.title(f\"Linear Quantile Regression (α={alpha})\")\n",
    "    plt.grid(alpha=.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ─────────────────────  main training routine  ───────────────────────────\n",
    "def train_quantile_linear_model(df: pd.DataFrame,\n",
    "                                target_col: str = \"value\",\n",
    "                                alpha: float = 0.05,\n",
    "                                lags: List[int] = (1, 2, 3),\n",
    "                                test_size: float = 0.2,\n",
    "                                max_iter: int = 5000,\n",
    "                                p_tol: float = 1e-5,\n",
    "                                random_state: int | None = 42\n",
    "                                ) -> Tuple[sm.regression.linear_model.RegressionResultsWrapper, dict]:\n",
    "    \"\"\"\n",
    "    Fits a *linear* quantile regression (statsmodels.QuantReg).\n",
    "    • df must have a DateTime-like index already sorted ascending.\n",
    "    Returns (fitted_results, metrics_dict).\n",
    "    \"\"\"\n",
    "\n",
    "    # 1 . Feature engineering – add lags\n",
    "    data = build_lagged_features(df, target_col, lags)\n",
    "\n",
    "    # 2 . Drop NaNs caused by shifting\n",
    "    data = data.dropna()\n",
    "\n",
    "    # 3 . Time-based split\n",
    "    split_idx = int(len(data) * (1 - test_size))\n",
    "    train, test = data.iloc[:split_idx], data.iloc[split_idx:]\n",
    "\n",
    "    X_train = train.drop(columns=[target_col])\n",
    "    y_train = train[target_col]\n",
    "    X_test = test.drop(columns=[target_col])\n",
    "    y_test = test[target_col]\n",
    "\n",
    "    # 4 . Add constant term\n",
    "    X_train_c = sm.add_constant(X_train, has_constant=\"add\")\n",
    "    X_test_c = sm.add_constant(X_test, has_constant=\"add\")\n",
    "\n",
    "    # 5 . Fit linear quantile regression\n",
    "    qr_mod = sm.QuantReg(y_train, X_train_c)\n",
    "    qr_res = qr_mod.fit(q=alpha,\n",
    "                        max_iter=max_iter,\n",
    "                        p_tol=p_tol,\n",
    "                        disp=False)\n",
    "\n",
    "    # 6 . Predict and evaluate\n",
    "    y_pred = qr_res.predict(X_test_c)\n",
    "    pinball = mean_pinball_loss(y_test, y_pred, alpha=alpha)\n",
    "    coverage = (y_test < y_pred).mean()\n",
    "\n",
    "    metrics = {\n",
    "        \"pinball_loss\": pinball,\n",
    "        \"coverage\": coverage,\n",
    "        \"y_test\": y_test.to_numpy(),\n",
    "        \"y_pred\": y_pred.to_numpy(),\n",
    "    }\n",
    "\n",
    "    return qr_res, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1562b61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantile_linear_regression import (\n",
    "    train_quantile_linear_model,\n",
    "    plot_actual_vs_pred,\n",
    ")\n",
    "\n",
    "# df is a DateTime-indexed DataFrame with column 'value'\n",
    "results, metrics = train_quantile_linear_model(\n",
    "    df,\n",
    "    target_col=\"value\",\n",
    "    alpha=0.01,          # 1 % lower-tail quantile\n",
    "    lags=[1, 2, 3, 6, 12],\n",
    "    test_size=0.25\n",
    ")\n",
    "\n",
    "print(results.summary())                 # full regression table\n",
    "print(\"Pinball loss:\", metrics[\"pinball_loss\"])\n",
    "print(\"Empirical coverage:\", metrics[\"coverage\"])  # ≈ 0.01 if well-calibrated\n",
    "\n",
    "plot_actual_vs_pred(metrics[\"y_test\"], metrics[\"y_pred\"], alpha=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbefff18",
   "metadata": {},
   "source": [
    "### alll rewrote "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c803a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Tuple, Dict, Union\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ───────────────────────── helpers ──────────────────────────\n",
    "def plot_pr_curve(y_true: np.ndarray, y_proba: np.ndarray) -> None:\n",
    "    prec, rec, _ = precision_recall_curve(y_true, y_proba)\n",
    "    ap = average_precision_score(y_true, y_proba)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.step(rec, prec, where=\"post\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(f\"Precision–Recall curve  (AP = {ap:.3f})\")\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.xlim(0, 1.0)\n",
    "    plt.grid(alpha=.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ───────────────────── main training routine ─────────────────────\n",
    "def train_logistic_model(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    test_size: float = 0.2,\n",
    "    class_weight: Union[Dict[int, float], str, None] = None,\n",
    "    solver: str = \"lbfgs\",\n",
    "    max_iter: int = 1000,\n",
    "    random_state: int | None = 42,\n",
    ") -> Tuple[LogisticRegression, dict]:\n",
    "    \"\"\"\n",
    "    Train a class-weighted LogisticRegression (no scaling, no feature engineering).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : DataFrame\n",
    "        Feature matrix with a DateTime-like, ascending index.\n",
    "    y : Series\n",
    "        0/1 labels aligned on the same index as X.\n",
    "    test_size : float\n",
    "        Fraction of the most-recent rows kept for testing (time-ordered split).\n",
    "    class_weight : dict | \"balanced\" | None\n",
    "        Passed straight to sklearn’s LogisticRegression.\n",
    "    \"\"\"\n",
    "\n",
    "    if not X.index.is_monotonic_increasing:\n",
    "        raise ValueError(\"X must be sorted in chronological order (oldest → newest).\")\n",
    "\n",
    "    # 1. Time-based split\n",
    "    split_idx = int(len(X) * (1 - test_size))\n",
    "    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "    # 2. Fit model\n",
    "    clf = LogisticRegression(\n",
    "        class_weight=class_weight,\n",
    "        solver=solver,\n",
    "        max_iter=max_iter,\n",
    "        random_state=random_state,\n",
    "    ).fit(X_train, y_train)\n",
    "\n",
    "    # 3. Evaluate\n",
    "    y_proba = clf.predict_proba(X_test)[:, 1]\n",
    "    ap = average_precision_score(y_test, y_proba)\n",
    "    prec, rec, thr = precision_recall_curve(y_test, y_proba)\n",
    "\n",
    "    metrics = {\n",
    "        \"average_precision\": ap,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"thresholds\": thr,\n",
    "        \"y_test\": y_test.to_numpy(),\n",
    "        \"y_proba\": y_proba,\n",
    "    }\n",
    "\n",
    "    return clf, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ebd103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_pinball_loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_actual_vs_pred(y_true: np.ndarray, y_pred: np.ndarray, alpha: float) -> None:\n",
    "    plt.figure()\n",
    "    plt.scatter(y_true, y_pred, s=10, alpha=0.3)\n",
    "    lims = [y_true.min(), y_true.max()]\n",
    "    plt.plot(lims, lims, \"--\", linewidth=1)\n",
    "    plt.xlabel(\"Actual\")\n",
    "    plt.ylabel(f\"Predicted Q{int(alpha*100)}\")\n",
    "    plt.title(f\"GBR Quantile α={alpha}\")\n",
    "    plt.grid(alpha=.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def train_quantile_gbr(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    alpha: float = 0.05,\n",
    "    test_size: float = 0.2,\n",
    "    n_estimators: int = 500,\n",
    "    learning_rate: float = 0.03,\n",
    "    max_depth: int = 3,\n",
    "    random_state: int | None = 42,\n",
    ") -> Tuple[GradientBoostingRegressor, dict]:\n",
    "    \"\"\"\n",
    "    Tree-based quantile regression without any automatic feature engineering.\n",
    "    \"\"\"\n",
    "\n",
    "    if not X.index.is_monotonic_increasing:\n",
    "        raise ValueError(\"X must be in chronological order.\")\n",
    "\n",
    "    split_idx = int(len(X) * (1 - test_size))\n",
    "    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "    gbr = GradientBoostingRegressor(\n",
    "        loss=\"quantile\",\n",
    "        alpha=alpha,\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        max_depth=max_depth,\n",
    "        random_state=random_state,\n",
    "    ).fit(X_train, y_train)\n",
    "\n",
    "    y_pred = gbr.predict(X_test)\n",
    "    pinball = mean_pinball_loss(y_test, y_pred, alpha=alpha)\n",
    "    coverage = (y_test < y_pred).mean()\n",
    "\n",
    "    metrics = {\n",
    "        \"pinball_loss\": pinball,\n",
    "        \"coverage\": coverage,\n",
    "        \"y_test\": y_test.to_numpy(),\n",
    "        \"y_pred\": y_pred,\n",
    "    }\n",
    "\n",
    "    return gbr, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babfbdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_pinball_loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_actual_vs_pred(y_true: np.ndarray, y_pred: np.ndarray, alpha: float) -> None:\n",
    "    plt.figure()\n",
    "    plt.scatter(y_true, y_pred, s=14, alpha=0.3)\n",
    "    lims = [y_true.min(), y_true.max()]\n",
    "    plt.plot(lims, lims, \"--\", linewidth=1)\n",
    "    plt.xlabel(\"Actual\")\n",
    "    plt.ylabel(f\"Predicted Q{int(alpha*100)}\")\n",
    "    plt.title(f\"Linear Quantile α={alpha}\")\n",
    "    plt.grid(alpha=.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def train_quantile_linear(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    alpha: float = 0.05,\n",
    "    test_size: float = 0.2,\n",
    "    max_iter: int = 5000,\n",
    "    p_tol: float = 1e-5,\n",
    ") -> Tuple[sm.regression.linear_model.RegressionResultsWrapper, dict]:\n",
    "    \"\"\"\n",
    "    statsmodels QuantReg on pre-prepared features.\n",
    "    \"\"\"\n",
    "\n",
    "    if not X.index.is_monotonic_increasing:\n",
    "        raise ValueError(\"X must be in chronological order.\")\n",
    "\n",
    "    split_idx = int(len(X) * (1 - test_size))\n",
    "    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "    X_train_c = sm.add_constant(X_train, has_constant=\"add\")\n",
    "    X_test_c = sm.add_constant(X_test, has_constant=\"add\")\n",
    "\n",
    "    qr_res = (\n",
    "        sm.QuantReg(y_train, X_train_c)\n",
    "        .fit(q=alpha, max_iter=max_iter, p_tol=p_tol, disp=False)\n",
    "    )\n",
    "\n",
    "    y_pred = qr_res.predict(X_test_c)\n",
    "    pinball = mean_pinball_loss(y_test, y_pred, alpha=alpha)\n",
    "    coverage = (y_test < y_pred).mean()\n",
    "\n",
    "    metrics = {\n",
    "        \"pinball_loss\": pinball,\n",
    "        \"coverage\": coverage,\n",
    "        \"y_test\": y_test.to_numpy(),\n",
    "        \"y_pred\": y_pred.to_numpy(),\n",
    "    }\n",
    "\n",
    "    return qr_res, metrics\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
