{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12ea8320",
   "metadata": {},
   "source": [
    "### Edge classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db264527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnn_edge_clf.py\n",
    "# --------------------------------------------------------------\n",
    "# Dependencies: torch, torch_geometric, pandas, numpy, networkx\n",
    "# --------------------------------------------------------------\n",
    "import json, pathlib, math, random\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, Any, Optional, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCN, GraphSAGE, GAT, GATv2Conv\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1. Configuration\n",
    "# ---------------------------------------------------------------------\n",
    "@dataclass\n",
    "class GNNConfig:\n",
    "    model_name: str = \"gcn\"         # {\"gcn\",\"graphsage\",\"gat\"}\n",
    "    hidden_dim: int = 128\n",
    "    num_layers: int = 2\n",
    "    heads: int = 8                  # for GAT*\n",
    "    layer_dropout: float = 0.3\n",
    "\n",
    "    # optimisation\n",
    "    batch_size: int = 32\n",
    "    lr: float = 1e-3\n",
    "    epochs: int = 100\n",
    "    patience: int = 15\n",
    "    grad_clip: float = 1.0\n",
    "\n",
    "    # split\n",
    "    split_mode: str = \"date\"        # {\"date\",\"ratio\"}\n",
    "    cutoff_date: Optional[str] = None\n",
    "    val_ratio: float = 0.10\n",
    "    test_ratio: float = 0.10\n",
    "    shuffle_in_split: bool = False\n",
    "\n",
    "    # misc\n",
    "    device: str = \"cuda\"\n",
    "    seed: int = 42\n",
    "\n",
    "    # edge imbalance override\n",
    "    edge_pos_weights: Optional[List[float]] = None\n",
    "\n",
    "    @staticmethod\n",
    "    def load(x):                     # allow json / dict / object\n",
    "        if isinstance(x, GNNConfig):\n",
    "            return x\n",
    "        if isinstance(x, dict):\n",
    "            return GNNConfig(**x)\n",
    "        p = pathlib.Path(x)\n",
    "        with open(p) as f:\n",
    "            return GNNConfig(**json.load(f))\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2. Mini Encoder Zoo  (node → z)\n",
    "# ---------------------------------------------------------------------\n",
    "class EncoderGCN(nn.Module):\n",
    "    def __init__(self, d_in, h, L, p):\n",
    "        super().__init__()\n",
    "        self.body = GCN(d_in, h, L, norm=\"batch\", act=\"relu\")\n",
    "        self.drop = nn.Dropout(p)\n",
    "    def forward(self, x, ei, ea):\n",
    "        return self.drop(self.body(x, ei))\n",
    "\n",
    "class EncoderSAGE(nn.Module):\n",
    "    def __init__(self, d_in, h, L, p):\n",
    "        super().__init__()\n",
    "        self.body = GraphSAGE(d_in, h, L, norm=\"batch\", act=\"relu\")\n",
    "        self.drop = nn.Dropout(p)\n",
    "    def forward(self, x, ei, ea):\n",
    "        return self.drop(self.body(x, ei))\n",
    "\n",
    "class EncoderGAT(nn.Module):\n",
    "    def __init__(self, d_in, h, L, heads, p):\n",
    "        super().__init__()\n",
    "        self.body = GAT(d_in, h, L, heads=heads, norm=\"batch\", act=\"relu\")\n",
    "        self.drop = nn.Dropout(p)\n",
    "    def forward(self, x, ei, ea):\n",
    "        return self.drop(self.body(x, ei))\n",
    "\n",
    "ENCODER_FACTORY = {\n",
    "    \"gcn\":   EncoderGCN,\n",
    "    \"graphsage\": EncoderSAGE,\n",
    "    \"sage\":  EncoderSAGE,\n",
    "    \"gat\":   EncoderGAT,\n",
    "}\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3. Edge decoder  (z_u , z_v) → logit\n",
    "# ---------------------------------------------------------------------\n",
    "class DotDecoder(nn.Module):\n",
    "    def forward(self, z_src, z_dst):\n",
    "        return (z_src * z_dst).sum(-1, keepdim=True)  # (E,1)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4. Experiment wrapper\n",
    "# ---------------------------------------------------------------------\n",
    "class EdgeExperiment:\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_frames: Dict[str, pd.DataFrame],\n",
    "        edge_frames: Dict[str, pd.DataFrame],\n",
    "        graph: nx.DiGraph,\n",
    "        cfg: GNNConfig | Dict[str, Any] | str = GNNConfig(),\n",
    "    ):\n",
    "        self.cfg = GNNConfig.load(cfg)\n",
    "        torch.manual_seed(self.cfg.seed); np.random.seed(self.cfg.seed); random.seed(self.cfg.seed)\n",
    "\n",
    "        self.node_frames = node_frames\n",
    "        self.edge_frames = edge_frames\n",
    "        self.graph = graph\n",
    "\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if self.cfg.device==\"cuda\" and torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "\n",
    "        # placeholders\n",
    "        self.edge_order: List[Tuple[int,int]] = []\n",
    "        self.snapshots: List[Data] = []\n",
    "\n",
    "    # -------------------------------------------- snapshot builder\n",
    "    def prepare_snapshots(self):\n",
    "        nodes = sorted(self.graph.nodes)\n",
    "        n2idx = {n:i for i,n in enumerate(nodes)}\n",
    "        self.edge_order = [(n2idx[u], n2idx[v]) for u,v in self.graph.edges]\n",
    "        E = len(self.edge_order)\n",
    "\n",
    "        # intersect timestamps\n",
    "        idxs = [df.index for df in self.node_frames.values()] \\\n",
    "             + [df.index for df in self.edge_frames.values()]\n",
    "        ts_common = sorted(set.intersection(*map(set, idxs)))\n",
    "\n",
    "        snaps: List[Data] = []\n",
    "        for ts in ts_common:\n",
    "            # node feats\n",
    "            x = torch.tensor(\n",
    "                np.vstack([ self.node_frames[n].loc[ts].to_numpy(dtype=np.float32)\n",
    "                            for n in nodes ]),\n",
    "                dtype=torch.float32)\n",
    "            # edge feats & labels\n",
    "            edge_feat_rows, edge_labels = [], []\n",
    "            for (s_idx,d_idx) in self.edge_order:\n",
    "                s,d = nodes[s_idx], nodes[d_idx]\n",
    "                row = self.edge_frames[f\"{s}-{d}\".lower()].loc[ts]\n",
    "                edge_labels.append(row[\"target\"])\n",
    "                edge_feat_rows.append(row.drop(\"target\").to_numpy(dtype=np.float32))\n",
    "            edge_attr = torch.tensor(np.vstack(edge_feat_rows), dtype=torch.float32)\n",
    "            edge_label = torch.tensor(edge_labels, dtype=torch.float32)\n",
    "            edge_index = torch.tensor(np.array(self.edge_order).T, dtype=torch.long)\n",
    "\n",
    "            snaps.append(Data(\n",
    "                x=x, edge_index=edge_index, edge_attr=edge_attr,\n",
    "                edge_label=edge_label,\n",
    "                snap_time=torch.tensor([pd.Timestamp(ts).value]),\n",
    "            ))\n",
    "        self.snapshots = snaps\n",
    "        return self\n",
    "\n",
    "    # -------------------------------------------- train/val/test split\n",
    "    def build_loaders(self):\n",
    "        snaps_sorted = sorted(self.snapshots, key=lambda g:g.snap_time.item())\n",
    "        if self.cfg.split_mode==\"date\":\n",
    "            cut = pd.Timestamp(self.cfg.cutoff_date).value\n",
    "            train = [g for g in snaps_sorted if g.snap_time.item()<=cut]\n",
    "            hold  = [g for g in snaps_sorted if g.snap_time.item()>cut]\n",
    "            mid = len(hold)//2\n",
    "            val, test = hold[:mid], hold[mid:]\n",
    "        else:\n",
    "            n=len(snaps_sorted)\n",
    "            nv = int(n*self.cfg.val_ratio); nt=int(n*self.cfg.test_ratio)\n",
    "            train, val, test = snaps_sorted[:n-nv-nt], snaps_sorted[n-nv-nt:n-nt], snaps_sorted[n-nt:]\n",
    "        self._compute_edge_weights(train)                     # <- imbalance weights\n",
    "\n",
    "        bs = self.cfg.batch_size\n",
    "        self.train_dl = DataLoader(train, batch_size=bs, shuffle=self.cfg.shuffle_in_split)\n",
    "        self.val_dl   = DataLoader(val,   batch_size=bs, shuffle=False)\n",
    "        self.test_dl  = DataLoader(test,  batch_size=bs, shuffle=False)\n",
    "        return self\n",
    "\n",
    "    # -------------------------------------------- imbalance weights\n",
    "    def _compute_edge_weights(self, train_snaps: List[Data]):\n",
    "        E = len(self.edge_order)\n",
    "        pos = torch.zeros(E); neg = torch.zeros(E)\n",
    "        for g in train_snaps:\n",
    "            pos += g.edge_label\n",
    "            neg += 1 - g.edge_label\n",
    "        w = neg / (pos + 1e-6)\n",
    "        if self.cfg.edge_pos_weights is not None:\n",
    "            w = torch.tensor(self.cfg.edge_pos_weights, dtype=torch.float32)\n",
    "        # attach to every snapshot (clone so grads aren’t tracked)\n",
    "        for g in self.snapshots:\n",
    "            g.edge_weight = w.clone()\n",
    "        self.edge_weight = w             # store for metrics\n",
    "\n",
    "    # -------------------------------------------- model init\n",
    "    def init_model(self):\n",
    "        d_in  = self.snapshots[0].x.size(1)\n",
    "        d_e   = self.snapshots[0].edge_attr.size(1)\n",
    "        enc_cls = ENCODER_FACTORY[self.cfg.model_name.lower()]\n",
    "        enc = enc_cls(d_in, self.cfg.hidden_dim, self.cfg.num_layers,\n",
    "                      self.cfg.layer_dropout, **({\"heads\":self.cfg.heads} if \"gat\" in self.cfg.model_name else {}))\n",
    "        dec = DotDecoder()\n",
    "        self.model = nn.ModuleDict({\"enc\":enc, \"dec\":dec}).to(self.device)\n",
    "        return self\n",
    "\n",
    "    # -------------------------------------------- optimiser / loss\n",
    "    def compile(self):\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "        self.opt = torch.optim.Adam(self.model.parameters(), lr=self.cfg.lr)\n",
    "\n",
    "    # -------------------------------------------- train helpers\n",
    "    def _forward_edges(self, data:Data):\n",
    "        z = self.model[\"enc\"](data.x, data.edge_index, data.edge_attr)\n",
    "        src,dst = data.edge_index        # edges are aligned with labels\n",
    "        logits = self.model[\"dec\"](z[src], z[dst]).squeeze(-1)\n",
    "        return logits\n",
    "\n",
    "    def _run_epoch(self, loader, train:bool):\n",
    "        if train: self.model.train();   torch.set_grad_enabled(True)\n",
    "        else:     self.model.eval();    torch.set_grad_enabled(False)\n",
    "\n",
    "        tot_loss = 0.0\n",
    "        for data in loader:\n",
    "            data = data.to(self.device)\n",
    "            logits = self._forward_edges(data)\n",
    "            loss_vec = self.loss_fn(logits, data.edge_label)\n",
    "            loss = (loss_vec * data.edge_weight.to(loss_vec)).mean()\n",
    "\n",
    "            if train:\n",
    "                self.opt.zero_grad(); loss.backward()\n",
    "                if self.cfg.grad_clip: nn.utils.clip_grad_norm_(self.model.parameters(), self.cfg.grad_clip)\n",
    "                self.opt.step()\n",
    "            tot_loss += loss.item()\n",
    "        return tot_loss/len(loader)\n",
    "\n",
    "    # -------------------------------------------- fit\n",
    "    def train(self):\n",
    "        best,val_best=1e9,0; patience=0\n",
    "        for epoch in range(1, self.cfg.epochs+1):\n",
    "            tr = self._run_epoch(self.train_dl, True)\n",
    "            vl = self._run_epoch(self.val_dl,   False)\n",
    "            print(f\"[{epoch:03d}] train {tr:.4f} | val {vl:.4f}\")\n",
    "            if vl<best: best=vl; patience=0; self.best_state= {k:v.cpu() for k,v in self.model.state_dict().items()}\n",
    "            else:       patience+=1\n",
    "            if patience>=self.cfg.patience:\n",
    "                print(\"Early stop.\"); break\n",
    "        self.model.load_state_dict(self.best_state)\n",
    "\n",
    "    # -------------------------------------------- predict on list of snapshots (loader)\n",
    "    @torch.no_grad()\n",
    "    def predict_loader(self, loader:DataLoader):\n",
    "        self.model.eval()\n",
    "        all_logits=[]\n",
    "        for data in loader:\n",
    "            data=data.to(self.device)\n",
    "            logits=self._forward_edges(data)\n",
    "            all_logits.append(logits.cpu())\n",
    "        return torch.cat(all_logits)    # concatenated over batches/snapshots\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Usage sketch (not executed here):\n",
    "# ---------------------------------------------------------------------\n",
    "#   exp = (EdgeExperiment(node_frames, edge_frames, graph)\n",
    "#            .prepare_snapshots()\n",
    "#            .build_loaders()\n",
    "#            .init_model()\n",
    "#            .compile())\n",
    "#   exp.train()\n",
    "#   test_logits = exp.predict_loader(exp.test_dl)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
