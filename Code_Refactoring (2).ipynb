{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5799bd9a",
   "metadata": {},
   "source": [
    "### DataLoading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5bb5143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Literal, Union\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Module-level docs & type aliases\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "Generic loaders & converters for regional-timeseries data.\n",
    "\n",
    "Canonical representation\n",
    "------------------------\n",
    "    CanonicalData = dict[str, pd.DataFrame]\n",
    "        key   – region name\n",
    "        value – DataFrame indexed by datetime; columns = features\n",
    "\n",
    "Disk layouts we support\n",
    "-----------------------\n",
    "    \"multi\" : MultiIndex columns (level-0 = region, level-1 = feature)\n",
    "    \"long\"  : Normal DataFrame with a *region* column\n",
    "    \"wide\"  : Wide DataFrame whose column names are \"<region><sep><feature>\"\n",
    "\"\"\"\n",
    "\n",
    "CanonicalData = Dict[str, pd.DataFrame]\n",
    "Layout        = Literal[\"multi\", \"long\", \"wide\"]\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# ─── Converters: DISK ➜ CANONICAL ────────────────────────────────────────────\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def _multi_to_dict(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    cols: List[str] | None = None\n",
    ") -> CanonicalData:\n",
    "    \"\"\"\n",
    "    Convert a Multi-Index column DataFrame to the canonical dict form.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df   : DataFrame with two column levels (region, feature)\n",
    "    cols : optional list of feature names to *keep* (others are dropped)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, DataFrame]  mapping region → feature-matrix\n",
    "    \"\"\"\n",
    "    out: CanonicalData = {}\n",
    "    for region in df.columns.get_level_values(0).unique():\n",
    "        sub = df.xs(region, axis=1, level=0)\n",
    "        if cols is not None:\n",
    "            sub = sub[[c for c in cols if c in sub.columns]]\n",
    "        out[region] = sub.copy()\n",
    "    return out\n",
    "\n",
    "\n",
    "def _long_to_dict(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    region_col: str = \"region\",\n",
    "    cols: List[str] | None = None,\n",
    ") -> CanonicalData:\n",
    "    \"\"\"\n",
    "    Convert a ‘long’ (tidy) DataFrame to canonical dict form.\n",
    "\n",
    "    A *long* frame must contain a `region_col` column; all other columns\n",
    "    are interpreted as features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df         : tidy DataFrame with duplicate timestamps per region\n",
    "    region_col : column that identifies the region\n",
    "    cols       : optional feature filter\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Canonical dict keyed by region\n",
    "    \"\"\"\n",
    "    if region_col not in df.columns:\n",
    "        raise KeyError(f\"Expected column '{region_col}' in long-layout dataframe.\")\n",
    "\n",
    "    features = [c for c in df.columns if c != region_col]\n",
    "    if cols is not None:\n",
    "        features = [c for c in features if c in cols]\n",
    "\n",
    "    out: CanonicalData = {}\n",
    "    for region, grp in df.groupby(region_col):\n",
    "        frame = grp.drop(columns=region_col)\n",
    "        out[region] = frame[features].copy()\n",
    "    return out\n",
    "\n",
    "\n",
    "def _wide_to_dict(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    sep: str = \"-\",\n",
    "    cols: List[str] | None = None,\n",
    ") -> CanonicalData:\n",
    "    \"\"\"\n",
    "    Convert a wide DataFrame with ‘region-feature’ column names into canon dict.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df   : DataFrame whose columns look like \"tokyo-demand\"\n",
    "    sep  : separator between region and feature\n",
    "    cols : optional feature filter\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Canonical dict keyed by region\n",
    "    \"\"\"\n",
    "    parts = df.columns.to_series().str.split(sep, n=1, expand=True)\n",
    "    if parts.isna().any().any():\n",
    "        raise ValueError(\n",
    "            f\"Column names do not all match <region>{sep}<feature> pattern.\"\n",
    "        )\n",
    "\n",
    "    out: CanonicalData = {}\n",
    "    for region in parts[0].unique():\n",
    "        mask      = parts[0] == region\n",
    "        sub_cols  = df.columns[mask]\n",
    "        features  = parts[1][mask]\n",
    "        sub_frame = df[sub_cols].copy()\n",
    "        sub_frame.columns = features\n",
    "        if cols is not None:\n",
    "            sub_frame = sub_frame[[c for c in cols if c in sub_frame.columns]]\n",
    "        out[region] = sub_frame\n",
    "    return out\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# ─── Converters: CANONICAL ➜ DISK ────────────────────────────────────────────\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def _dict_to_multi(data: CanonicalData) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combine a canonical dict into a Multi-Index column DataFrame.\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    for region, df in data.items():\n",
    "        tmp = df.copy()\n",
    "        tmp.columns = pd.MultiIndex.from_product([[region], tmp.columns])\n",
    "        frames.append(tmp)\n",
    "    return pd.concat(frames, axis=1).sort_index(axis=1)\n",
    "\n",
    "\n",
    "def _dict_to_long(\n",
    "    data: CanonicalData,\n",
    "    *,\n",
    "    region_col: str = \"region\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combine a canonical dict into a long (tidy) DataFrame.\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    for region, df in data.items():\n",
    "        tmp = df.copy()\n",
    "        tmp[region_col] = region\n",
    "        frames.append(tmp)\n",
    "    combined = pd.concat(frames)\n",
    "    order = [c for c in combined.columns if c != region_col] + [region_col]\n",
    "    return combined[order]\n",
    "\n",
    "\n",
    "def _dict_to_wide(\n",
    "    data: CanonicalData,\n",
    "    *,\n",
    "    sep: str = \"-\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combine a canonical dict into wide ‘region-feature’ columns.\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    for region, df in data.items():\n",
    "        tmp = df.copy()\n",
    "        tmp.columns = [f\"{region}{sep}{c}\" for c in tmp.columns]\n",
    "        frames.append(tmp)\n",
    "    return pd.concat(frames, axis=1)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# ─── Public helpers: one-file load / convert ────────────────────────────────\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def load_parquet_as_canonical(\n",
    "    path: str | Path,\n",
    "    *,\n",
    "    layout: Layout,\n",
    "    region_col: str = \"region\",\n",
    "    sep: str = \"-\",\n",
    "    cols: List[str] | None = None,\n",
    ") -> CanonicalData:\n",
    "    \"\"\"\n",
    "    Read a parquet file of known *layout* and return canonical dict form.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path       : file path\n",
    "    layout     : \"multi\", \"long\", or \"wide\" (must match the file)\n",
    "    region_col : name of the region column for *long* layout\n",
    "    sep        : region-feature separator for *wide* layout\n",
    "    cols       : optional feature subset\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    CanonicalData\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(path)\n",
    "    if layout == \"multi\":\n",
    "        return _multi_to_dict(df, cols=cols)\n",
    "    if layout == \"long\":\n",
    "        return _long_to_dict(df, region_col=region_col, cols=cols)\n",
    "    if layout == \"wide\":\n",
    "        return _wide_to_dict(df, sep=sep, cols=cols)\n",
    "    raise ValueError(f\"Unsupported layout '{layout}'.\")\n",
    "\n",
    "\n",
    "def canonical_to_layout(\n",
    "    data: CanonicalData,\n",
    "    layout: Layout,\n",
    "    *,\n",
    "    region_col: str = \"region\",\n",
    "    sep: str = \"-\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert *canonical* dict back to the requested layout format.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data       : canonical dict\n",
    "    layout     : target layout (\"multi\", \"long\", \"wide\")\n",
    "    region_col : name of region column for long layout\n",
    "    sep        : separator for wide layout\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame in the specified layout\n",
    "    \"\"\"\n",
    "    if layout == \"multi\":\n",
    "        return _dict_to_multi(data)\n",
    "    if layout == \"long\":\n",
    "        return _dict_to_long(data, region_col=region_col)\n",
    "    if layout == \"wide\":\n",
    "        return _dict_to_wide(data, sep=sep)\n",
    "    raise ValueError(f\"Unsupported target layout '{layout}'.\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# ─── Helpers for multi-file ingestion ───────────────────────────────────────\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def _collect_by_layout(\n",
    "    paths: List[str] | None,\n",
    "    layout: Layout,\n",
    "    *,\n",
    "    cols: List[str] | None,\n",
    "    region_col: str,\n",
    "    sep: str,\n",
    ") -> Dict[str, List[pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Internal helper: load every file in `paths` (given its layout)\n",
    "    and collate DataFrames by region.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[region, list[DataFrame]]\n",
    "    \"\"\"\n",
    "    bucket: Dict[str, List[pd.DataFrame]] = {}\n",
    "    for p in paths or []:\n",
    "        data = load_parquet_as_canonical(\n",
    "            p, layout=layout, region_col=region_col, sep=sep, cols=cols\n",
    "        )\n",
    "        for region, df in data.items():\n",
    "            bucket.setdefault(region, []).append(df)\n",
    "    return bucket\n",
    "\n",
    "\n",
    "def load_region_dataset(\n",
    "    *,\n",
    "    region: str,\n",
    "    multi_paths: List[str] | None = None,\n",
    "    long_paths: List[str] | None = None,\n",
    "    wide_paths: List[str] | None = None,\n",
    "    cols: List[str] | None = None,\n",
    "    region_col: str = \"region\",\n",
    "    sep: str = \"-\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate **one** region’s data from any number of parquet files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    region      : name of the region to extract\n",
    "    multi_paths : list of files in *multi* layout\n",
    "    long_paths  : list of files in *long* layout\n",
    "    wide_paths  : list of files in *wide* layout\n",
    "    cols        : optional feature subset\n",
    "    region_col  : region column (long layout)\n",
    "    sep         : separator (wide layout)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame with union of timestamps & features for that region\n",
    "    (duplicate columns are de-duplicated, keeping last-read version).\n",
    "    \"\"\"\n",
    "    frames: List[pd.DataFrame] = []\n",
    "    for layout, paths in (\n",
    "        (\"multi\", multi_paths),\n",
    "        (\"long\", long_paths),\n",
    "        (\"wide\", wide_paths),\n",
    "    ):\n",
    "        for p in paths or []:\n",
    "            data = load_parquet_as_canonical(\n",
    "                p, layout=layout, region_col=region_col, sep=sep, cols=cols\n",
    "            )\n",
    "            if region in data:\n",
    "                frames.append(data[region])\n",
    "\n",
    "    if not frames:\n",
    "        raise KeyError(f\"Region '{region}' not found in supplied paths.\")\n",
    "\n",
    "    combined = pd.concat(frames, axis=1)\n",
    "    combined = combined.loc[:, ~combined.columns.duplicated()]  # keep last duplicate\n",
    "    combined.sort_index(inplace=True)\n",
    "    return combined\n",
    "\n",
    "\n",
    "def load_all_regions_dataset(\n",
    "    *,\n",
    "    multi_paths: List[str] | None = None,\n",
    "    long_paths: List[str] | None = None,\n",
    "    wide_paths: List[str] | None = None,\n",
    "    regions: List[str] | None = None,\n",
    "    cols: List[str] | None = None,\n",
    "    region_col: str = \"region\",\n",
    "    sep: str = \"-\",\n",
    ") -> CanonicalData:\n",
    "    \"\"\"\n",
    "    Load **every** region (or a specified subset) from the supplied file lists.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    multi_paths : parquet files in *multi* layout\n",
    "    long_paths  : parquet files in *long* layout\n",
    "    wide_paths  : parquet files in *wide* layout\n",
    "    regions     : optional subset of region names to return\n",
    "    cols        : optional feature subset\n",
    "    region_col  : region column (long layout)\n",
    "    sep         : separator (wide layout)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    CanonicalData containing one DataFrame per region.\n",
    "    \"\"\"\n",
    "    # Read & collate per-layout\n",
    "    bucket: Dict[str, List[pd.DataFrame]] = {}\n",
    "\n",
    "    for paths, layout in (\n",
    "        (multi_paths, \"multi\"),\n",
    "        (long_paths, \"long\"),\n",
    "        (wide_paths, \"wide\"),\n",
    "    ):\n",
    "        part = _collect_by_layout(\n",
    "            paths,\n",
    "            layout,\n",
    "            cols=cols,\n",
    "            region_col=region_col,\n",
    "            sep=sep,\n",
    "        )\n",
    "        for region, frames in part.items():\n",
    "            bucket.setdefault(region, []).extend(frames)\n",
    "\n",
    "    # Filter to requested subset (if any)\n",
    "    if regions is not None:\n",
    "        bucket = {r: bucket[r] for r in regions if r in bucket}\n",
    "\n",
    "    # Merge frames per region\n",
    "    out: CanonicalData = {}\n",
    "    for region, frames in bucket.items():\n",
    "        df = pd.concat(frames, axis=1)\n",
    "        df = df.loc[:, ~df.columns.duplicated()]\n",
    "        df.sort_index(inplace=True)\n",
    "        out[region] = df\n",
    "\n",
    "    # Handle missing regions gracefully\n",
    "    if regions is not None:\n",
    "        missing = set(regions) - set(out)\n",
    "        if missing:\n",
    "            raise KeyError(\n",
    "                f\"The following requested regions were not found in any file: \"\n",
    "                f\"{', '.join(sorted(missing))}\"\n",
    "            )\n",
    "\n",
    "    return out\n",
    "#88888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Literal\n",
    "import pandas as pd\n",
    "\n",
    "# -------------  Canonical type & layout tags  --------------------------------\n",
    "CanonicalData = Dict[str, pd.DataFrame]\n",
    "Layout        = Literal[\"multi\", \"long\", \"wide\"]\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# ═════════════════════════════ I/O CONVERSION ════════════════════════════════\n",
    "# (unchanged from earlier — trimmed for brevity; keep the same helper fns:\n",
    "#     _multi_to_dict, _long_to_dict, _wide_to_dict,\n",
    "#     _dict_to_multi,  _dict_to_long,  _dict_to_wide,\n",
    "#     load_parquet_as_canonical, canonical_to_layout)\n",
    "# ----------------------------------------------------------------------------- \n",
    "\n",
    "# ...  <keep earlier converter code here exactly as before> ...\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# ══════════════════════════ PRE-MERGE FEATURE PASS ═══════════════════════════\n",
    "# ----------------------------------------------------------------------------- \n",
    "\n",
    "from jp_da_imb.utils.time import construct_time_features   # ← your existing util\n",
    "\n",
    "\n",
    "def preprocess_region_df(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    start_date: str | pd.Timestamp | None = None,\n",
    "    end_date:   str | pd.Timestamp | None = None,\n",
    "    freq: str | None = \"30T\",\n",
    "    na_removal: bool = True,\n",
    "    add_time_feats: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply *per-file* hygiene & feature engineering **before** merging with other\n",
    "    DataFrames for the same region.\n",
    "\n",
    "    Steps (all optional, controlled by kwargs)\n",
    "    -----------------------------------------\n",
    "    1. Clip to [start_date, end_date] (if provided)\n",
    "    2. Resample to `freq` (mean aggregation)       — skip if `freq=None`\n",
    "    3. Drop rows with any NA                       — only if `na_removal=True`\n",
    "    4. Add calendar/time features (and cast them\n",
    "       to `category`) using your `construct_time_features`\n",
    "       — only if `add_time_feats=True`\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    # 1. Clip date range -------------------------------------------------------\n",
    "    if start_date is not None:\n",
    "        out = out.loc[out.index >= pd.to_datetime(start_date)]\n",
    "    if end_date is not None:\n",
    "        out = out.loc[out.index <= pd.to_datetime(end_date)]\n",
    "\n",
    "    # 2. Resample --------------------------------------------------------------\n",
    "    if freq is not None:\n",
    "        out = out.resample(freq).mean()\n",
    "\n",
    "    # 3. Remove NAs ------------------------------------------------------------\n",
    "    if na_removal:\n",
    "        out = out.dropna()\n",
    "\n",
    "    # 4. Calendar / categorical time features ---------------------------------\n",
    "    if add_time_feats:\n",
    "        construct_time_features(out)     # mutates in place, adds columns\n",
    "        time_cols = [\n",
    "            \"weekday\", \"hour\", \"month\", \"quarter\",\n",
    "            \"koma\", \"koma_week\", \"is_holiday\",\n",
    "            \"is_peak\", \"is_weekend\",\n",
    "        ]\n",
    "        for c in time_cols:\n",
    "            if c in out.columns and not pd.api.types.is_categorical_dtype(out[c]):\n",
    "                out[c] = pd.Categorical(out[c])\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------- \n",
    "# ═══════════════  MULTI-FILE INGEST, NOW WITH PRE-PASS  ══════════════════════\n",
    "# ----------------------------------------------------------------------------- \n",
    "\n",
    "def _collect_by_layout(\n",
    "    paths: List[str] | None,\n",
    "    layout: Layout,\n",
    "    *,\n",
    "    cols: List[str] | None,\n",
    "    region_col: str,\n",
    "    sep: str,\n",
    "    preprocess_kwargs: dict,\n",
    ") -> Dict[str, List[pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Load every parquet file in `paths` -> canonical dict,\n",
    "    run `preprocess_region_df` on each region DataFrame,\n",
    "    and bucket them by region.\n",
    "    \"\"\"\n",
    "    bucket: Dict[str, List[pd.DataFrame]] = {}\n",
    "    for p in paths or []:\n",
    "        data = load_parquet_as_canonical(\n",
    "            p, layout=layout, region_col=region_col, sep=sep, cols=cols\n",
    "        )\n",
    "        for region, df in data.items():\n",
    "            df_proc = preprocess_region_df(df, **preprocess_kwargs)\n",
    "            bucket.setdefault(region, []).append(df_proc)\n",
    "    return bucket\n",
    "\n",
    "\n",
    "def load_region_dataset(\n",
    "    *,\n",
    "    region: str,\n",
    "    multi_paths: List[str] | None = None,\n",
    "    long_paths:  List[str] | None = None,\n",
    "    wide_paths:  List[str] | None = None,\n",
    "    cols:       List[str] | None = None,\n",
    "    region_col: str = \"region\",\n",
    "    sep: str = \"-\",\n",
    "    # --- pre-processing options ----------------------------------------------\n",
    "    start_date: str | pd.Timestamp | None = None,\n",
    "    end_date:   str | pd.Timestamp | None = None,\n",
    "    freq: str | None = \"30T\",\n",
    "    na_removal: bool = True,\n",
    "    add_time_feats: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a **single-region** DataFrame by reading any combination of\n",
    "    *multi*, *long*, and *wide* parquet files, applying the pre-processing\n",
    "    steps to each file **before** they are merged together.\n",
    "\n",
    "    Other parameters are identical to the earlier version, plus the\n",
    "    pre-processing kwargs shown above.\n",
    "    \"\"\"\n",
    "    preprocess_kwargs = dict(\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        freq=freq,\n",
    "        na_removal=na_removal,\n",
    "        add_time_feats=add_time_feats,\n",
    "    )\n",
    "\n",
    "    frames: List[pd.DataFrame] = []\n",
    "    for layout, paths in (\n",
    "        (\"multi\", multi_paths),\n",
    "        (\"long\",  long_paths),\n",
    "        (\"wide\",  wide_paths),\n",
    "    ):\n",
    "        for p in paths or []:\n",
    "            data = load_parquet_as_canonical(\n",
    "                p, layout=layout, region_col=region_col, sep=sep, cols=cols\n",
    "            )\n",
    "            if region in data:\n",
    "                frames.append(preprocess_region_df(data[region], **preprocess_kwargs))\n",
    "\n",
    "    if not frames:\n",
    "        raise KeyError(f\"Region '{region}' not found in supplied paths.\")\n",
    "\n",
    "    combined = pd.concat(frames, axis=1)\n",
    "    combined = combined.loc[:, ~combined.columns.duplicated()]  # keep last dup\n",
    "    combined.sort_index(inplace=True)\n",
    "    return combined\n",
    "\n",
    "\n",
    "def load_all_regions_dataset(\n",
    "    *,\n",
    "    multi_paths: List[str] | None = None,\n",
    "    long_paths:  List[str] | None = None,\n",
    "    wide_paths:  List[str] | None = None,\n",
    "    regions:     List[str] | None = None,\n",
    "    cols:       List[str] | None = None,\n",
    "    region_col: str = \"region\",\n",
    "    sep: str = \"-\",\n",
    "    # --- pre-processing options ----------------------------------------------\n",
    "    start_date: str | pd.Timestamp | None = None,\n",
    "    end_date:   str | pd.Timestamp | None = None,\n",
    "    freq: str | None = \"30T\",\n",
    "    na_removal: bool = True,\n",
    "    add_time_feats: bool = True,\n",
    ") -> CanonicalData:\n",
    "    \"\"\"\n",
    "    Load **all** regions (or a given subset) from the provided file lists,\n",
    "    applying the pre-merge feature-creation steps to every individual file.\n",
    "    \"\"\"\n",
    "    preprocess_kwargs = dict(\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        freq=freq,\n",
    "        na_removal=na_removal,\n",
    "        add_time_feats=add_time_feats,\n",
    "    )\n",
    "\n",
    "    bucket: Dict[str, List[pd.DataFrame]] = {}\n",
    "\n",
    "    for paths, layout in (\n",
    "        (multi_paths, \"multi\"),\n",
    "        (long_paths,  \"long\"),\n",
    "        (wide_paths,  \"wide\"),\n",
    "    ):\n",
    "        part = _collect_by_layout(\n",
    "            paths,\n",
    "            layout,\n",
    "            cols=cols,\n",
    "            region_col=region_col,\n",
    "            sep=sep,\n",
    "            preprocess_kwargs=preprocess_kwargs,\n",
    "        )\n",
    "        for region, frames in part.items():\n",
    "            bucket.setdefault(region, []).extend(frames)\n",
    "\n",
    "    # subset filter\n",
    "    if regions is not None:\n",
    "        bucket = {r: bucket[r] for r in regions if r in bucket}\n",
    "\n",
    "    # merge frames per region\n",
    "    out: CanonicalData = {}\n",
    "    for region, frames in bucket.items():\n",
    "        df = pd.concat(frames, axis=1)\n",
    "        df = df.loc[:, ~df.columns.duplicated()]\n",
    "        df.sort_index(inplace=True)\n",
    "        out[region] = df\n",
    "\n",
    "    # raise on missing explicit requests\n",
    "    if regions is not None:\n",
    "        missing = set(regions) - set(out)\n",
    "        if missing:\n",
    "            raise KeyError(\n",
    "                f\"The following requested regions were not found in any file: \"\n",
    "                f\\\"{', '.join(sorted(missing))}\\\"\n",
    "            )\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb11b702",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Literal\n",
    "import pandas as pd\n",
    "from jp_da_imb.utils.time import construct_time_features   # ← your util\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Canonical type & layout tags\n",
    "# -----------------------------------------------------------\n",
    "CanonicalData = Dict[str, pd.DataFrame]\n",
    "Layout        = Literal[\"multi\", \"long\", \"wide\"]\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "#  DISK ➜ CANONICAL converters (unchanged from earlier)\n",
    "#  _multi_to_dict, _long_to_dict, _wide_to_dict\n",
    "#  CANONICAL ➜ DISK converters\n",
    "#  _dict_to_multi, _dict_to_long, _dict_to_wide\n",
    "#  load_parquet_as_canonical, canonical_to_layout\n",
    "#  (omitted here for brevity but keep exactly as before)\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "#  Per-file pre-merge feature pass (unchanged)\n",
    "# -----------------------------------------------------------\n",
    "def preprocess_region_df(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    start_date: str | pd.Timestamp | None = None,\n",
    "    end_date:   str | pd.Timestamp | None = None,\n",
    "    freq: str | None = \"30T\",\n",
    "    na_removal: bool = True,\n",
    "    add_time_feats: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "\n",
    "    if start_date is not None:\n",
    "        out = out.loc[out.index >= pd.to_datetime(start_date)]\n",
    "    if end_date is not None:\n",
    "        out = out.loc[out.index <= pd.to_datetime(end_date)]\n",
    "\n",
    "    if freq is not None:\n",
    "        out = out.resample(freq).mean()\n",
    "\n",
    "    if na_removal:\n",
    "        out = out.dropna()\n",
    "\n",
    "    if add_time_feats:\n",
    "        construct_time_features(out)\n",
    "        for c in [\n",
    "            \"weekday\", \"hour\", \"month\", \"quarter\",\n",
    "            \"koma\", \"koma_week\", \"is_holiday\",\n",
    "            \"is_peak\", \"is_weekend\",\n",
    "        ]:\n",
    "            if c in out.columns and not pd.api.types.is_categorical_dtype(out[c]):\n",
    "                out[c] = pd.Categorical(out[c])\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "#  SINGLE-REGION LOADER  (long files can differ in region_col)\n",
    "# -----------------------------------------------------------\n",
    "def load_region_dataset(\n",
    "    *,\n",
    "    region: str,\n",
    "    multi_paths: List[str] | None = None,\n",
    "    long_paths:  List[str] | None = None,\n",
    "    wide_paths:  List[str] | None = None,\n",
    "    long_region_cols: List[str] | None = None,\n",
    "    cols:       List[str] | None = None,\n",
    "    sep: str = \"-\",\n",
    "    # --- pre-processing options\n",
    "    start_date: str | pd.Timestamp | None = None,\n",
    "    end_date:   str | pd.Timestamp | None = None,\n",
    "    freq: str | None = \"30T\",\n",
    "    na_removal: bool = True,\n",
    "    add_time_feats: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate *one* region’s data from any mix of multi/long/wide parquet files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    region            : which region to return\n",
    "    long_region_cols  : list parallel to `long_paths` giving the column\n",
    "                        name that holds the region label *for each* file.\n",
    "                        Omit to default every long file to \\\"region\\\".\n",
    "    All other args are as before.\n",
    "    \"\"\"\n",
    "    preprocess_kwargs = dict(\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        freq=freq,\n",
    "        na_removal=na_removal,\n",
    "        add_time_feats=add_time_feats,\n",
    "    )\n",
    "\n",
    "    frames: List[pd.DataFrame] = []\n",
    "\n",
    "    # ---- multi layout (single region column level 0) ------------------------\n",
    "    for p in multi_paths or []:\n",
    "        data = load_parquet_as_canonical(p, layout=\"multi\", cols=cols)\n",
    "        if region in data:\n",
    "            frames.append(preprocess_region_df(data[region], **preprocess_kwargs))\n",
    "\n",
    "    # ---- wide layout --------------------------------------------------------\n",
    "    for p in wide_paths or []:\n",
    "        data = load_parquet_as_canonical(p, layout=\"wide\", sep=sep, cols=cols)\n",
    "        if region in data:\n",
    "            frames.append(preprocess_region_df(data[region], **preprocess_kwargs))\n",
    "\n",
    "    # ---- long layout  (per-file region_col!) -------------------------------\n",
    "    if long_paths:\n",
    "        # fill default list if none supplied\n",
    "        if long_region_cols is None:\n",
    "            long_region_cols = [\"region\"] * len(long_paths)\n",
    "        if len(long_region_cols) != len(long_paths):\n",
    "            raise ValueError(\n",
    "                \"long_region_cols must be the same length as long_paths\"\n",
    "            )\n",
    "\n",
    "        for p, reg_col in zip(long_paths, long_region_cols):\n",
    "            data = load_parquet_as_canonical(\n",
    "                p, layout=\"long\", region_col=reg_col, cols=cols\n",
    "            )\n",
    "            if region in data:\n",
    "                frames.append(preprocess_region_df(data[region], **preprocess_kwargs))\n",
    "\n",
    "    if not frames:\n",
    "        raise KeyError(f\"Region '{region}' not found in supplied paths.\")\n",
    "\n",
    "    combined = pd.concat(frames, axis=1)\n",
    "    combined = combined.loc[:, ~combined.columns.duplicated()]\n",
    "    combined.sort_index(inplace=True)\n",
    "    return combined\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "#  ALL-REGIONS LOADER  (long paths paired with region_cols)\n",
    "# -----------------------------------------------------------\n",
    "def load_all_regions_dataset(\n",
    "    *,\n",
    "    multi_paths: List[str] | None = None,\n",
    "    long_paths:  List[str] | None = None,\n",
    "    wide_paths:  List[str] | None = None,\n",
    "    long_region_cols: List[str] | None = None,\n",
    "    regions:     List[str] | None = None,\n",
    "    cols:        List[str] | None = None,\n",
    "    sep: str = \"-\",\n",
    "    # --- pre-processing options\n",
    "    start_date: str | pd.Timestamp | None = None,\n",
    "    end_date:   str | pd.Timestamp | None = None,\n",
    "    freq: str | None = \"30T\",\n",
    "    na_removal: bool = True,\n",
    "    add_time_feats: bool = True,\n",
    ") -> CanonicalData:\n",
    "    \"\"\"\n",
    "    Load every region (or the ones in `regions`) applying the pre-merge\n",
    "    feature pass. Long-format files can each declare their own region column\n",
    "    via `long_region_cols`.\n",
    "    \"\"\"\n",
    "    preprocess_kwargs = dict(\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        freq=freq,\n",
    "        na_removal=na_removal,\n",
    "        add_time_feats=add_time_feats,\n",
    "    )\n",
    "\n",
    "    bucket: Dict[str, List[pd.DataFrame]] = {}\n",
    "\n",
    "    # -------- helper to merge into bucket -----------------\n",
    "    def _collect(data_dict: CanonicalData):\n",
    "        for r, df in data_dict.items():\n",
    "            bucket.setdefault(r, []).append(\n",
    "                preprocess_region_df(df, **preprocess_kwargs)\n",
    "            )\n",
    "\n",
    "    # ---- multi files -------------------------------------\n",
    "    for p in multi_paths or []:\n",
    "        _collect(load_parquet_as_canonical(p, layout=\"multi\", cols=cols))\n",
    "\n",
    "    # ---- wide files --------------------------------------\n",
    "    for p in wide_paths or []:\n",
    "        _collect(load_parquet_as_canonical(p, layout=\"wide\", sep=sep, cols=cols))\n",
    "\n",
    "    # ---- long files (per-file region_col) ----------------\n",
    "    if long_paths:\n",
    "        if long_region_cols is None:\n",
    "            long_region_cols = [\"region\"] * len(long_paths)\n",
    "        if len(long_region_cols) != len(long_paths):\n",
    "            raise ValueError(\n",
    "                \"long_region_cols must be the same length as long_paths\"\n",
    "            )\n",
    "        for p, reg_col in zip(long_paths, long_region_cols):\n",
    "            _collect(load_parquet_as_canonical(\n",
    "                p, layout=\"long\", region_col=reg_col, cols=cols\n",
    "            ))\n",
    "\n",
    "    # ---- subset / merge ----------------------------------\n",
    "    if regions is not None:\n",
    "        bucket = {r: bucket[r] for r in regions if r in bucket}\n",
    "\n",
    "    out: CanonicalData = {}\n",
    "    for r, frames in bucket.items():\n",
    "        df = pd.concat(frames, axis=1)\n",
    "        df = df.loc[:, ~df.columns.duplicated()]\n",
    "        df.sort_index(inplace=True)\n",
    "        out[r] = df\n",
    "\n",
    "    if regions is not None:\n",
    "        missing = set(regions) - set(out)\n",
    "        if missing:\n",
    "            raise KeyError(\n",
    "                \"The following requested regions were not found in any file: \"\n",
    "                + \", \".join(sorted(missing))\n",
    "            )\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86756802",
   "metadata": {},
   "source": [
    "### Target Creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c595a9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Dict\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Helper 1 ── single DataFrame\n",
    "# ------------------------------------------------------------------\n",
    "def add_target_column(\n",
    "    df: pd.DataFrame,\n",
    "    minuend: str,\n",
    "    subtrahend: str,\n",
    "    *,\n",
    "    target_name: str = \"target\",\n",
    "    trim_to_valid: bool = False,\n",
    "    inplace: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add `target_name = df[minuend] - df[subtrahend]` to **one** DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    minuend, subtrahend : column names to subtract\n",
    "    target_name         : name of new column (default \\\"target\\\")\n",
    "    trim_to_valid       : if True, cut the DataFrame to the slice\n",
    "                          [first_non_NA, last_non_NA] of the new column\n",
    "    inplace             : True → mutate the original frame, False → return a copy\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame  (original or copy, according to `inplace`)\n",
    "    \"\"\"\n",
    "    if not inplace:\n",
    "        df = df.copy()\n",
    "\n",
    "    if minuend not in df.columns or subtrahend not in df.columns:\n",
    "        raise KeyError(f\\\"Missing '{minuend}' or '{subtrahend}' in columns.\\\")\n",
    "\n",
    "    df[target_name] = df[minuend] - df[subtrahend]\n",
    "\n",
    "    if trim_to_valid:\n",
    "        first = df[target_name].first_valid_index()\n",
    "        last  = df[target_name].last_valid_index()\n",
    "        if first is None or last is None:\n",
    "            raise ValueError(f\\\"`{target_name}` has no non-NA values to trim to.\\\")\n",
    "        df = df.loc[first:last]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Helper 2 ── canonical dict (ℹ️ deep-copies unless inplace=True)\n",
    "# ------------------------------------------------------------------\n",
    "CanonicalData = Dict[str, pd.DataFrame]\n",
    "\n",
    "def add_target_to_canonical(\n",
    "    data: CanonicalData,\n",
    "    minuend: str,\n",
    "    subtrahend: str,\n",
    "    *,\n",
    "    target_name: str = \"target\",\n",
    "    trim_to_valid: bool = False,\n",
    "    inplace: bool = False,\n",
    ") -> CanonicalData:\n",
    "    \"\"\"\n",
    "    Add the `target_name` column to **each** DataFrame in a canonical dict.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data              : dict[str, DataFrame]\n",
    "    minuend, subtrahend : columns to subtract\n",
    "    target_name       : new column name\n",
    "    trim_to_valid     : if True, slice every DataFrame to the first/last\n",
    "                        non-NA of the new column\n",
    "    inplace           : True → mutate the dict & frames in place\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    CanonicalData  (original or a deep copy, per `inplace`)\n",
    "    \"\"\"\n",
    "    out = data if inplace else {r: df.copy() for r, df in data.items()}\n",
    "\n",
    "    for region, df in out.items():\n",
    "        if minuend not in df.columns or subtrahend not in df.columns:\n",
    "            raise KeyError(\n",
    "                f\\\"Region '{region}' is missing '{minuend}' or '{subtrahend}'.\\\"\n",
    "            )\n",
    "\n",
    "        df[target_name] = df[minuend] - df[subtrahend]\n",
    "\n",
    "        if trim_to_valid:\n",
    "            first = df[target_name].first_valid_index()\n",
    "            last  = df[target_name].last_valid_index()\n",
    "            if first is None or last is None:\n",
    "                raise ValueError(\n",
    "                    f\\\"Region '{region}': `{target_name}` has no non-NA values to trim to.\\\"\n",
    "                )\n",
    "            out[region] = df.loc[first:last]\n",
    "\n",
    "    return out\n",
    "How to use them\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "# Single DataFrame example\n",
    "tokyo_df = add_target_column(\n",
    "    tokyo_df,\n",
    "    minuend=\"pri_imb_down_¥_kwh_jst_min30_a\",\n",
    "    subtrahend=\"pri_spot_jepx_¥_kwh_jst_min30_a\",\n",
    "    target_name=\"ImbalanceMinusSpot\",\n",
    "    trim_to_valid=True,     # ← crops to first/last non-NA\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# Canonical dict example\n",
    "data = add_target_to_canonical(\n",
    "    data,\n",
    "    minuend=\"demand\",\n",
    "    subtrahend=\"supply\",\n",
    "    target_name=\"net_demand\",\n",
    "    trim_to_valid=True,     # each region trimmed independently\n",
    "    inplace=False,          # get a new dict back\n",
    ")\n",
    "Both functions now guarantee that your returned frames start at the first timestamp\n",
    "where target is defined and end at the last one—handy for modelling pipelines\n",
    "that can’t handle leading/trailing NaNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d737ed31",
   "metadata": {},
   "source": [
    "### calculating the number of subgraphs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b67c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from functools import lru_cache\n",
    "\n",
    "def connected_subsets_containing(G, start, available):\n",
    "    \"\"\"\n",
    "    Yield every connected subset S⊆available that includes 'start'.\n",
    "    \"\"\"\n",
    "    stack = [({start}, set(G.neighbors(start)) & available)]\n",
    "    while stack:\n",
    "        current, frontier = stack.pop()\n",
    "        yield frozenset(current)          # every prefix is connected\n",
    "        for v in list(frontier):          # expand by one frontier node\n",
    "            new_current  = current | {v}\n",
    "            new_frontier = (frontier | (set(G.neighbors(v)) & available)) - new_current\n",
    "            stack.append((new_current, new_frontier))\n",
    "\n",
    "def all_connected_partitions(G):\n",
    "    nodes = frozenset(G.nodes)\n",
    "\n",
    "    @lru_cache(None)\n",
    "    def helper(remaining):\n",
    "        if not remaining:\n",
    "            return {()}                   # empty partition\n",
    "        start = min(remaining)            # canonical choice\n",
    "        result = set()\n",
    "        for S in connected_subsets_containing(G, start, remaining):\n",
    "            rest = remaining - S\n",
    "            for tail in helper(rest):\n",
    "                result.add(tuple(sorted((S,)+tail, key=sorted)))\n",
    "        return result\n",
    "\n",
    "    return helper(nodes)\n",
    "\n",
    "# -----------------  example  -----------------\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(range(9))\n",
    "G.add_edges_from([(0,1),(1,2),(2,3),(3,4),    # a 5-node path\n",
    "                  (5,6),(6,7),(7,5),          # a triangle\n",
    "                  (4,5)])                     # a bridge 4-5\n",
    "# node 8 is isolated\n",
    "\n",
    "parts = all_connected_partitions(G)\n",
    "print(\"number of possible connectivity states:\", len(parts))\n",
    "print(\"some examples:\")\n",
    "for p in sorted(list(parts)[:10]):            # show first 10\n",
    "    print(\"  \", [sorted(block) for block in p])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6e8825",
   "metadata": {},
   "source": [
    "### Combining regions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdcef10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Sequence\n",
    "import pandas as pd\n",
    "\n",
    "CanonicalData = Dict[str, pd.DataFrame]\n",
    "\n",
    "def combine_regions(\n",
    "    data: CanonicalData,\n",
    "    regions: Sequence[str],\n",
    "    *,\n",
    "    new_region: str | None = None,          # ← now optional\n",
    "    add: Sequence[str] | None = None,\n",
    "    average: Sequence[str] | None = None,\n",
    "    drop: Sequence[str] | None = None,\n",
    "    keep_first: Sequence[str] | None = None,\n",
    "    remove_originals: bool = False,\n",
    "    inplace: bool = False,\n",
    ") -> CanonicalData:\n",
    "    \"\"\"\n",
    "    Collapse multiple regional DataFrames into a single aggregate.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data, regions            : see earlier version\n",
    "    new_region               : dict key for the combined frame; if omitted,\n",
    "                               defaults to \\\"region1_region2_...\\\" (order-preserving)\n",
    "    add, average, drop,\n",
    "    keep_first, remove_originals, inplace\n",
    "                              : same semantics as before\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    CanonicalData\n",
    "    \"\"\"\n",
    "    if not regions:\n",
    "        raise ValueError(\"`regions` must contain at least one region name.\")\n",
    "\n",
    "    missing = [r for r in regions if r not in data]\n",
    "    if missing:\n",
    "        raise KeyError(\"Regions not found: \" + \", \".join(missing))\n",
    "\n",
    "    # auto-generate name if not supplied\n",
    "    if new_region is None:\n",
    "        new_region = \"_\".join(regions)\n",
    "\n",
    "    add      = set(add or [])\n",
    "    average  = set(average or [])\n",
    "    drop_set = set(drop or [])\n",
    "    keep1    = set(keep_first or [])\n",
    "\n",
    "    # guard against overlaps\n",
    "    overlap = (add & average) | (add & keep1) | (average & keep1)\n",
    "    overlap |= (add | average | keep1) & drop_set\n",
    "    if overlap:\n",
    "        raise ValueError(\n",
    "            \"Columns appear in multiple operation lists: \"\n",
    "            + \", \".join(sorted(overlap))\n",
    "        )\n",
    "\n",
    "    out: CanonicalData = data if inplace else {r: df.copy() for r, df in data.items()}\n",
    "\n",
    "    # build wide MultiIndex frame: level-0 = region\n",
    "    wide = pd.concat([out[r] for r in regions], axis=1, keys=regions)\n",
    "    features = wide.columns.get_level_values(1).unique()\n",
    "\n",
    "    combined_cols = {}\n",
    "    for col in features:\n",
    "        if col in drop_set:\n",
    "            continue\n",
    "        if col in add:\n",
    "            combined_cols[col] = wide.xs(col, level=1, axis=1).sum(axis=1)\n",
    "        elif col in average:\n",
    "            combined_cols[col] = wide.xs(col, level=1, axis=1).mean(axis=1)\n",
    "        elif col in keep1:\n",
    "            combined_cols[col] = out[regions[0]][col]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"No rule provided for column '{col}'. \"\n",
    "                \"Put it in add/average/keep_first/drop.\"\n",
    "            )\n",
    "\n",
    "    combined_df = pd.concat(combined_cols, axis=1).sort_index()\n",
    "\n",
    "    out[new_region] = combined_df\n",
    "\n",
    "    if remove_originals:\n",
    "        for r in regions:\n",
    "            out.pop(r, None)\n",
    "\n",
    "    return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
